{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\ude80 MCPOmni Connect - Universal Gateway to MCP Servers","text":"<p>New to MCPOmni Connect?</p> <p>Start with the Installation Guide and Quick Start to get up and running in minutes.</p> <p>MCPOmni Connect is a powerful, intelligent AI agent system and universal command-line interface (CLI) that goes beyond being just a gateway to the Model Context Protocol (MCP) ecosystem. It acts as an autonomous agent through its ReAct Agent Mode and Orchestrator Mode, capable of independent reasoning, decision-making, and complex task execution.</p>"},{"location":"#key-features","title":"\u2728 Key Features","text":""},{"location":"#intelligent-agent-system","title":"\ud83e\udd16 Intelligent Agent System","text":"ReAct Agent ModeOrchestrator Agent ModeInteractive Chat Mode <ul> <li>Autonomous task execution with reasoning and action cycles</li> <li>Independent decision-making without human intervention</li> <li>Advanced problem-solving through iterative reasoning</li> <li>Self-guided tool selection and execution</li> <li>Complex task decomposition and handling</li> </ul> <ul> <li>Strategic multi-step task planning and execution</li> <li>Intelligent coordination across multiple MCP servers</li> <li>Dynamic agent delegation and communication</li> <li>Parallel task execution when possible</li> <li>Sophisticated workflow management with real-time progress monitoring</li> </ul> <ul> <li>Human-in-the-loop task execution with approval workflows</li> <li>Step-by-step guidance and explanations</li> <li>Educational mode for understanding AI decision processes</li> </ul>"},{"location":"#universal-connectivity","title":"\ud83d\udd0c Universal Connectivity","text":"<ul> <li>Multi-Protocol Support: stdio, Server-Sent Events (SSE), Streamable HTTP</li> <li>Authentication Support: OAuth 2.0, Bearer tokens, custom headers</li> <li>Container Integration: Docker and NPX package execution</li> <li>Extensible Transport Layer: Future-ready protocol support</li> </ul>"},{"location":"#ai-powered-intelligence","title":"\ud83e\udde0 AI-Powered Intelligence","text":"<p>MCPOmni Connect uses LiteLLM for unified access to 100+ AI models across all major providers:</p> <ul> <li>OpenAI (GPT-4, GPT-3.5, etc.)</li> <li>Anthropic (Claude 3.5 Sonnet, Claude 3 Haiku, etc.)</li> <li>Google (Gemini Pro, Gemini Flash, etc.)</li> <li>Groq (Llama, Mixtral, Gemma, etc.)</li> <li>DeepSeek (DeepSeek-V3, DeepSeek-Coder, etc.)</li> <li>Azure OpenAI, OpenRouter, Ollama (local models)</li> </ul>"},{"location":"#security-privacy","title":"\ud83d\udd12 Security &amp; Privacy","text":"<ul> <li>Explicit User Control: All tool executions require user approval in chat mode</li> <li>Data Protection: Strict access controls and server-specific isolation</li> <li>Privacy-First: Minimal data collection, encrypted transport protocols</li> </ul>"},{"location":"#memory-management","title":"\ud83d\udcbe Memory Management","text":"<ul> <li>Redis-Powered Persistence: Long-term conversation memory storage</li> <li>File-Based Chat History: Save and restore complete conversations</li> <li>Intelligent Context Management: Automatic context pruning and retrieval</li> </ul>"},{"location":"#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<pre><code>graph TB\n    A[MCPOmni Connect] --&gt; B[Transport Layer]\n    A --&gt; C[Session Management]\n    A --&gt; D[Tool Management]\n    A --&gt; E[AI Integration]\n\n    B --&gt; B1[Stdio Transport]\n    B --&gt; B2[SSE Transport]\n    B --&gt; B3[Docker Integration]\n\n    C --&gt; C1[Multi-Server Orchestration]\n    C --&gt; C2[Connection Lifecycle]\n\n    D --&gt; D1[Dynamic Tool Discovery]\n    D --&gt; D2[Cross-Server Tool Routing]\n    D --&gt; D3[Tool Execution Engine]\n\n    E --&gt; E1[LLM Processing]\n    E --&gt; E2[Context Management]\n    E --&gt; E3[Response Generation]</code></pre>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Get Started in 3 Steps</p> <ol> <li>Install: <code>uv add mcpomni-connect</code> or <code>pip install mcpomni-connect</code></li> <li>Configure: Set up your <code>.env</code> file and <code>servers_config.json</code></li> <li>Run: Execute <code>mcpomni_connect</code> to start the CLI</li> </ol> <p>Detailed Installation Guide \u2192</p>"},{"location":"#documentation-structure","title":"\ud83d\udcd6 Documentation Structure","text":"<ul> <li> <p>\ud83d\ude80 Getting Started</p> <p>Installation, quick start, and basic setup instructions</p> <p> Get Started</p> </li> <li> <p>\u2699\ufe0f Configuration</p> <p>Complete configuration guide including transport types and authentication</p> <p> Configure</p> </li> <li> <p>\ud83d\udcda User Guide</p> <p>Detailed usage instructions, commands, and operation modes</p> <p> Learn</p> </li> <li> <p>\ud83c\udfaf Features</p> <p>Deep dive into agent systems, tool orchestration, and advanced features</p> <p> Explore</p> </li> <li> <p>\ud83d\udd27 Advanced</p> <p>Architecture details, API reference, and advanced examples</p> <p> Advanced</p> </li> <li> <p>\ud83d\udc68\u200d\ud83d\udcbb Development</p> <p>Testing, contributing, and development guidelines</p> <p> Contribute</p> </li> </ul>"},{"location":"#use-cases","title":"\ud83c\udfaf Use Cases","text":"<p>MCPOmni Connect excels in scenarios requiring:</p> <ul> <li>Multi-System Integration: Connect and orchestrate across different MCP servers</li> <li>Autonomous Task Execution: Let AI agents handle complex workflows independently</li> <li>Interactive Development: Build and test MCP integrations with immediate feedback</li> <li>Enterprise Automation: Scale AI-powered automation across organizational tools</li> </ul>"},{"location":"#support-community","title":"\ud83d\udcec Support &amp; Community","text":"<ul> <li>Documentation: You're reading it! \ud83d\udcd6</li> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> <li>Email: abiolaadedayo1993@gmail.com</li> </ul> <p>Ready to connect your AI to everything? Let's get started! \ud83d\ude80 </p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to MCPOmni Connect will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Comprehensive MkDocs documentation site</li> <li>Enhanced configuration guides with visual examples</li> <li>Interactive command reference</li> </ul>"},{"location":"changelog/#improved","title":"Improved","text":"<ul> <li>Documentation structure and navigation</li> <li>Code examples and usage patterns</li> <li>Troubleshooting guides</li> </ul>"},{"location":"changelog/#0117-2025-05-28","title":"[0.1.17] - 2025-05-28","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>OAuth Authentication Support:</li> <li>Added OAuth 2.0 authentication flow for MCP servers</li> <li>Support for multiple authentication methods:<ul> <li>OAuth 2.0</li> <li>Bearer token</li> <li>Custom headers</li> </ul> </li> <li>Flexible authentication configuration in server settings</li> <li>Secure credential management</li> <li>Enhanced Server Configuration:</li> <li>Updated server configuration format to support OAuth</li> <li>Added authentication method specification</li> <li>Improved server connection security</li> <li>Better error handling for authentication failures</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Updated server configuration examples to include OAuth support</li> <li>Enhanced documentation for authentication methods</li> <li>Improved security section in README</li> <li>Updated server management commands documentation</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Improved authentication error handling</li> <li>Enhanced security documentation</li> <li>Updated configuration validation for authentication methods</li> </ul>"},{"location":"changelog/#0116-2025-05-16","title":"[0.1.16] - 2025-05-16","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>New Streamable HTTP Transport:</li> <li>Added support for streamable HTTP transport protocol</li> <li>Efficient data streaming capabilities</li> <li>Configurable timeout and read timeout settings</li> <li>Header support for authentication and custom configurations</li> <li>Dynamic Server Management:</li> <li>New <code>/add_servers:&lt;config.json&gt;</code> command to add one or more servers</li> <li>New <code>/remove_server:&lt;server_name&gt;</code> command to remove servers</li> <li>Support for adding multiple servers from a single configuration file</li> <li>Real-time server capability updates after adding/removing servers</li> <li>Enhanced Server Configuration:</li> <li>Added streamable HTTP server configuration examples</li> <li>Updated documentation for new transport type</li> <li>Improved server management commands documentation</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Updated README with new server management commands</li> <li>Enhanced server configuration examples to include streamable HTTP</li> <li>Improved documentation for transport protocols</li> <li>Updated interactive commands section with new server management features</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Improved server connection handling</li> <li>Enhanced error messages for server management commands</li> <li>Updated documentation formatting for consistency</li> </ul>"},{"location":"changelog/#0115-2025-05-05","title":"[0.1.15] - 2025-05-05","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Token &amp; Usage Management:</li> <li><code>/api_stats</code> command to view total tokens used, total requests, response tokens, and number of requests</li> <li>Ability to set limits for total requests and total token usage; agent will automatically stop when limits are reached</li> <li>Configurable tool call timeout and max steps; agent will terminate if these thresholds are exceeded</li> <li>Developer Integration Enhancements:</li> <li>Expanded documentation and examples for using MCPOmni Connect as a backend Python library</li> <li>FastAPI example for building custom API servers with support for both ReAct Agent and Orchestrator Agent modes</li> <li>Minimal code snippets for custom MCP client integration in Python projects</li> <li>FastAPI API Documentation:</li> <li>Documented <code>/chat/agent_chat</code> endpoint with request/response examples</li> <li>Added web client usage instructions for <code>examples/index.html</code></li> <li>Environment Variables Reference:</li> <li>Added table of supported environment variables and their descriptions in the README</li> <li>Typos and Documentation Improvements:</li> <li>Fixed typos and improved clarity throughout the README</li> <li>Clarified configuration options and usage instructions</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Updated server configuration examples to clarify usage of <code>tool_call_timeout</code>, <code>max_steps</code>, <code>request_limit</code>, and <code>total_tokens_limit</code></li> <li>Improved README structure with new \"Examples\", \"Developer Integration\", \"Token &amp; Usage Management\", and \"FastAPI API Endpoints\" sections</li> <li>Enhanced error handling and documentation for agent termination on reaching usage limits</li> </ul>"},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Corrected typos in documentation and configuration comments</li> <li>Improved consistency in code examples and documentation formatting</li> </ul>"},{"location":"changelog/#0114-2025-04-18","title":"[0.1.14] - 2025-04-18","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>DeepSeek model integration with full support for tool execution</li> <li>New Orchestrator Agent Mode:</li> <li>Advanced planning for complex multi-step tasks</li> <li>Strategic delegation across multiple MCP servers</li> <li>Intelligent agent coordination and communication</li> <li>Parallel task execution capabilities</li> <li>Dynamic resource allocation</li> <li>Sophisticated workflow management</li> <li>Real-time progress monitoring</li> <li>Adaptive task prioritization</li> <li>Client-Side Sampling Support:</li> <li>Dynamic sampling configuration from client</li> <li>Flexible LLM response generation</li> <li>Customizable sampling parameters</li> <li>Real-time sampling adjustments</li> <li>Chat History File Storage:</li> <li>Save complete chat conversations to files</li> <li>Load previous conversations from saved files</li> <li>Continue conversations from where you left off</li> <li>File-based backup and restoration</li> <li>Persistent chat history across sessions</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Enhanced Mode System with three distinct modes:</li> <li>Chat Mode (Default)</li> <li>Autonomous Mode</li> <li>Orchestrator Mode</li> <li>Updated AI model integration documentation</li> <li>Improved chat history management system</li> <li>Enhanced server configuration options for new features</li> </ul>"},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Improved mode switching reliability</li> <li>Enhanced chat history persistence</li> <li>Optimized orchestrator mode performance</li> </ul>"},{"location":"changelog/#0113-2025-04-14","title":"[0.1.13] - 2025-04-14","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>Gemini model integration with full support for tool execution</li> <li>Redis-powered memory persistence:</li> <li>Conversation history tracking</li> <li>State management across sessions</li> <li>Configurable memory retention</li> <li>Efficient data serialization and retrieval</li> <li>Multi-server memory synchronization</li> <li>Agentic Mode capabilities:</li> <li>Autonomous task execution without human intervention</li> <li>Advanced reasoning and decision-making</li> <li>Complex task decomposition and handling</li> <li>Self-guided tool selection and execution</li> <li>Advanced prompt features:</li> <li>Dynamic prompt discovery across servers</li> <li>JSON and key-value format support</li> <li>Nested argument structures</li> <li>Automatic type conversion and validation</li> <li>Comprehensive troubleshooting guide with:</li> <li>Common issues and solutions</li> <li>Debug mode instructions</li> <li>Support workflow</li> <li>Detailed architecture documentation with component breakdown</li> <li>Advanced server configuration examples for:</li> <li>Multiple transport protocols</li> <li>Various LLM providers</li> <li>Docker integration</li> </ul>"},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>Enhanced installation process with UV package manager</li> <li>Improved development quick start guide</li> <li>Updated server configuration format to support multiple LLM providers</li> <li>Expanded model support documentation for all providers</li> <li>Enhanced security documentation with explicit user control details</li> <li>Restructured README with clearer sections and examples</li> </ul>"},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Standardized command formatting in documentation</li> <li>Improved code block consistency</li> <li>Enhanced example clarity and completeness</li> </ul>"},{"location":"changelog/#011-2025-03-27","title":"[0.1.1] - 2025-03-27","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>Comprehensive Security &amp; Privacy section with detailed subsections:</li> <li>Explicit User Control</li> <li>Data Protection</li> <li>Privacy-First Approach</li> <li>Secure Communication</li> <li>Detailed Model Support section covering:</li> <li>OpenAI Models</li> <li>OpenRouter Models</li> <li>Groq Models</li> <li>Universal Model Support through ReAct Agent</li> <li>Structured Testing section with:</li> <li>Multiple test running options</li> <li>Test directory structure</li> <li>Coverage reporting instructions</li> <li>Support for additional LLM providers:</li> <li>OpenRouter integration</li> <li>Groq integration</li> <li>Universal model support through ReAct Agent</li> </ul>"},{"location":"changelog/#changed_5","title":"Changed","text":"<ul> <li>Improved AI-Powered Intelligence section:</li> <li>Added support for multiple LLM providers (OpenAI, OpenRouter, Groq)</li> <li>Added detailed ReAct Agent capabilities for models without function calling</li> <li>Fixed typos in \"seamless\"</li> <li>Enhanced Server Configuration Examples:</li> <li>Added support for multiple LLM providers</li> <li>Updated model examples</li> <li>Added comments for supported providers</li> <li>Updated Prerequisites:</li> <li>Changed Python version requirement from 3.12+ to 3.10+</li> <li>Updated API key requirements to support multiple providers</li> <li>Improved environment variable setup:</li> <li>Changed from OPENAI_API_KEY to LLM_API_KEY for broader provider support</li> <li>Added support for multiple API keys in .env file</li> </ul>"},{"location":"changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Typos in model integration descriptions</li> <li>Formatting issues in various sections</li> <li>Inconsistent capitalization in headers</li> <li>Fixed typo in \"client\" command (was \"cient\")</li> <li>Improved code block formatting and consistency</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>Redundant security information</li> <li>Simplified test section</li> <li>Removed specific OpenAI model references in favor of provider-agnostic examples</li> <li>Removed redundant prompt examples in favor of more structured documentation</li> </ul>"},{"location":"changelog/#010-2025-03-21","title":"[0.1.0] - 2025-03-21","text":"<ul> <li>Initial release</li> <li>Basic MCP server integration</li> <li>OpenAI model support</li> <li>Core CLI functionality</li> </ul>"},{"location":"changelog/#release-process","title":"Release Process","text":"<p>MCPOmni Connect follows these practices for releases:</p> <ol> <li>Semantic Versioning: We use SemVer for version numbers</li> <li>Regular Releases: New features are released regularly with proper testing</li> <li>Backwards Compatibility: We maintain backwards compatibility within major versions</li> <li>Security Updates: Security fixes are released immediately as patch versions</li> </ol>"},{"location":"changelog/#getting-updates","title":"Getting Updates","text":"<p>To update to the latest version:</p> <pre><code># Using UV (recommended)\nuv sync --upgrade\n\n# Using pip\npip install --upgrade mcpomni-connect\n\n# Check your version\nmcpomni_connect --version\n</code></pre>"},{"location":"changelog/#contributing-to-releases","title":"Contributing to Releases","text":"<p>We welcome contributions! See our Contributing Guide for:</p> <ul> <li>How to submit bug reports</li> <li>Feature request process</li> <li>Development setup</li> <li>Testing requirements</li> <li>Release criteria </li> </ul>"},{"location":"advanced/architecture/","title":"Architecture","text":"<p>MCPOmni Connect is built with a modular, extensible architecture designed for scalability, reliability, and ease of use. This document provides a comprehensive overview of the system's design and components.</p>"},{"location":"advanced/architecture/#system-overview","title":"System Overview","text":"<p>MCPOmni Connect acts as an intelligent gateway between users and the Model Context Protocol (MCP) ecosystem, providing AI-powered automation and orchestration capabilities.</p> <pre><code>graph TB\n    User[\ud83d\udc64 User] --&gt; CLI[\ud83d\udda5\ufe0f CLI Interface]\n    CLI --&gt; Core[\ud83e\udde0 Core Engine]\n\n    Core --&gt; LLM[\ud83e\udd16 LLM Integration]\n    Core --&gt; Memory[\ud83d\udcbe Memory Management]\n    Core --&gt; Session[\ud83d\udd04 Session Management]\n    Core --&gt; Transport[\ud83d\ude80 Transport Layer]\n\n    LLM --&gt; Providers[\u2601\ufe0f LLM Providers]\n    Memory --&gt; Redis[\ud83d\udcca Redis]\n    Memory --&gt; Files[\ud83d\udcc1 File Storage]\n\n    Transport --&gt; Stdio[\ud83d\udcfa Stdio]\n    Transport --&gt; SSE[\ud83d\udce1 SSE]\n    Transport --&gt; HTTP[\ud83c\udf10 HTTP]\n\n    Stdio --&gt; LocalMCP[\ud83d\udd27 Local MCP Servers]\n    SSE --&gt; RemoteMCP[\ud83c\udf0d Remote MCP Servers]\n    HTTP --&gt; APIMnCP[\ud83d\udd0c API MCP Servers]\n\n    Core --&gt; Agent[\ud83e\udd16 Agent System]\n    Agent --&gt; Chat[\ud83d\udcac Chat Mode]\n    Agent --&gt; Auto[\u26a1 Autonomous Mode]\n    Agent --&gt; Orch[\ud83c\udfaf Orchestrator Mode]</code></pre>"},{"location":"advanced/architecture/#core-components","title":"Core Components","text":""},{"location":"advanced/architecture/#1-cli-interface-layer","title":"1. CLI Interface Layer","text":"<p>The user-facing command-line interface that handles input/output and user interactions.</p> <p>Responsibilities: - Command parsing and validation - User input handling - Output formatting and display - Interactive prompts and confirmations - Error message presentation</p> <p>Key Features: - Rich text formatting with syntax highlighting - Interactive command completion - Real-time status updates - Debug mode visualization</p>"},{"location":"advanced/architecture/#2-core-engine","title":"2. Core Engine","text":"<p>The central orchestrator that coordinates all system components.</p> <p>Responsibilities: - Component lifecycle management - Event coordination and messaging - Configuration management - Error handling and recovery - System state management</p> <p>Components: <pre><code>class CoreEngine:\n    def __init__(self):\n        self.session_manager = SessionManager()\n        self.transport_layer = TransportLayer()\n        self.llm_integration = LLMIntegration()\n        self.memory_manager = MemoryManager()\n        self.agent_system = AgentSystem()\n</code></pre></p>"},{"location":"advanced/architecture/#3-agent-system","title":"3. Agent System","text":"<p>The AI-powered decision-making and execution engine.</p> <pre><code>graph LR\n    Agent[\ud83e\udde0 Agent System] --&gt; ReAct[\ud83d\udd04 ReAct Engine]\n    Agent --&gt; Orchestrator[\ud83c\udfaf Orchestrator]\n    Agent --&gt; Context[\ud83d\udcda Context Manager]\n\n    ReAct --&gt; Reasoning[\ud83d\udcad Reasoning]\n    ReAct --&gt; Acting[\u26a1 Acting]\n    ReAct --&gt; Observing[\ud83d\udc41\ufe0f Observing]\n\n    Orchestrator --&gt; Planning[\ud83d\udccb Planning]\n    Orchestrator --&gt; Coordination[\ud83e\udd1d Coordination]\n    Orchestrator --&gt; Monitoring[\ud83d\udcca Monitoring]</code></pre> <p>Mode Architecture:</p> Chat ModeAutonomous ModeOrchestrator Mode <pre><code>class ChatMode:\n    def process_request(self, user_input):\n        # 1. Parse user intent\n        intent = self.parse_intent(user_input)\n\n        # 2. Plan actions\n        actions = self.plan_actions(intent)\n\n        # 3. Request approval for each action\n        for action in actions:\n            if self.request_approval(action):\n                result = self.execute_action(action)\n                self.present_result(result)\n</code></pre> <pre><code>class AutonomousMode:\n    def process_request(self, user_input):\n        # 1. Parse and understand goal\n        goal = self.parse_goal(user_input)\n\n        # 2. ReAct loop\n        while not self.goal_achieved(goal):\n            thought = self.think(current_state)\n            action = self.plan_action(thought)\n            observation = self.execute_action(action)\n            self.update_state(observation)\n\n        # 3. Report completion\n        return self.generate_report()\n</code></pre> <pre><code>class OrchestratorMode:\n    def process_request(self, user_input):\n        # 1. Strategic analysis\n        strategy = self.analyze_requirements(user_input)\n\n        # 2. Multi-phase planning\n        phases = self.create_execution_plan(strategy)\n\n        # 3. Coordinate execution\n        for phase in phases:\n            agents = self.allocate_agents(phase)\n            results = self.execute_parallel(agents)\n            self.merge_results(results)\n\n        return self.final_report()\n</code></pre>"},{"location":"advanced/architecture/#transport-layer","title":"Transport Layer","text":""},{"location":"advanced/architecture/#transport-architecture","title":"Transport Architecture","text":"<pre><code>graph TB\n    TL[\ud83d\ude80 Transport Layer] --&gt; TM[\ud83d\udccb Transport Manager]\n    TM --&gt; Registry[\ud83d\udcca Transport Registry]\n    TM --&gt; Factory[\ud83c\udfed Transport Factory]\n\n    Factory --&gt; StdioT[\ud83d\udcfa Stdio Transport]\n    Factory --&gt; SSET[\ud83d\udce1 SSE Transport]\n    Factory --&gt; HTTPT[\ud83c\udf10 HTTP Transport]\n\n    StdioT --&gt; Process[\u2699\ufe0f Process Manager]\n    SSET --&gt; EventStream[\ud83d\udcc8 Event Stream]\n    HTTPT --&gt; AuthManager[\ud83d\udd10 Auth Manager]\n\n    AuthManager --&gt; OAuth[\ud83d\udd11 OAuth Handler]\n    AuthManager --&gt; Bearer[\ud83c\udfab Bearer Token]\n    AuthManager --&gt; Custom[\ud83d\udd27 Custom Headers]</code></pre>"},{"location":"advanced/architecture/#transport-implementations","title":"Transport Implementations","text":""},{"location":"advanced/architecture/#stdio-transport","title":"Stdio Transport","text":"<pre><code>class StdioTransport:\n    def __init__(self, command, args):\n        self.process = subprocess.Popen(\n            [command] + args,\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE\n        )\n\n    async def send_message(self, message):\n        await self.process.stdin.write(message)\n\n    async def receive_message(self):\n        return await self.process.stdout.readline()\n</code></pre>"},{"location":"advanced/architecture/#sse-transport","title":"SSE Transport","text":"<pre><code>class SSETransport:\n    def __init__(self, url, headers):\n        self.url = url\n        self.headers = headers\n        self.client = httpx.AsyncClient()\n\n    async def connect(self):\n        self.stream = self.client.stream(\n            \"GET\", self.url, headers=self.headers\n        )\n\n    async def receive_events(self):\n        async for line in self.stream.aiter_lines():\n            if line.startswith(\"data: \"):\n                yield json.loads(line[6:])\n</code></pre>"},{"location":"advanced/architecture/#http-transport","title":"HTTP Transport","text":"<pre><code>class HTTPTransport:\n    def __init__(self, url, auth_config):\n        self.url = url\n        self.auth = self.setup_auth(auth_config)\n        self.client = httpx.AsyncClient()\n\n    async def send_request(self, data):\n        response = await self.client.post(\n            self.url,\n            json=data,\n            headers=self.auth.get_headers()\n        )\n        return response.json()\n</code></pre>"},{"location":"advanced/architecture/#session-management","title":"Session Management","text":""},{"location":"advanced/architecture/#session-architecture","title":"Session Architecture","text":"<pre><code>graph LR\n    SM[\ud83d\udd04 Session Manager] --&gt; SL[\ud83d\udcca Session Lifecycle]\n    SM --&gt; CR[\ud83d\udd17 Connection Registry]\n    SM --&gt; HM[\ud83d\udc96 Health Monitor]\n\n    CR --&gt; Servers[\ud83d\udda5\ufe0f Server Connections]\n    HM --&gt; Heartbeat[\ud83d\udc93 Heartbeat]\n    HM --&gt; Recovery[\ud83d\udd04 Recovery]\n\n    Servers --&gt; Active[\u2705 Active]\n    Servers --&gt; Idle[\ud83d\ude34 Idle]\n    Servers --&gt; Failed[\u274c Failed]</code></pre>"},{"location":"advanced/architecture/#connection-management","title":"Connection Management","text":"<pre><code>class SessionManager:\n    def __init__(self):\n        self.connections = {}\n        self.health_monitor = HealthMonitor()\n        self.recovery_manager = RecoveryManager()\n\n    async def connect_server(self, server_config):\n        transport = self.create_transport(server_config)\n        connection = await transport.connect()\n\n        self.connections[server_config.name] = connection\n        self.health_monitor.add_connection(connection)\n\n        return connection\n\n    async def health_check(self):\n        for name, connection in self.connections.items():\n            if not await connection.is_healthy():\n                await self.recovery_manager.recover(name, connection)\n</code></pre>"},{"location":"advanced/architecture/#llm-integration","title":"LLM Integration","text":""},{"location":"advanced/architecture/#litellm-integration-architecture","title":"LiteLLM Integration Architecture","text":"<pre><code>graph TB\n    LLM[\ud83e\udd16 LLM Integration] --&gt; LiteLLM[\u26a1 LiteLLM]\n    LLM --&gt; Config[\u2699\ufe0f Config Manager]\n    LLM --&gt; Context[\ud83d\udcda Context Manager]\n\n    LiteLLM --&gt; OpenAI[\ud83d\udd35 OpenAI]\n    LiteLLM --&gt; Anthropic[\ud83d\udfe3 Anthropic]\n    LiteLLM --&gt; Google[\ud83d\udd34 Google]\n    LiteLLM --&gt; Others[... Others]\n\n    Context --&gt; Window[\ud83e\ude9f Context Window]\n    Context --&gt; History[\ud83d\udcdc History]\n    Context --&gt; Pruning[\u2702\ufe0f Pruning]</code></pre>"},{"location":"advanced/architecture/#llm-integration-implementation","title":"LLM Integration Implementation","text":"<pre><code>class LLMIntegration:\n    def __init__(self, config):\n        self.config = config\n        self.context_manager = ContextManager()\n        self.client = self.setup_litellm()\n\n    def setup_litellm(self):\n        return litellm.completion\n\n    async def generate_response(self, messages, tools=None):\n        # Prepare context\n        context = self.context_manager.prepare_context(messages)\n\n        # Call LLM\n        response = await self.client(\n            model=f\"{self.config.provider}/{self.config.model}\",\n            messages=context,\n            tools=tools,\n            temperature=self.config.temperature,\n            max_tokens=self.config.max_tokens\n        )\n\n        return response\n</code></pre>"},{"location":"advanced/architecture/#memory-management","title":"Memory Management","text":""},{"location":"advanced/architecture/#memory-architecture","title":"Memory Architecture","text":"<pre><code>graph TB\n    MM[\ud83d\udcbe Memory Manager] --&gt; SM[\ud83e\udde0 Session Memory]\n    MM --&gt; RM[\ud83d\udcca Redis Memory]\n    MM --&gt; FM[\ud83d\udcc1 File Memory]\n\n    SM --&gt; Current[\u26a1 Current Context]\n    SM --&gt; Buffer[\ud83d\udce6 Message Buffer]\n\n    RM --&gt; Persistence[\ud83d\udcbe Persistence]\n    RM --&gt; TTL[\u23f0 TTL Management]\n\n    FM --&gt; Save[\ud83d\udcbe Save Operations]\n    FM --&gt; Load[\ud83d\udce5 Load Operations]\n    FM --&gt; Backup[\ud83d\udd04 Backup]</code></pre>"},{"location":"advanced/architecture/#memory-implementation","title":"Memory Implementation","text":"<pre><code>class MemoryManager:\n    def __init__(self, config):\n        self.session_memory = SessionMemory()\n        self.redis_memory = RedisMemory(config.redis) if config.redis else None\n        self.file_memory = FileMemory()\n        self.enabled = False\n\n    async def store_message(self, message):\n        # Always store in session\n        self.session_memory.add(message)\n\n        # Store in Redis if enabled\n        if self.enabled and self.redis_memory:\n            await self.redis_memory.store(message)\n\n    async def get_context(self, limit=None):\n        # Get from Redis if available\n        if self.enabled and self.redis_memory:\n            return await self.redis_memory.get_context(limit)\n\n        # Fallback to session memory\n        return self.session_memory.get_context(limit)\n</code></pre>"},{"location":"advanced/architecture/#tool-management","title":"Tool Management","text":""},{"location":"advanced/architecture/#tool-discovery-and-execution","title":"Tool Discovery and Execution","text":"<pre><code>graph LR\n    TM[\ud83d\udd27 Tool Manager] --&gt; Discovery[\ud83d\udd0d Discovery]\n    TM --&gt; Registry[\ud83d\udccb Registry]\n    TM --&gt; Executor[\u26a1 Executor]\n\n    Discovery --&gt; Servers[\ud83d\udda5\ufe0f Server Tools]\n    Registry --&gt; Metadata[\ud83d\udcca Tool Metadata]\n    Registry --&gt; Routing[\ud83d\udee4\ufe0f Routing Rules]\n\n    Executor --&gt; Parallel[\u26a1 Parallel Exec]\n    Executor --&gt; Serial[\ud83d\udd04 Serial Exec]\n    Executor --&gt; Fallback[\ud83d\udd04 Fallback]</code></pre>"},{"location":"advanced/architecture/#tool-execution-engine","title":"Tool Execution Engine","text":"<pre><code>class ToolManager:\n    def __init__(self):\n        self.registry = ToolRegistry()\n        self.executor = ToolExecutor()\n        self.router = ToolRouter()\n\n    async def discover_tools(self, connections):\n        for connection in connections:\n            tools = await connection.list_tools()\n            for tool in tools:\n                self.registry.register(tool, connection)\n\n    async def execute_tool(self, tool_name, parameters):\n        # Route to appropriate server\n        connection = self.router.route(tool_name)\n\n        # Execute with timeout and retry\n        return await self.executor.execute(\n            connection, tool_name, parameters\n        )\n</code></pre>"},{"location":"advanced/architecture/#security-architecture","title":"Security Architecture","text":""},{"location":"advanced/architecture/#security-layers","title":"Security Layers","text":"<pre><code>graph TB\n    Security[\ud83d\udd10 Security] --&gt; Auth[\ud83d\udd11 Authentication]\n    Security --&gt; Authz[\ud83d\udee1\ufe0f Authorization]\n    Security --&gt; Encryption[\ud83d\udd12 Encryption]\n    Security --&gt; Isolation[\ud83c\udff0 Isolation]\n\n    Auth --&gt; OAuth[\ud83d\udd10 OAuth 2.0]\n    Auth --&gt; Tokens[\ud83c\udfab Bearer Tokens]\n    Auth --&gt; Custom[\ud83d\udd27 Custom Auth]\n\n    Authz --&gt; ServerLevel[\ud83d\udda5\ufe0f Server Level]\n    Authz --&gt; ToolLevel[\ud83d\udd27 Tool Level]\n\n    Encryption --&gt; Transit[\ud83d\ude80 In Transit]\n    Encryption --&gt; Rest[\ud83d\udcbe At Rest]\n\n    Isolation --&gt; ServerIso[\ud83c\udfe0 Server Isolation]\n    Isolation --&gt; DataIso[\ud83d\udcca Data Isolation]</code></pre>"},{"location":"advanced/architecture/#security-implementation","title":"Security Implementation","text":"<pre><code>class SecurityManager:\n    def __init__(self):\n        self.auth_manager = AuthenticationManager()\n        self.authz_manager = AuthorizationManager()\n        self.crypto = CryptographyManager()\n\n    async def authenticate_server(self, server_config):\n        if server_config.auth_method == \"oauth\":\n            return await self.auth_manager.oauth_flow(server_config)\n        elif server_config.auth_method == \"bearer\":\n            return self.auth_manager.bearer_token(server_config)\n\n    def encrypt_sensitive_data(self, data):\n        return self.crypto.encrypt(data)\n\n    def authorize_tool_access(self, tool, user_context):\n        return self.authz_manager.check_permission(tool, user_context)\n</code></pre>"},{"location":"advanced/architecture/#performance-and-scalability","title":"Performance and Scalability","text":""},{"location":"advanced/architecture/#performance-architecture","title":"Performance Architecture","text":"<pre><code>graph LR\n    Perf[\u26a1 Performance] --&gt; Caching[\ud83d\uddc4\ufe0f Caching]\n    Perf --&gt; Pooling[\ud83c\udfca Connection Pooling]\n    Perf --&gt; Async[\ud83d\udd04 Async Processing]\n    Perf --&gt; Monitoring[\ud83d\udcca Monitoring]\n\n    Caching --&gt; ToolCache[\ud83d\udd27 Tool Results]\n    Caching --&gt; ContextCache[\ud83d\udcda Context Cache]\n\n    Pooling --&gt; ConnPool[\ud83d\udd17 Connection Pool]\n    Pooling --&gt; ThreadPool[\ud83e\uddf5 Thread Pool]\n\n    Async --&gt; EventLoop[\ud83d\udd04 Event Loop]\n    Async --&gt; Coroutines[\u26a1 Coroutines]</code></pre>"},{"location":"advanced/architecture/#performance-optimizations","title":"Performance Optimizations","text":"<pre><code>class PerformanceManager:\n    def __init__(self):\n        self.cache = CacheManager()\n        self.connection_pool = ConnectionPool()\n        self.metrics = MetricsCollector()\n\n    async def execute_with_cache(self, tool_call):\n        cache_key = self.generate_cache_key(tool_call)\n\n        # Check cache first\n        cached_result = await self.cache.get(cache_key)\n        if cached_result:\n            self.metrics.record_cache_hit(tool_call)\n            return cached_result\n\n        # Execute and cache result\n        result = await self.execute_tool(tool_call)\n        await self.cache.set(cache_key, result, ttl=300)\n\n        self.metrics.record_cache_miss(tool_call)\n        return result\n</code></pre>"},{"location":"advanced/architecture/#configuration-system","title":"Configuration System","text":""},{"location":"advanced/architecture/#configuration-architecture","title":"Configuration Architecture","text":"<pre><code>graph TB\n    Config[\u2699\ufe0f Configuration] --&gt; Env[\ud83c\udf0d Environment]\n    Config --&gt; JSON[\ud83d\udcc4 JSON Config]\n    Config --&gt; Runtime[\u26a1 Runtime Config]\n\n    Env --&gt; APIKeys[\ud83d\udd11 API Keys]\n    Env --&gt; Redis[\ud83d\udcca Redis Config]\n    Env --&gt; Debug[\ud83d\udc1b Debug Settings]\n\n    JSON --&gt; LLMConfig[\ud83e\udd16 LLM Config]\n    JSON --&gt; Servers[\ud83d\udda5\ufe0f Server Config]\n    JSON --&gt; AgentConfig[\ud83e\udd16 Agent Config]\n\n    Runtime --&gt; Dynamic[\ud83d\udd04 Dynamic Updates]\n    Runtime --&gt; Validation[\u2705 Validation]</code></pre>"},{"location":"advanced/architecture/#configuration-management","title":"Configuration Management","text":"<pre><code>class ConfigurationManager:\n    def __init__(self):\n        self.env_config = self.load_env_config()\n        self.json_config = self.load_json_config()\n        self.runtime_config = {}\n        self.validators = ConfigValidators()\n\n    def load_env_config(self):\n        return {\n            'llm_api_key': os.getenv('LLM_API_KEY'),\n            'redis_host': os.getenv('REDIS_HOST', 'localhost'),\n            'redis_port': int(os.getenv('REDIS_PORT', 6379)),\n            'debug': os.getenv('DEBUG', 'false').lower() == 'true'\n        }\n\n    def validate_configuration(self):\n        errors = []\n\n        # Validate environment variables\n        if not self.env_config.get('llm_api_key'):\n            errors.append(\"LLM_API_KEY is required\")\n\n        # Validate JSON configuration\n        if not self.json_config.get('LLM'):\n            errors.append(\"LLM configuration is required\")\n\n        if errors:\n            raise ConfigurationError(errors)\n</code></pre>"},{"location":"advanced/architecture/#error-handling-and-recovery","title":"Error Handling and Recovery","text":""},{"location":"advanced/architecture/#error-handling-strategy","title":"Error Handling Strategy","text":"<pre><code>graph TB\n    Error[\u274c Error Handling] --&gt; Detection[\ud83d\udd0d Detection]\n    Error --&gt; Classification[\ud83d\udcca Classification]\n    Error --&gt; Recovery[\ud83d\udd04 Recovery]\n    Error --&gt; Reporting[\ud83d\udce2 Reporting]\n\n    Detection --&gt; Monitoring[\ud83d\udcca Monitoring]\n    Detection --&gt; Logging[\ud83d\udcdd Logging]\n\n    Classification --&gt; Transient[\u23f1\ufe0f Transient]\n    Classification --&gt; Permanent[\ud83d\udd12 Permanent]\n    Classification --&gt; Unknown[\u2753 Unknown]\n\n    Recovery --&gt; Retry[\ud83d\udd04 Retry]\n    Recovery --&gt; Fallback[\ud83d\udd04 Fallback]\n    Recovery --&gt; Graceful[\u2705 Graceful Degradation]</code></pre>"},{"location":"advanced/architecture/#recovery-implementation","title":"Recovery Implementation","text":"<pre><code>class ErrorRecoveryManager:\n    def __init__(self):\n        self.retry_policies = RetryPolicies()\n        self.fallback_strategies = FallbackStrategies()\n        self.circuit_breakers = CircuitBreakerRegistry()\n\n    async def handle_error(self, error, context):\n        error_type = self.classify_error(error)\n\n        if error_type == ErrorType.TRANSIENT:\n            return await self.retry_with_backoff(context)\n        elif error_type == ErrorType.PERMANENT:\n            return await self.execute_fallback(context)\n        else:\n            return await self.graceful_degradation(context)\n\n    async def retry_with_backoff(self, context, max_retries=3):\n        for attempt in range(max_retries):\n            try:\n                await asyncio.sleep(2 ** attempt)  # Exponential backoff\n                return await context.retry()\n            except Exception as e:\n                if attempt == max_retries - 1:\n                    raise e\n                continue\n</code></pre>"},{"location":"advanced/architecture/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"advanced/architecture/#observability-stack","title":"Observability Stack","text":"<pre><code>graph LR\n    Obs[\ud83d\udc41\ufe0f Observability] --&gt; Metrics[\ud83d\udcca Metrics]\n    Obs --&gt; Logging[\ud83d\udcdd Logging]\n    Obs --&gt; Tracing[\ud83d\udd0d Tracing]\n    Obs --&gt; Health[\ud83d\udc96 Health Checks]\n\n    Metrics --&gt; Performance[\u26a1 Performance]\n    Metrics --&gt; Usage[\ud83d\udcc8 Usage]\n    Metrics --&gt; Errors[\u274c Errors]\n\n    Logging --&gt; Structured[\ud83d\udccb Structured]\n    Logging --&gt; Levels[\ud83d\udcca Log Levels]\n\n    Tracing --&gt; Requests[\ud83d\udccd Request Tracing]\n    Tracing --&gt; Dependencies[\ud83d\udd17 Dependency Tracing]</code></pre>"},{"location":"advanced/architecture/#monitoring-implementation","title":"Monitoring Implementation","text":"<pre><code>class MonitoringManager:\n    def __init__(self):\n        self.metrics_collector = MetricsCollector()\n        self.logger = StructuredLogger()\n        self.tracer = DistributedTracer()\n        self.health_checker = HealthChecker()\n\n    def record_tool_execution(self, tool_name, duration, success):\n        self.metrics_collector.increment(\n            'tool_executions_total',\n            tags={'tool': tool_name, 'success': success}\n        )\n        self.metrics_collector.histogram(\n            'tool_execution_duration',\n            duration,\n            tags={'tool': tool_name}\n        )\n\n    def log_user_interaction(self, user_input, response, context):\n        self.logger.info(\n            \"user_interaction\",\n            user_input=user_input,\n            response_length=len(response),\n            mode=context.mode,\n            servers_connected=len(context.servers)\n        )\n</code></pre>"},{"location":"advanced/architecture/#extensibility-and-plugin-system","title":"Extensibility and Plugin System","text":""},{"location":"advanced/architecture/#plugin-architecture","title":"Plugin Architecture","text":"<pre><code>graph TB\n    Plugin[\ud83d\udd0c Plugin System] --&gt; Registry[\ud83d\udccb Plugin Registry]\n    Plugin --&gt; Loader[\ud83d\udce5 Plugin Loader]\n    Plugin --&gt; Lifecycle[\ud83d\udd04 Lifecycle Manager]\n\n    Registry --&gt; Transport[\ud83d\ude80 Transport Plugins]\n    Registry --&gt; LLM[\ud83e\udd16 LLM Plugins]\n    Registry --&gt; Tool[\ud83d\udd27 Tool Plugins]\n    Registry --&gt; Memory[\ud83d\udcbe Memory Plugins]\n\n    Loader --&gt; Discovery[\ud83d\udd0d Discovery]\n    Loader --&gt; Validation[\u2705 Validation]\n    Loader --&gt; Installation[\ud83d\udce6 Installation]</code></pre> <p>This architecture provides a solid foundation for MCPOmni Connect's current capabilities while allowing for future expansion and customization.</p> <p>Next: API Reference \u2192 </p>"},{"location":"configuration/authentication/","title":"Authentication","text":"<p>MCPOmni Connect supports multiple authentication methods to securely connect to MCP servers. Choose the appropriate method based on your server's requirements.</p>"},{"location":"configuration/authentication/#authentication-methods-overview","title":"Authentication Methods Overview","text":"Method Use Case Security Level Setup Complexity OAuth 2.0 Enterprise APIs High Medium Bearer Token API services Medium Low Custom Headers Proprietary systems Medium Low No Auth Local/trusted servers Low None"},{"location":"configuration/authentication/#oauth-20-authentication","title":"OAuth 2.0 Authentication","text":"<p>OAuth 2.0 provides secure, token-based authentication with automatic token refresh.</p>"},{"location":"configuration/authentication/#configuration","title":"Configuration","text":"<pre><code>{\n    \"oauth-server\": {\n        \"transport_type\": \"streamable_http\",\n        \"auth\": {\n            \"method\": \"oauth\"\n        },\n        \"url\": \"https://api.example.com/mcp\"\n    }\n}\n</code></pre>"},{"location":"configuration/authentication/#oauth-flow","title":"OAuth Flow","text":"<ol> <li>Automatic Server Start: MCPOmni Connect starts callback server on <code>http://localhost:3000</code></li> <li>Browser Authorization: Opens browser for user authentication</li> <li>Token Exchange: Receives authorization code and exchanges for access token</li> <li>Automatic Refresh: Handles token refresh automatically</li> </ol>"},{"location":"configuration/authentication/#oauth-server-behavior","title":"OAuth Server Behavior","text":"<p>OAuth Callback Server</p> <ul> <li>Address: <code>http://localhost:3000</code> (hardcoded)</li> <li>Automatic: Starts only when OAuth is configured</li> <li>Security: Uses PKCE (Proof Key for Code Exchange)</li> <li>Cleanup: Stops when MCPOmni Connect exits</li> </ul>"},{"location":"configuration/authentication/#example-output","title":"Example Output","text":"<pre><code>&gt; mcpomni_connect\n\ud83d\udda5\ufe0f  Started callback server on http://localhost:3000\n\ud83d\udd10 Opening browser for OAuth authentication...\n\u2705 OAuth authentication successful\n\ud83d\ude80 MCPOmni Connect - Universal Gateway to MCP Servers\nConnected to 1 MCP server: oauth-server\n</code></pre>"},{"location":"configuration/authentication/#bearer-token-authentication","title":"Bearer Token Authentication","text":"<p>Simple token-based authentication using HTTP headers.</p>"},{"location":"configuration/authentication/#configuration_1","title":"Configuration","text":"<pre><code>{\n    \"api-server\": {\n        \"transport_type\": \"streamable_http\",\n        \"url\": \"https://api.example.com/mcp\",\n        \"headers\": {\n            \"Authorization\": \"Bearer your-token-here\"\n        }\n    }\n}\n</code></pre>"},{"location":"configuration/authentication/#token-types","title":"Token Types","text":"API KeyJWT TokenCustom Token <pre><code>{\n    \"headers\": {\n        \"Authorization\": \"Bearer sk-1234567890abcdef\"\n    }\n}\n</code></pre> <pre><code>{\n    \"headers\": {\n        \"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\"\n    }\n}\n</code></pre> <pre><code>{\n    \"headers\": {\n        \"Authorization\": \"Bearer custom-token-format\"\n    }\n}\n</code></pre>"},{"location":"configuration/authentication/#token-management","title":"Token Management","text":"<ul> <li>Rotation: Manually update tokens when they expire</li> <li>Storage: Stored securely in configuration</li> <li>Scope: Ensure tokens have appropriate permissions</li> </ul>"},{"location":"configuration/authentication/#custom-headers-authentication","title":"Custom Headers Authentication","text":"<p>Flexible authentication using custom HTTP headers.</p>"},{"location":"configuration/authentication/#basic-custom-headers","title":"Basic Custom Headers","text":"<pre><code>{\n    \"custom-server\": {\n        \"transport_type\": \"streamable_http\",\n        \"url\": \"https://internal.api.com/mcp\",\n        \"headers\": {\n            \"X-API-Key\": \"your-api-key\",\n            \"X-Client-ID\": \"mcpomni-connect\",\n            \"X-Service-Version\": \"v1\"\n        }\n    }\n}\n</code></pre>"},{"location":"configuration/authentication/#advanced-authentication-schemes","title":"Advanced Authentication Schemes","text":"API Key + SecretCustom AuthorizationCertificate-Based <pre><code>{\n    \"headers\": {\n        \"X-API-Key\": \"public-key-123\",\n        \"X-API-Secret\": \"secret-key-456\",\n        \"X-Timestamp\": \"2024-01-15T10:30:00Z\"\n    }\n}\n</code></pre> <pre><code>{\n    \"headers\": {\n        \"Authorization\": \"CustomScheme key=value, signature=abc123\",\n        \"X-Request-ID\": \"unique-request-id\"\n    }\n}\n</code></pre> <pre><code>{\n    \"headers\": {\n        \"X-Client-Certificate\": \"cert-fingerprint\",\n        \"X-Certificate-Subject\": \"CN=client.example.com\"\n    }\n}\n</code></pre>"},{"location":"configuration/authentication/#sse-authentication","title":"SSE Authentication","text":"<p>Server-Sent Events with authentication headers.</p>"},{"location":"configuration/authentication/#configuration_2","title":"Configuration","text":"<pre><code>{\n    \"sse-server\": {\n        \"transport_type\": \"sse\",\n        \"url\": \"https://stream.api.com/sse\",\n        \"headers\": {\n            \"Authorization\": \"Bearer sse-token-123\",\n            \"X-Stream-Type\": \"mcp-events\"\n        },\n        \"sse_read_timeout\": 120\n    }\n}\n</code></pre>"},{"location":"configuration/authentication/#sse-specific-considerations","title":"SSE-Specific Considerations","text":"<ul> <li>Long-lived Connections: Ensure tokens don't expire during connection</li> <li>Reconnection: Handle authentication on reconnect</li> <li>Event Filtering: Use headers to specify event types</li> </ul>"},{"location":"configuration/authentication/#no-authentication","title":"No Authentication","text":"<p>For local or trusted servers that don't require authentication.</p>"},{"location":"configuration/authentication/#configuration_3","title":"Configuration","text":"<pre><code>{\n    \"local-server\": {\n        \"transport_type\": \"stdio\",\n        \"command\": \"uvx\",\n        \"args\": [\"mcp-server-filesystem\", \"/tmp\"]\n    }\n}\n</code></pre> <pre><code>{\n    \"public-server\": {\n        \"transport_type\": \"streamable_http\",\n        \"url\": \"http://localhost:8080/mcp\"\n    }\n}\n</code></pre>"},{"location":"configuration/authentication/#environment-variables-for-secrets","title":"Environment Variables for Secrets","text":"<p>Store sensitive authentication data in environment variables.</p>"},{"location":"configuration/authentication/#env-file","title":".env File","text":".env<pre><code># API Keys\nOPENAI_API_KEY=sk-1234567890abcdef\nDATABASE_API_KEY=db-token-xyz789\nNOTIFICATION_SECRET=notify-secret-abc\n\n# OAuth Credentials\nOAUTH_CLIENT_ID=your-oauth-client-id\nOAUTH_CLIENT_SECRET=your-oauth-client-secret\n\n# Custom Authentication\nCUSTOM_API_KEY=custom-key-123\nCUSTOM_SECRET=custom-secret-456\n</code></pre>"},{"location":"configuration/authentication/#reference-in-configuration","title":"Reference in Configuration","text":"<pre><code>{\n    \"api-server\": {\n        \"transport_type\": \"streamable_http\",\n        \"url\": \"https://api.example.com/mcp\",\n        \"headers\": {\n            \"Authorization\": \"Bearer ${OPENAI_API_KEY}\",\n            \"X-Custom-Key\": \"${CUSTOM_API_KEY}\"\n        }\n    }\n}\n</code></pre>"},{"location":"configuration/authentication/#security-best-practices","title":"Security Best Practices","text":""},{"location":"configuration/authentication/#token-security","title":"Token Security","text":"<p>Token Management</p> <ul> <li>Never commit tokens to version control</li> <li>Use environment variables for sensitive data</li> <li>Rotate tokens regularly according to your security policy</li> <li>Use minimal permissions for each token</li> <li>Monitor token usage for suspicious activity</li> </ul>"},{"location":"configuration/authentication/#network-security","title":"Network Security","text":"<ul> <li>Use HTTPS for all remote connections</li> <li>Validate certificates in production</li> <li>Use VPN/private networks when possible</li> <li>Implement rate limiting to prevent abuse</li> </ul>"},{"location":"configuration/authentication/#configuration-security","title":"Configuration Security","text":"\u2705 Secure Configuration<pre><code>{\n    \"server\": {\n        \"transport_type\": \"streamable_http\",\n        \"url\": \"https://secure-api.example.com/mcp\",\n        \"headers\": {\n            \"Authorization\": \"Bearer ${API_TOKEN}\"\n        },\n        \"timeout\": 30\n    }\n}\n</code></pre> \u274c Insecure Configuration<pre><code>{\n    \"server\": {\n        \"transport_type\": \"streamable_http\",\n        \"url\": \"http://api.example.com/mcp\",\n        \"headers\": {\n            \"Authorization\": \"Bearer hardcoded-token-123\"\n        }\n    }\n}\n</code></pre>"},{"location":"configuration/authentication/#troubleshooting-authentication","title":"Troubleshooting Authentication","text":""},{"location":"configuration/authentication/#common-issues","title":"Common Issues","text":"<p>OAuth Server Error</p> <p>Error: <code>Failed to start OAuth callback server</code></p> <p>Solutions: - Check if port 3000 is available - Stop other applications using port 3000 - Ensure firewall allows localhost connections</p> <p>Invalid Token</p> <p>Error: <code>401 Unauthorized</code></p> <p>Solutions: - Verify token is correct and not expired - Check token has required permissions - Ensure token format matches server expectations</p> <p>Missing Headers</p> <p>Error: <code>Authentication headers missing</code></p> <p>Solutions: - Verify all required headers are configured - Check header names match server requirements - Ensure environment variables are loaded</p>"},{"location":"configuration/authentication/#debug-authentication","title":"Debug Authentication","text":"<pre><code># Enable debug mode for detailed auth logging\n/debug\n\n# Check current server connections\n/connections\n\n# Refresh server capabilities (re-authenticates)\n/refresh\n</code></pre>"},{"location":"configuration/authentication/#testing-authentication","title":"Testing Authentication","text":"<pre><code># Test with curl before configuring\ncurl -H \"Authorization: Bearer your-token\" \\\n     -H \"Content-Type: application/json\" \\\n     https://api.example.com/mcp/health\n\n# Test OAuth flow manually\n# Visit the OAuth authorization URL in browser\n</code></pre>"},{"location":"configuration/authentication/#authentication-examples-by-provider","title":"Authentication Examples by Provider","text":""},{"location":"configuration/authentication/#github-api","title":"GitHub API","text":"<pre><code>{\n    \"github\": {\n        \"transport_type\": \"streamable_http\",\n        \"url\": \"https://api.github.com/mcp\",\n        \"headers\": {\n            \"Authorization\": \"Bearer ${GITHUB_TOKEN}\",\n            \"Accept\": \"application/vnd.github.v3+json\",\n            \"User-Agent\": \"MCPOmni-Connect\"\n        }\n    }\n}\n</code></pre>"},{"location":"configuration/authentication/#aws-services","title":"AWS Services","text":"<pre><code>{\n    \"aws-service\": {\n        \"transport_type\": \"streamable_http\",\n        \"url\": \"https://service.us-east-1.amazonaws.com/mcp\",\n        \"headers\": {\n            \"Authorization\": \"AWS4-HMAC-SHA256 ${AWS_SIGNATURE}\",\n            \"X-Amz-Date\": \"${TIMESTAMP}\",\n            \"X-Amz-Security-Token\": \"${AWS_SESSION_TOKEN}\"\n        }\n    }\n}\n</code></pre>"},{"location":"configuration/authentication/#google-cloud","title":"Google Cloud","text":"<pre><code>{\n    \"gcp-service\": {\n        \"transport_type\": \"streamable_http\",\n        \"url\": \"https://service.googleapis.com/mcp\",\n        \"headers\": {\n            \"Authorization\": \"Bearer ${GCP_ACCESS_TOKEN}\",\n            \"Content-Type\": \"application/json\"\n        }\n    }\n}\n</code></pre> <p>Next: Troubleshooting \u2192 </p>"},{"location":"configuration/configuration-guide/","title":"Configuration Guide","text":"<p>MCPOmni Connect uses two separate configuration files for different purposes. Understanding this separation is crucial for proper setup.</p> <p>Configuration Files Overview</p> <ul> <li><code>.env</code> \u2192 Environment variables (API keys, Redis settings)</li> <li><code>servers_config.json</code> \u2192 Application settings (LLM config, MCP servers, agent settings)</li> </ul>"},{"location":"configuration/configuration-guide/#configuration-files","title":"Configuration Files","text":""},{"location":"configuration/configuration-guide/#1-env-file-environment-variables","title":"1. <code>.env</code> File - Environment Variables","text":"<p>Contains sensitive information like API keys and optional settings:</p> .env<pre><code># Required: Your LLM provider API key\nLLM_API_KEY=your_api_key_here\n\n# Optional: Redis configuration (for persistent memory)\nREDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_DB=0\nREDIS_PASSWORD=your_redis_password  # if password protected\n\n# Optional: Custom settings\nDEBUG=false\nLOG_LEVEL=INFO\n</code></pre> <p>Security</p> <ul> <li>Never commit your <code>.env</code> file to version control</li> <li>Keep your API keys secure and rotate them regularly</li> <li>Use environment-specific <code>.env</code> files for different deployments</li> </ul>"},{"location":"configuration/configuration-guide/#2-servers_configjson-application-configuration","title":"2. <code>servers_config.json</code> - Application Configuration","text":"<p>Contains application settings, LLM configuration, and MCP server connections:</p> servers_config.json<pre><code>{\n    \"AgentConfig\": {\n        \"tool_call_timeout\": 30,\n        \"max_steps\": 15,\n        \"request_limit\": 1000,\n        \"total_tokens_limit\": 100000\n    },\n    \"LLM\": {\n        \"provider\": \"openai\",\n        \"model\": \"gpt-4o-mini\",\n        \"temperature\": 0.5,\n        \"max_tokens\": 5000,\n        \"max_context_length\": 30000,\n        \"top_p\": 0.7\n    },\n    \"mcpServers\": {\n        \"your-server-name\": {\n            \"transport_type\": \"stdio\",\n            \"command\": \"uvx\",\n            \"args\": [\"mcp-server-package\"]\n        }\n    }\n}\n</code></pre>"},{"location":"configuration/configuration-guide/#agent-configuration","title":"Agent Configuration","text":"<p>Configure the behavior of MCPOmni Connect's agent system:</p> AgentConfig section<pre><code>{\n    \"AgentConfig\": {\n        \"tool_call_timeout\": 30,        // Tool execution timeout (seconds)\n        \"max_steps\": 15,                // Maximum agent steps per task\n        \"request_limit\": 1000,          // Maximum LLM requests per session\n        \"total_tokens_limit\": 100000    // Maximum tokens per session\n    }\n}\n</code></pre>"},{"location":"configuration/configuration-guide/#agent-configuration-options","title":"Agent Configuration Options","text":"Setting Description Default Range <code>tool_call_timeout</code> Seconds before tool execution times out 30 5-300 <code>max_steps</code> Maximum reasoning steps per task 15 1-50 <code>request_limit</code> Maximum LLM API calls per session 1000 10-10000 <code>total_tokens_limit</code> Maximum tokens consumed per session 100000 1000-1000000"},{"location":"configuration/configuration-guide/#llm-configuration","title":"LLM Configuration","text":"<p>Configure your AI model provider and settings:</p> LLM section<pre><code>{\n    \"LLM\": {\n        \"provider\": \"openai\",           // Provider name\n        \"model\": \"gpt-4o-mini\",        // Model identifier\n        \"temperature\": 0.5,             // Creativity level (0-1)\n        \"max_tokens\": 5000,            // Max response length\n        \"max_context_length\": 30000,   // Context window size\n        \"top_p\": 0.7                   // Nucleus sampling\n    }\n}\n</code></pre>"},{"location":"configuration/configuration-guide/#llm-configuration-options","title":"LLM Configuration Options","text":"Setting Description Typical Range <code>provider</code> LLM provider (openai, anthropic, etc.) See LLM Providers <code>model</code> Specific model name Provider-specific <code>temperature</code> Response creativity/randomness 0.0 (deterministic) - 1.0 (creative) <code>max_tokens</code> Maximum response length 100 - 8000 <code>max_context_length</code> Context window size Model-dependent <code>top_p</code> Nucleus sampling parameter 0.1 - 1.0"},{"location":"configuration/configuration-guide/#mcp-server-configuration","title":"MCP Server Configuration","text":"<p>Configure connections to MCP servers using different transport types:</p>"},{"location":"configuration/configuration-guide/#basic-structure","title":"Basic Structure","text":"mcpServers section<pre><code>{\n    \"mcpServers\": {\n        \"server-name\": {\n            \"transport_type\": \"stdio|sse|streamable_http\",\n            // Additional configuration depends on transport type\n        }\n    }\n}\n</code></pre>"},{"location":"configuration/configuration-guide/#configuration-by-transport-type","title":"Configuration by Transport Type","text":"stdiossestreamable_http <pre><code>{\n    \"filesystem\": {\n        \"transport_type\": \"stdio\",\n        \"command\": \"uvx\",\n        \"args\": [\"mcp-server-filesystem\", \"/tmp\"]\n    }\n}\n</code></pre> <pre><code>{\n    \"sse-server\": {\n        \"transport_type\": \"sse\",\n        \"url\": \"http://localhost:3000/sse\",\n        \"headers\": {\n            \"Authorization\": \"Bearer your-token\"\n        },\n        \"timeout\": 60,\n        \"sse_read_timeout\": 120\n    }\n}\n</code></pre> <pre><code>{\n    \"http-server\": {\n        \"transport_type\": \"streamable_http\",\n        \"url\": \"http://localhost:3000/mcp\",\n        \"headers\": {\n            \"Authorization\": \"Bearer your-token\"\n        },\n        \"timeout\": 60\n    }\n}\n</code></pre>"},{"location":"configuration/configuration-guide/#complete-configuration-examples","title":"Complete Configuration Examples","text":""},{"location":"configuration/configuration-guide/#minimal-setup","title":"Minimal Setup","text":"Minimal servers_config.json<pre><code>{\n    \"LLM\": {\n        \"provider\": \"openai\",\n        \"model\": \"gpt-4o-mini\"\n    },\n    \"mcpServers\": {}\n}\n</code></pre>"},{"location":"configuration/configuration-guide/#production-setup","title":"Production Setup","text":"Production servers_config.json<pre><code>{\n    \"AgentConfig\": {\n        \"tool_call_timeout\": 60,\n        \"max_steps\": 25,\n        \"request_limit\": 5000,\n        \"total_tokens_limit\": 500000\n    },\n    \"LLM\": {\n        \"provider\": \"openai\",\n        \"model\": \"gpt-4\",\n        \"temperature\": 0.3,\n        \"max_tokens\": 4000,\n        \"max_context_length\": 100000,\n        \"top_p\": 0.8\n    },\n    \"mcpServers\": {\n        \"database\": {\n            \"transport_type\": \"streamable_http\",\n            \"url\": \"https://db-api.company.com/mcp\",\n            \"headers\": {\n                \"Authorization\": \"Bearer prod-token-xyz\"\n            },\n            \"timeout\": 120\n        },\n        \"filesystem\": {\n            \"transport_type\": \"stdio\",\n            \"command\": \"uvx\",\n            \"args\": [\"mcp-server-filesystem\", \"/data\"]\n        },\n        \"notifications\": {\n            \"transport_type\": \"sse\",\n            \"url\": \"https://notify.company.com/sse\",\n            \"headers\": {\n                \"Authorization\": \"Bearer notify-token-abc\"\n            },\n            \"sse_read_timeout\": 300\n        }\n    }\n}\n</code></pre>"},{"location":"configuration/configuration-guide/#development-setup","title":"Development Setup","text":"Development servers_config.json<pre><code>{\n    \"AgentConfig\": {\n        \"tool_call_timeout\": 10,\n        \"max_steps\": 5,\n        \"request_limit\": 100,\n        \"total_tokens_limit\": 10000\n    },\n    \"LLM\": {\n        \"provider\": \"ollama\",\n        \"model\": \"llama3.1:8b\",\n        \"temperature\": 0.7,\n        \"max_tokens\": 2000,\n        \"ollama_host\": \"http://localhost:11434\"\n    },\n    \"mcpServers\": {\n        \"local-tools\": {\n            \"transport_type\": \"stdio\",\n            \"command\": \"python\",\n            \"args\": [\"local_mcp_server.py\"]\n        }\n    }\n}\n</code></pre>"},{"location":"configuration/configuration-guide/#environment-specific-configuration","title":"Environment-Specific Configuration","text":""},{"location":"configuration/configuration-guide/#using-multiple-configuration-files","title":"Using Multiple Configuration Files","text":"<p>You can use different configuration files for different environments:</p> <pre><code># Development\nmcpomni_connect --config dev.servers_config.json\n\n# Production\nmcpomni_connect --config prod.servers_config.json\n\n# Testing\nmcpomni_connect --config test.servers_config.json\n</code></pre>"},{"location":"configuration/configuration-guide/#environment-variables-in-configuration","title":"Environment Variables in Configuration","text":"<p>Reference environment variables in your JSON configuration:</p> <pre><code>{\n    \"LLM\": {\n        \"provider\": \"openai\",\n        \"model\": \"${MODEL_NAME:-gpt-4o-mini}\",\n        \"temperature\": \"${TEMPERATURE:-0.5}\"\n    }\n}\n</code></pre>"},{"location":"configuration/configuration-guide/#validation-and-testing","title":"Validation and Testing","text":""},{"location":"configuration/configuration-guide/#validate-configuration","title":"Validate Configuration","text":"<pre><code># Test configuration without starting full application\nmcpomni_connect --validate-config\n\n# Test specific configuration file\nmcpomni_connect --config custom.json --validate-config\n</code></pre>"},{"location":"configuration/configuration-guide/#debug-configuration-issues","title":"Debug Configuration Issues","text":"<pre><code># Start with debug mode for detailed logging\nmcpomni_connect --debug\n\n# Or enable debug in your session\n/debug\n</code></pre>"},{"location":"configuration/configuration-guide/#best-practices","title":"Best Practices","text":"<p>Configuration Best Practices</p> <ol> <li>Start Simple: Begin with minimal configuration and add complexity gradually</li> <li>Use Version Control: Track your <code>servers_config.json</code> in git (but not <code>.env</code>)</li> <li>Environment Separation: Use different configs for dev/test/prod</li> <li>Regular Backups: Keep backups of working configurations</li> <li>Document Changes: Comment complex configurations for team members</li> </ol> <p>Common Mistakes</p> <ul> <li>Mixing up <code>.env</code> and <code>servers_config.json</code> purposes</li> <li>Hardcoding sensitive data in <code>servers_config.json</code></li> <li>Using incorrect transport types for your servers</li> <li>Setting unrealistic timeout values</li> <li>Forgetting to restart after configuration changes</li> </ul> <p>Next Steps: - Transport Types \u2192 - Authentication \u2192 - LLM Providers \u2192 </p>"},{"location":"configuration/llm-providers/","title":"LLM Providers","text":"<p>MCPOmni Connect uses LiteLLM to provide unified access to 100+ AI models across all major providers. This page covers configuration for each supported provider.</p>"},{"location":"configuration/llm-providers/#supported-providers","title":"Supported Providers","text":"Provider Models API Key Required Local/Remote OpenAI GPT-4, GPT-3.5, etc. Yes Remote Anthropic Claude 3.5, Claude 3 Yes Remote Google Gemini Pro, Flash Yes Remote Groq Llama, Mixtral, Gemma Yes Remote DeepSeek DeepSeek-V3, Coder Yes Remote Azure OpenAI GPT models Yes Remote OpenRouter 200+ models Yes Remote Ollama Local models No Local"},{"location":"configuration/llm-providers/#openai","title":"OpenAI","text":""},{"location":"configuration/llm-providers/#configuration","title":"Configuration","text":"<pre><code>{\n    \"LLM\": {\n        \"provider\": \"openai\",\n        \"model\": \"gpt-4o-mini\",\n        \"temperature\": 0.7,\n        \"max_tokens\": 4000,\n        \"max_context_length\": 128000,\n        \"top_p\": 0.9\n    }\n}\n</code></pre>"},{"location":"configuration/llm-providers/#available-models","title":"Available Models","text":"Model Context Length Use Case <code>gpt-4o</code> 128K Most capable, latest <code>gpt-4o-mini</code> 128K Fast, cost-effective <code>gpt-4-turbo</code> 128K High performance <code>gpt-4</code> 8K Standard GPT-4 <code>gpt-3.5-turbo</code> 16K Fast, affordable"},{"location":"configuration/llm-providers/#environment-setup","title":"Environment Setup","text":".env<pre><code>LLM_API_KEY=sk-your-openai-api-key-here\n</code></pre>"},{"location":"configuration/llm-providers/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>{\n    \"LLM\": {\n        \"provider\": \"openai\",\n        \"model\": \"gpt-4o\",\n        \"temperature\": 0.3,\n        \"max_tokens\": 8000,\n        \"max_context_length\": 128000,\n        \"top_p\": 0.8,\n        \"frequency_penalty\": 0.1,\n        \"presence_penalty\": 0.1,\n        \"stop\": [\"&lt;/end&gt;\"]\n    }\n}\n</code></pre>"},{"location":"configuration/llm-providers/#anthropic-claude","title":"Anthropic (Claude)","text":""},{"location":"configuration/llm-providers/#configuration_1","title":"Configuration","text":"<pre><code>{\n    \"LLM\": {\n        \"provider\": \"anthropic\",\n        \"model\": \"claude-3-5-sonnet-20241022\",\n        \"temperature\": 0.7,\n        \"max_tokens\": 4000,\n        \"max_context_length\": 200000,\n        \"top_p\": 0.95\n    }\n}\n</code></pre>"},{"location":"configuration/llm-providers/#available-models_1","title":"Available Models","text":"Model Context Length Strengths <code>claude-3-5-sonnet-20241022</code> 200K Best overall, coding <code>claude-3-5-haiku-20241022</code> 200K Fast, efficient <code>claude-3-opus-20240229</code> 200K Most capable (legacy) <code>claude-3-sonnet-20240229</code> 200K Balanced (legacy) <code>claude-3-haiku-20240307</code> 200K Fast (legacy)"},{"location":"configuration/llm-providers/#environment-setup_1","title":"Environment Setup","text":".env<pre><code>LLM_API_KEY=sk-ant-your-anthropic-api-key-here\n</code></pre>"},{"location":"configuration/llm-providers/#example-code-analysis-setup","title":"Example: Code Analysis Setup","text":"<pre><code>{\n    \"LLM\": {\n        \"provider\": \"anthropic\",\n        \"model\": \"claude-3-5-sonnet-20241022\",\n        \"temperature\": 0.1,\n        \"max_tokens\": 8000,\n        \"max_context_length\": 200000,\n        \"top_p\": 0.9\n    }\n}\n</code></pre>"},{"location":"configuration/llm-providers/#google-gemini","title":"Google (Gemini)","text":""},{"location":"configuration/llm-providers/#configuration_2","title":"Configuration","text":"<pre><code>{\n    \"LLM\": {\n        \"provider\": \"google\",\n        \"model\": \"gemini-1.5-pro\",\n        \"temperature\": 0.7,\n        \"max_tokens\": 4000,\n        \"max_context_length\": 1000000,\n        \"top_p\": 0.9\n    }\n}\n</code></pre>"},{"location":"configuration/llm-providers/#available-models_2","title":"Available Models","text":"Model Context Length Strengths <code>gemini-1.5-pro</code> 1M Largest context window <code>gemini-1.5-flash</code> 1M Fast, efficient <code>gemini-pro</code> 32K Standard model"},{"location":"configuration/llm-providers/#environment-setup_2","title":"Environment Setup","text":".env<pre><code>LLM_API_KEY=your-google-api-key-here\n</code></pre>"},{"location":"configuration/llm-providers/#long-context-configuration","title":"Long Context Configuration","text":"<pre><code>{\n    \"LLM\": {\n        \"provider\": \"google\",\n        \"model\": \"gemini-1.5-pro\",\n        \"temperature\": 0.3,\n        \"max_tokens\": 8000,\n        \"max_context_length\": 1000000,\n        \"top_p\": 0.8\n    }\n}\n</code></pre>"},{"location":"configuration/llm-providers/#groq-fast-inference","title":"Groq (Fast Inference)","text":""},{"location":"configuration/llm-providers/#configuration_3","title":"Configuration","text":"<pre><code>{\n    \"LLM\": {\n        \"provider\": \"groq\",\n        \"model\": \"llama-3.1-8b-instant\",\n        \"temperature\": 0.5,\n        \"max_tokens\": 2000,\n        \"max_context_length\": 8000,\n        \"top_p\": 0.9\n    }\n}\n</code></pre>"},{"location":"configuration/llm-providers/#available-models_3","title":"Available Models","text":"Model Context Length Speed <code>llama-3.1-8b-instant</code> 8K Very Fast <code>llama-3.1-70b-versatile</code> 8K Fast <code>mixtral-8x7b-32768</code> 32K Fast <code>gemma-7b-it</code> 8K Fast"},{"location":"configuration/llm-providers/#environment-setup_3","title":"Environment Setup","text":".env<pre><code>LLM_API_KEY=gsk_your-groq-api-key-here\n</code></pre>"},{"location":"configuration/llm-providers/#high-speed-configuration","title":"High-Speed Configuration","text":"<pre><code>{\n    \"LLM\": {\n        \"provider\": \"groq\",\n        \"model\": \"llama-3.1-8b-instant\",\n        \"temperature\": 0.1,\n        \"max_tokens\": 1000,\n        \"max_context_length\": 8000,\n        \"top_p\": 0.8\n    }\n}\n</code></pre>"},{"location":"configuration/llm-providers/#deepseek","title":"DeepSeek","text":""},{"location":"configuration/llm-providers/#configuration_4","title":"Configuration","text":"<pre><code>{\n    \"LLM\": {\n        \"provider\": \"deepseek\",\n        \"model\": \"deepseek-chat\",\n        \"temperature\": 0.5,\n        \"max_tokens\": 4000,\n        \"max_context_length\": 32000,\n        \"top_p\": 0.8\n    }\n}\n</code></pre>"},{"location":"configuration/llm-providers/#available-models_4","title":"Available Models","text":"Model Context Length Specialization <code>deepseek-chat</code> 32K General chat <code>deepseek-coder</code> 32K Code generation <code>deepseek-reasoner</code> 32K Reasoning tasks"},{"location":"configuration/llm-providers/#environment-setup_4","title":"Environment Setup","text":".env<pre><code>LLM_API_KEY=sk-your-deepseek-api-key-here\n</code></pre>"},{"location":"configuration/llm-providers/#azure-openai","title":"Azure OpenAI","text":""},{"location":"configuration/llm-providers/#configuration_5","title":"Configuration","text":"<pre><code>{\n    \"LLM\": {\n        \"provider\": \"azure\",\n        \"model\": \"gpt-4\",\n        \"temperature\": 0.7,\n        \"max_tokens\": 4000,\n        \"max_context_length\": 8000,\n        \"top_p\": 0.9,\n        \"azure_endpoint\": \"https://your-resource.openai.azure.com\",\n        \"azure_api_version\": \"2024-02-01\",\n        \"azure_deployment\": \"your-deployment-name\"\n    }\n}\n</code></pre>"},{"location":"configuration/llm-providers/#environment-setup_5","title":"Environment Setup","text":".env<pre><code>LLM_API_KEY=your-azure-openai-api-key\nAZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com\nAZURE_OPENAI_API_VERSION=2024-02-01\n</code></pre>"},{"location":"configuration/llm-providers/#enterprise-configuration","title":"Enterprise Configuration","text":"<pre><code>{\n    \"LLM\": {\n        \"provider\": \"azure\",\n        \"model\": \"gpt-4-turbo\",\n        \"temperature\": 0.3,\n        \"max_tokens\": 8000,\n        \"max_context_length\": 128000,\n        \"top_p\": 0.8,\n        \"azure_endpoint\": \"${AZURE_OPENAI_ENDPOINT}\",\n        \"azure_api_version\": \"${AZURE_OPENAI_API_VERSION}\",\n        \"azure_deployment\": \"gpt-4-turbo-deployment\"\n    }\n}\n</code></pre>"},{"location":"configuration/llm-providers/#openrouter","title":"OpenRouter","text":"<p>Access to 200+ models through a single API.</p>"},{"location":"configuration/llm-providers/#configuration_6","title":"Configuration","text":"<pre><code>{\n    \"LLM\": {\n        \"provider\": \"openrouter\",\n        \"model\": \"anthropic/claude-3.5-sonnet\",\n        \"temperature\": 0.7,\n        \"max_tokens\": 4000,\n        \"max_context_length\": 200000,\n        \"top_p\": 0.95\n    }\n}\n</code></pre>"},{"location":"configuration/llm-providers/#popular-models","title":"Popular Models","text":"Model Provider Strengths <code>anthropic/claude-3.5-sonnet</code> Anthropic Best overall <code>openai/gpt-4o</code> OpenAI Latest GPT-4 <code>google/gemini-pro-1.5</code> Google Large context <code>meta-llama/llama-3.1-8b-instruct</code> Meta Open source <code>mistralai/mixtral-8x7b-instruct</code> Mistral Efficient"},{"location":"configuration/llm-providers/#environment-setup_6","title":"Environment Setup","text":".env<pre><code>LLM_API_KEY=sk-or-your-openrouter-api-key-here\n</code></pre>"},{"location":"configuration/llm-providers/#ollama-local-models","title":"Ollama (Local Models)","text":"<p>Run models locally for privacy and offline usage.</p>"},{"location":"configuration/llm-providers/#configuration_7","title":"Configuration","text":"<pre><code>{\n    \"LLM\": {\n        \"provider\": \"ollama\",\n        \"model\": \"llama3.1:8b\",\n        \"temperature\": 0.7,\n        \"max_tokens\": 4000,\n        \"max_context_length\": 8000,\n        \"top_p\": 0.9,\n        \"ollama_host\": \"http://localhost:11434\"\n    }\n}\n</code></pre>"},{"location":"configuration/llm-providers/#popular-local-models","title":"Popular Local Models","text":"Model Size Use Case <code>llama3.1:8b</code> 4.7GB General purpose <code>llama3.1:13b</code> 7.3GB Better quality <code>codellama:7b</code> 3.8GB Code generation <code>mistral:7b</code> 4.1GB Efficient <code>qwen2:7b</code> 4.4GB Multilingual"},{"location":"configuration/llm-providers/#setup-ollama","title":"Setup Ollama","text":"<pre><code># Install Ollama\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Pull a model\nollama pull llama3.1:8b\n\n# Start Ollama service\nollama serve\n</code></pre>"},{"location":"configuration/llm-providers/#no-api-key-required","title":"No API Key Required","text":"<pre><code>{\n    \"LLM\": {\n        \"provider\": \"ollama\",\n        \"model\": \"llama3.1:8b\",\n        \"temperature\": 0.5,\n        \"max_tokens\": 2000,\n        \"ollama_host\": \"http://localhost:11434\"\n    }\n}\n</code></pre>"},{"location":"configuration/llm-providers/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"configuration/llm-providers/#common-parameters","title":"Common Parameters","text":"Parameter Description Typical Range Default <code>temperature</code> Response creativity 0.0 - 2.0 0.7 <code>max_tokens</code> Response length limit 1 - 8000 1000 <code>top_p</code> Nucleus sampling 0.1 - 1.0 1.0 <code>frequency_penalty</code> Repetition penalty -2.0 - 2.0 0.0 <code>presence_penalty</code> Topic diversity -2.0 - 2.0 0.0"},{"location":"configuration/llm-providers/#provider-specific-parameters","title":"Provider-Specific Parameters","text":"OpenAIAnthropicGoogle <pre><code>{\n    \"stop\": [\"&lt;/end&gt;\", \"\\n\\n\"],\n    \"logit_bias\": {\"-1\": -100},\n    \"user\": \"user-123\"\n}\n</code></pre> <pre><code>{\n    \"top_k\": 40,\n    \"stop_sequences\": [\"&lt;/thinking&gt;\"]\n}\n</code></pre> <pre><code>{\n    \"candidate_count\": 1,\n    \"safety_settings\": []\n}\n</code></pre>"},{"location":"configuration/llm-providers/#model-selection-guide","title":"Model Selection Guide","text":""},{"location":"configuration/llm-providers/#by-use-case","title":"By Use Case","text":"Use Case Recommended Models General Chat GPT-4o-mini, Claude 3.5 Sonnet Code Generation Claude 3.5 Sonnet, DeepSeek Coder Long Documents Gemini 1.5 Pro, Claude 3.5 Sonnet Fast Responses Groq Llama 3.1, GPT-3.5 Turbo Cost Effective GPT-4o-mini, Groq models Privacy/Local Ollama Llama 3.1, Mistral"},{"location":"configuration/llm-providers/#by-performance","title":"By Performance","text":"Priority Models Trade-offs Quality Claude 3.5 Sonnet, GPT-4o Higher cost Speed Groq models, GPT-3.5 Lower accuracy Context Gemini 1.5 Pro Google ecosystem Cost GPT-4o-mini, DeepSeek Some capability limits"},{"location":"configuration/llm-providers/#switching-between-providers","title":"Switching Between Providers","text":"<p>You can switch providers dynamically:</p> <pre><code># Update configuration and restart\nvim servers_config.json\n\n# Or use environment variables\nexport LLM_PROVIDER=anthropic\nexport LLM_MODEL=claude-3-5-sonnet-20241022\n</code></pre>"},{"location":"configuration/llm-providers/#cost-optimization","title":"Cost Optimization","text":""},{"location":"configuration/llm-providers/#token-usage-monitoring","title":"Token Usage Monitoring","text":"<pre><code># Check current usage\n/api_stats\n\n# Set usage limits in configuration\n{\n    \"AgentConfig\": {\n        \"total_tokens_limit\": 50000,\n        \"request_limit\": 1000\n    }\n}\n</code></pre>"},{"location":"configuration/llm-providers/#cost-effective-models","title":"Cost-Effective Models","text":"<pre><code>{\n    \"LLM\": {\n        \"provider\": \"openai\",\n        \"model\": \"gpt-4o-mini\",  // Most cost-effective GPT-4 class\n        \"max_tokens\": 1000,     // Limit response length\n        \"temperature\": 0.3      // More focused responses\n    }\n}\n</code></pre>"},{"location":"configuration/llm-providers/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configuration/llm-providers/#common-issues","title":"Common Issues","text":"<p>Invalid API Key</p> <p>Error: <code>Authentication failed</code></p> <p>Solutions: - Verify API key in <code>.env</code> file - Check key has proper permissions - Ensure key is for correct provider</p> <p>Model Not Found</p> <p>Error: <code>Model not available</code></p> <p>Solutions: - Check model name spelling - Verify model availability for your account - Try alternative model</p> <p>Rate Limit</p> <p>Error: <code>Rate limit exceeded</code></p> <p>Solutions: - Reduce request frequency - Upgrade API plan - Switch to different provider</p>"},{"location":"configuration/llm-providers/#testing-configuration","title":"Testing Configuration","text":"<pre><code># Test with simple query\n&gt; Hello, can you respond?\n\n# Check model info\n/api_stats\n\n# Enable debug for detailed logs\n/debug\n</code></pre> <p>Next: Troubleshooting \u2192 </p>"},{"location":"configuration/transport-types/","title":"Transport Types","text":"<p>MCPOmni Connect supports multiple transport protocols to connect with MCP servers. Choose the right transport type based on your server's capabilities and requirements.</p>"},{"location":"configuration/transport-types/#overview","title":"Overview","text":"Transport Type Use Case Authentication Performance <code>stdio</code> Local processes None High <code>sse</code> HTTP with Server-Sent Events Bearer/Headers Medium <code>streamable_http</code> HTTP with optional OAuth OAuth/Bearer/Headers Medium"},{"location":"configuration/transport-types/#stdio-transport","title":"stdio Transport","text":"<p>Best for: Local MCP servers running as separate processes</p>"},{"location":"configuration/transport-types/#configuration","title":"Configuration","text":"<pre><code>{\n    \"server-name\": {\n        \"transport_type\": \"stdio\",\n        \"command\": \"uvx\",\n        \"args\": [\"mcp-server-package\", \"arg1\", \"arg2\"]\n    }\n}\n</code></pre>"},{"location":"configuration/transport-types/#parameters","title":"Parameters","text":"Parameter Required Description Example <code>command</code> Yes Executable command <code>\"uvx\"</code>, <code>\"python\"</code>, <code>\"node\"</code> <code>args</code> Yes Command arguments <code>[\"mcp-server-git\", \"/path/to/repo\"]</code>"},{"location":"configuration/transport-types/#examples","title":"Examples","text":""},{"location":"configuration/transport-types/#python-mcp-server","title":"Python MCP Server","text":"<pre><code>{\n    \"python-server\": {\n        \"transport_type\": \"stdio\",\n        \"command\": \"python\",\n        \"args\": [\"my_mcp_server.py\", \"--config\", \"config.json\"]\n    }\n}\n</code></pre>"},{"location":"configuration/transport-types/#nodejs-mcp-server","title":"Node.js MCP Server","text":"<pre><code>{\n    \"node-server\": {\n        \"transport_type\": \"stdio\",\n        \"command\": \"node\",\n        \"args\": [\"server.js\", \"--port\", \"3000\"]\n    }\n}\n</code></pre>"},{"location":"configuration/transport-types/#uv-package","title":"UV Package","text":"<pre><code>{\n    \"filesystem\": {\n        \"transport_type\": \"stdio\",\n        \"command\": \"uvx\",\n        \"args\": [\"mcp-server-filesystem\", \"/tmp\"]\n    }\n}\n</code></pre>"},{"location":"configuration/transport-types/#characteristics","title":"Characteristics","text":"<p>\u2705 Advantages: - No network overhead - Direct process communication - No authentication needed - Highest performance - Simple setup</p> <p>\u274c Limitations: - Local processes only - No remote server support - Process lifecycle tied to MCPOmni Connect</p>"},{"location":"configuration/transport-types/#sse-transport","title":"sse Transport","text":"<p>Best for: HTTP-based MCP servers using Server-Sent Events for real-time communication</p>"},{"location":"configuration/transport-types/#configuration_1","title":"Configuration","text":"<pre><code>{\n    \"server-name\": {\n        \"transport_type\": \"sse\",\n        \"url\": \"http://localhost:3000/sse\",\n        \"headers\": {\n            \"Authorization\": \"Bearer your-token\"\n        },\n        \"timeout\": 60,\n        \"sse_read_timeout\": 120\n    }\n}\n</code></pre>"},{"location":"configuration/transport-types/#parameters_1","title":"Parameters","text":"Parameter Required Description Default <code>url</code> Yes SSE endpoint URL - <code>headers</code> No HTTP headers <code>{}</code> <code>timeout</code> No Connection timeout (seconds) <code>60</code> <code>sse_read_timeout</code> No SSE read timeout (seconds) <code>120</code>"},{"location":"configuration/transport-types/#examples_1","title":"Examples","text":""},{"location":"configuration/transport-types/#basic-sse-server","title":"Basic SSE Server","text":"<pre><code>{\n    \"sse-server\": {\n        \"transport_type\": \"sse\",\n        \"url\": \"http://localhost:4010/sse\"\n    }\n}\n</code></pre>"},{"location":"configuration/transport-types/#sse-with-authentication","title":"SSE with Authentication","text":"<pre><code>{\n    \"secure-sse\": {\n        \"transport_type\": \"sse\",\n        \"url\": \"https://api.example.com/mcp/sse\",\n        \"headers\": {\n            \"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\",\n            \"X-API-Version\": \"v1\"\n        },\n        \"timeout\": 30,\n        \"sse_read_timeout\": 180\n    }\n}\n</code></pre>"},{"location":"configuration/transport-types/#sse-with-custom-headers","title":"SSE with Custom Headers","text":"<pre><code>{\n    \"custom-sse\": {\n        \"transport_type\": \"sse\",\n        \"url\": \"http://internal.company.com/sse\",\n        \"headers\": {\n            \"X-Service-Key\": \"service-123\",\n            \"X-Client-ID\": \"mcpomni-connect\",\n            \"Content-Type\": \"application/json\"\n        }\n    }\n}\n</code></pre>"},{"location":"configuration/transport-types/#characteristics_1","title":"Characteristics","text":"<p>\u2705 Advantages: - Real-time bidirectional communication - HTTP-based (firewall friendly) - Supports authentication headers - Good for streaming data - Network resilient</p> <p>\u274c Limitations: - Requires SSE-compatible server - Network latency - More complex than stdio</p>"},{"location":"configuration/transport-types/#streamable_http-transport","title":"streamable_http Transport","text":"<p>Best for: HTTP-based MCP servers with flexible authentication options</p>"},{"location":"configuration/transport-types/#configuration-options","title":"Configuration Options","text":""},{"location":"configuration/transport-types/#without-oauth-bearer-token","title":"Without OAuth (Bearer Token)","text":"<pre><code>{\n    \"server-name\": {\n        \"transport_type\": \"streamable_http\",\n        \"url\": \"http://localhost:8080/mcp\",\n        \"headers\": {\n            \"Authorization\": \"Bearer your-token\"\n        },\n        \"timeout\": 60\n    }\n}\n</code></pre>"},{"location":"configuration/transport-types/#with-oauth","title":"With OAuth","text":"<pre><code>{\n    \"server-name\": {\n        \"transport_type\": \"streamable_http\",\n        \"auth\": {\n            \"method\": \"oauth\"\n        },\n        \"url\": \"http://localhost:8080/mcp\"\n    }\n}\n</code></pre>"},{"location":"configuration/transport-types/#parameters_2","title":"Parameters","text":"Parameter Required Description Default <code>url</code> Yes HTTP endpoint URL - <code>auth</code> No Authentication config - <code>headers</code> No HTTP headers <code>{}</code> <code>timeout</code> No Request timeout (seconds) <code>60</code>"},{"location":"configuration/transport-types/#oauth-configuration","title":"OAuth Configuration","text":"<p>When using OAuth, MCPOmni Connect automatically starts a callback server:</p> <p>OAuth Callback Server</p> <ul> <li>Automatically starts on <code>http://localhost:3000</code></li> <li>Cannot be changed (hardcoded for security)</li> <li>Only starts when <code>\"auth\": {\"method\": \"oauth\"}</code> is configured</li> <li>Stops when MCPOmni Connect shuts down</li> </ul>"},{"location":"configuration/transport-types/#examples_2","title":"Examples","text":""},{"location":"configuration/transport-types/#basic-http-server","title":"Basic HTTP Server","text":"<pre><code>{\n    \"http-server\": {\n        \"transport_type\": \"streamable_http\",\n        \"url\": \"http://localhost:8080/mcp\"\n    }\n}\n</code></pre>"},{"location":"configuration/transport-types/#http-with-bearer-token","title":"HTTP with Bearer Token","text":"<pre><code>{\n    \"api-server\": {\n        \"transport_type\": \"streamable_http\",\n        \"url\": \"https://api.example.com/mcp\",\n        \"headers\": {\n            \"Authorization\": \"Bearer sk-1234567890abcdef\",\n            \"User-Agent\": \"MCPOmni-Connect/1.0\"\n        },\n        \"timeout\": 120\n    }\n}\n</code></pre>"},{"location":"configuration/transport-types/#http-with-oauth","title":"HTTP with OAuth","text":"<pre><code>{\n    \"oauth-server\": {\n        \"transport_type\": \"streamable_http\",\n        \"auth\": {\n            \"method\": \"oauth\"\n        },\n        \"url\": \"https://secure-api.example.com/mcp\",\n        \"timeout\": 90\n    }\n}\n</code></pre>"},{"location":"configuration/transport-types/#http-with-custom-authentication","title":"HTTP with Custom Authentication","text":"<pre><code>{\n    \"custom-auth\": {\n        \"transport_type\": \"streamable_http\",\n        \"url\": \"http://internal.api/mcp\",\n        \"headers\": {\n            \"X-API-Key\": \"your-api-key\",\n            \"X-Client-Certificate\": \"cert-fingerprint\",\n            \"Authorization\": \"Custom-Scheme token-data\"\n        }\n    }\n}\n</code></pre>"},{"location":"configuration/transport-types/#characteristics_2","title":"Characteristics","text":"<p>\u2705 Advantages: - Most flexible authentication - HTTP-based (standard protocol) - Supports OAuth and custom auth - Good for enterprise APIs - RESTful communication</p> <p>\u274c Limitations: - Network latency - Requires HTTP-compatible server - More configuration complexity</p>"},{"location":"configuration/transport-types/#choosing-the-right-transport","title":"Choosing the Right Transport","text":""},{"location":"configuration/transport-types/#decision-matrix","title":"Decision Matrix","text":"<pre><code>flowchart TD\n    A[Choose Transport Type] --&gt; B{Local or Remote?}\n    B --&gt;|Local| C[Use stdio]\n    B --&gt;|Remote| D{Authentication Required?}\n    D --&gt;|No| E[Use streamable_http]\n    D --&gt;|Yes| F{OAuth or Token?}\n    F --&gt;|OAuth| G[Use streamable_http with auth]\n    F --&gt;|Token/Headers| H{Real-time Streaming?}\n    H --&gt;|Yes| I[Use sse]\n    H --&gt;|No| J[Use streamable_http with headers]</code></pre>"},{"location":"configuration/transport-types/#recommendations","title":"Recommendations","text":"Scenario Recommended Transport Reason Local development <code>stdio</code> Fastest, simplest setup Enterprise API <code>streamable_http</code> + OAuth Secure, standardized Real-time notifications <code>sse</code> Built for streaming Simple remote API <code>streamable_http</code> + Bearer Balance of features and simplicity Microservices <code>streamable_http</code> HTTP-native, scalable"},{"location":"configuration/transport-types/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"configuration/transport-types/#stdio-issues","title":"stdio Issues","text":"<p>Command Not Found</p> <p>Error: <code>FileNotFoundError: [Errno 2] No such file or directory: 'uvx'</code></p> <p>Solution: Install the required command or use full path: <pre><code>{\n    \"command\": \"/usr/local/bin/uvx\"\n    // or\n    \"command\": \"python3\"\n}\n</code></pre></p>"},{"location":"configuration/transport-types/#sse-issues","title":"SSE Issues","text":"<p>SSE Connection Failed</p> <p>Error: <code>Failed to connect to SSE endpoint</code></p> <p>Solutions: 1. Verify the URL is correct 2. Check if server supports SSE 3. Ensure proper headers are set 4. Increase timeout values</p>"},{"location":"configuration/transport-types/#http-issues","title":"HTTP Issues","text":"<p>OAuth Server Already Running</p> <p>Error: <code>Address already in use: localhost:3000</code></p> <p>Solutions: 1. Stop other processes using port 3000 2. Only one OAuth server runs at a time 3. Remove OAuth config if not needed</p>"},{"location":"configuration/transport-types/#performance-considerations","title":"Performance Considerations","text":""},{"location":"configuration/transport-types/#latency-comparison","title":"Latency Comparison","text":"Transport Local Latency Remote Latency Throughput <code>stdio</code> ~1ms N/A Very High <code>sse</code> ~5-10ms ~50-200ms High <code>streamable_http</code> ~5-10ms ~50-200ms Medium-High"},{"location":"configuration/transport-types/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Use stdio for local servers when possible</li> <li>Tune timeout values based on your network</li> <li>Minimize headers to reduce overhead</li> <li>Use connection pooling for multiple HTTP calls</li> <li>Consider geographic proximity for remote servers</li> </ol> <p>Next: Authentication \u2192 </p>"},{"location":"configuration/troubleshooting/","title":"Troubleshooting","text":"<p>This guide covers common issues and their solutions when using MCPOmni Connect.</p>"},{"location":"configuration/troubleshooting/#configuration-issues","title":"Configuration Issues","text":""},{"location":"configuration/troubleshooting/#failed-to-connect-to-server-session-terminated","title":"\"Failed to connect to server: Session terminated\"","text":"<p>This is the most common connection issue with several possible causes:</p> <p>Session Terminated Error</p> <p>Error: <code>Failed to connect to server 'server-name': Session terminated</code></p> <p>Possible Causes &amp; Solutions:</p>"},{"location":"configuration/troubleshooting/#1-wrong-transport-type","title":"1. Wrong Transport Type","text":"<pre><code>Problem: Your server expects 'stdio' but you configured 'streamable_http'\n</code></pre> <p>Solution: Check your server's documentation for the correct transport type</p> \u2705 Correct stdio configuration<pre><code>{\n    \"filesystem\": {\n        \"transport_type\": \"stdio\",\n        \"command\": \"uvx\",\n        \"args\": [\"mcp-server-filesystem\", \"/tmp\"]\n    }\n}\n</code></pre>"},{"location":"configuration/troubleshooting/#2-oauth-configuration-mismatch","title":"2. OAuth Configuration Mismatch","text":"<pre><code>Problem: Your server doesn't support OAuth but you have \"auth\": {\"method\": \"oauth\"}\n</code></pre> <p>Solution: Remove the <code>auth</code> section and use headers instead:</p> \u2705 Use Bearer token instead of OAuth<pre><code>{\n    \"api-server\": {\n        \"transport_type\": \"streamable_http\",\n        \"url\": \"http://localhost:8080/mcp\",\n        \"headers\": {\n            \"Authorization\": \"Bearer your-token\"\n        }\n    }\n}\n</code></pre>"},{"location":"configuration/troubleshooting/#3-server-not-running","title":"3. Server Not Running","text":"<pre><code>Problem: The MCP server at the specified URL is not running\n</code></pre> <p>Solution: Start your MCP server first, then connect with MCPOmni Connect</p> <pre><code># Example: Start a local MCP server\nuvx mcp-server-filesystem /tmp &amp;\n\n# Then start MCPOmni Connect\nmcpomni_connect\n</code></pre>"},{"location":"configuration/troubleshooting/#4-wrong-url-or-port","title":"4. Wrong URL or Port","text":"<pre><code>Problem: URL in config doesn't match where your server is running\n</code></pre> <p>Solution: Verify the server's actual address and port</p> <pre><code># Check if server is listening\ncurl http://localhost:8080/health\n\n# Or use netstat to see listening ports\nnetstat -tlnp | grep 8080\n</code></pre>"},{"location":"configuration/troubleshooting/#started-callback-server-on-httplocalhost3000-is-this-normal","title":"\"Started callback server on http://localhost:3000\" - Is This Normal?","text":"<p>OAuth Callback Server</p> <p>Yes, this is completely normal when: - You have <code>\"auth\": {\"method\": \"oauth\"}</code> in any server configuration - The OAuth server handles authentication tokens automatically - You cannot and should not try to change this address</p> <p>If you don't want the OAuth server: - Remove <code>\"auth\": {\"method\": \"oauth\"}</code> from all server configurations - Use alternative authentication methods like Bearer tokens</p> \u274c Causes OAuth server to start<pre><code>{\n    \"server\": {\n        \"auth\": {\"method\": \"oauth\"},\n        \"url\": \"http://example.com/mcp\"\n    }\n}\n</code></pre> \u2705 No OAuth server needed<pre><code>{\n    \"server\": {\n        \"url\": \"http://example.com/mcp\",\n        \"headers\": {\n            \"Authorization\": \"Bearer token\"\n        }\n    }\n}\n</code></pre>"},{"location":"configuration/troubleshooting/#authentication-issues","title":"Authentication Issues","text":""},{"location":"configuration/troubleshooting/#invalid-api-key-errors","title":"Invalid API Key Errors","text":"<p>API Key Problems</p> <p>Error: <code>Invalid API key</code> or <code>Authentication failed</code></p>"},{"location":"configuration/troubleshooting/#openai-api-key-issues","title":"OpenAI API Key Issues","text":"<pre><code># Check your .env file\ncat .env | grep LLM_API_KEY\n\n# Verify key format (should start with sk-)\nLLM_API_KEY=sk-your-key-here\n\n# Test key directly\ncurl -H \"Authorization: Bearer $LLM_API_KEY\" \\\n     https://api.openai.com/v1/models\n</code></pre>"},{"location":"configuration/troubleshooting/#anthropic-api-key-issues","title":"Anthropic API Key Issues","text":"<pre><code># Anthropic keys start with sk-ant-\nLLM_API_KEY=sk-ant-your-key-here\n\n# Test key\ncurl -H \"x-api-key: $LLM_API_KEY\" \\\n     https://api.anthropic.com/v1/messages\n</code></pre>"},{"location":"configuration/troubleshooting/#environment-variable-not-loaded","title":"Environment Variable Not Loaded","text":"<pre><code># Check if environment variable is loaded\necho $LLM_API_KEY\n\n# Restart MCPOmni Connect to reload .env\nmcpomni_connect\n</code></pre>"},{"location":"configuration/troubleshooting/#oauth-authentication-failures","title":"OAuth Authentication Failures","text":"<p>OAuth Issues</p> <p>Error: <code>OAuth authentication failed</code></p> <p>Solutions:</p> <ol> <li> <p>Port 3000 Already in Use <pre><code># Find what's using port 3000\nlsof -i :3000\n\n# Stop the conflicting service\nsudo kill -9 &lt;pid&gt;\n</code></pre></p> </li> <li> <p>Browser Not Opening <pre><code># Manual OAuth flow\n# Copy the authorization URL from the terminal\n# Open it manually in your browser\n</code></pre></p> </li> <li> <p>Firewall Blocking Localhost <pre><code># Allow localhost connections\nsudo ufw allow from 127.0.0.1\n</code></pre></p> </li> </ol>"},{"location":"configuration/troubleshooting/#server-configuration-issues","title":"Server Configuration Issues","text":""},{"location":"configuration/troubleshooting/#json-syntax-errors","title":"JSON Syntax Errors","text":"<p>Configuration Parsing</p> <p>Error: <code>JSON decode error</code> or <code>Invalid configuration</code></p> <p>Common JSON Mistakes:</p> \u274c Common JSON errors<pre><code>{\n    \"LLM\": {\n        \"provider\": \"openai\",\n        \"model\": \"gpt-4o-mini\"    // \u274c Trailing comma\n    },\n    \"mcpServers\": {\n        \"server\": {\n            \"url\": \"http://example.com\"    // \u274c Missing comma\n            \"timeout\": 60\n        }\n    }\n}\n</code></pre> \u2705 Correct JSON<pre><code>{\n    \"LLM\": {\n        \"provider\": \"openai\",\n        \"model\": \"gpt-4o-mini\"\n    },\n    \"mcpServers\": {\n        \"server\": {\n            \"url\": \"http://example.com\",\n            \"timeout\": 60\n        }\n    }\n}\n</code></pre> <p>Validation Tools: <pre><code># Validate JSON syntax\npython -m json.tool servers_config.json\n\n# Or use jq\njq . servers_config.json\n</code></pre></p>"},{"location":"configuration/troubleshooting/#missing-required-fields","title":"Missing Required Fields","text":"<p>Required Configuration</p> <p>Error: <code>Missing required field</code></p> <p>Check Required Fields:</p> Minimum required configuration<pre><code>{\n    \"LLM\": {\n        \"provider\": \"openai\",        // \u2705 Required\n        \"model\": \"gpt-4o-mini\"      // \u2705 Required\n    },\n    \"mcpServers\": {}                // \u2705 Required (can be empty)\n}\n</code></pre>"},{"location":"configuration/troubleshooting/#runtime-issues","title":"Runtime Issues","text":""},{"location":"configuration/troubleshooting/#tool-execution-failures","title":"Tool Execution Failures","text":"<p>Tool Errors</p> <p>Error: <code>Tool execution failed</code> or <code>Tool timeout</code></p>"},{"location":"configuration/troubleshooting/#timeout-issues","title":"Timeout Issues","text":"Increase timeout in configuration<pre><code>{\n    \"AgentConfig\": {\n        \"tool_call_timeout\": 60    // Increase from default 30\n    }\n}\n</code></pre> <pre><code># Enable debug mode for detailed logs\n/debug\n\n# Check tool availability\n/tools\n\n# Test simple command first\n/tools\n</code></pre>"},{"location":"configuration/troubleshooting/#permission-denied","title":"Permission Denied","text":"<pre><code># Check file permissions\nls -la /path/to/file\n\n# Ensure MCPOmni Connect has necessary permissions\nchmod +r /path/to/file\n</code></pre>"},{"location":"configuration/troubleshooting/#memory-issues","title":"Memory Issues","text":"<p>Redis Connection</p> <p>Error: <code>Could not connect to Redis</code></p> <p>Solutions:</p> <ol> <li> <p>Redis Not Running <pre><code># Start Redis\nsudo systemctl start redis-server\n\n# Or with Docker\ndocker run -d --name redis -p 6379:6379 redis:alpine\n</code></pre></p> </li> <li> <p>Wrong Redis Configuration .env<pre><code>REDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_DB=0\n# Add password if needed\nREDIS_PASSWORD=your-password\n</code></pre></p> </li> <li> <p>Disable Redis Memory <pre><code># Turn off memory persistence\n/memory\n</code></pre></p> </li> </ol>"},{"location":"configuration/troubleshooting/#rate-limiting","title":"Rate Limiting","text":"<p>API Limits</p> <p>Error: <code>Rate limit exceeded</code> or <code>Too many requests</code></p> <p>Solutions:</p> <ol> <li> <p>Reduce Request Frequency <pre><code>{\n    \"AgentConfig\": {\n        \"request_limit\": 100,        // Lower limit\n        \"total_tokens_limit\": 10000  // Lower token limit\n    }\n}\n</code></pre></p> </li> <li> <p>Switch to Different Provider <pre><code>{\n    \"LLM\": {\n        \"provider\": \"groq\",          // Often has higher limits\n        \"model\": \"llama-3.1-8b-instant\"\n    }\n}\n</code></pre></p> </li> <li> <p>Check Current Usage <pre><code>/api_stats\n</code></pre></p> </li> </ol>"},{"location":"configuration/troubleshooting/#network-issues","title":"Network Issues","text":""},{"location":"configuration/troubleshooting/#connection-timeouts","title":"Connection Timeouts","text":"<p>Network Timeouts</p> <p>Error: <code>Connection timeout</code> or <code>Request timeout</code></p> <p>Solutions:</p> <ol> <li> <p>Increase Timeout Values <pre><code>{\n    \"server\": {\n        \"transport_type\": \"streamable_http\",\n        \"url\": \"http://slow-server.com/mcp\",\n        \"timeout\": 120,              // Increase timeout\n        \"sse_read_timeout\": 300      // For SSE connections\n    }\n}\n</code></pre></p> </li> <li> <p>Check Network Connectivity <pre><code># Test basic connectivity\nping api.openai.com\n\n# Test HTTP connectivity\ncurl -I https://api.openai.com\n\n# Test specific server\ncurl -I http://your-server.com:8080\n</code></pre></p> </li> </ol>"},{"location":"configuration/troubleshooting/#firewall-issues","title":"Firewall Issues","text":"<p>Connection Blocked</p> <p>Error: <code>Connection refused</code> or <code>Network unreachable</code></p> <p>Solutions:</p> <ol> <li> <p>Check Firewall Rules <pre><code># Ubuntu/Debian\nsudo ufw status\n\n# Allow specific port\nsudo ufw allow 8080\n\n# Allow outbound HTTPS\nsudo ufw allow out 443\n</code></pre></p> </li> <li> <p>Corporate Firewall <pre><code># Test with proxy if needed\nexport https_proxy=http://proxy.company.com:8080\nmcpomni_connect\n</code></pre></p> </li> </ol>"},{"location":"configuration/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"configuration/troubleshooting/#slow-response-times","title":"Slow Response Times","text":"<p>Performance Problems</p> <p>Issue: MCPOmni Connect is responding slowly</p> <p>Diagnosis:</p> <ol> <li> <p>Enable Debug Mode <pre><code>/debug\n# Check where time is being spent\n</code></pre></p> </li> <li> <p>Check API Stats <pre><code>/api_stats\n# Look for high token usage or request counts\n</code></pre></p> </li> </ol> <p>Solutions:</p> <ol> <li> <p>Optimize Model Settings <pre><code>{\n    \"LLM\": {\n        \"provider\": \"groq\",              // Faster provider\n        \"model\": \"llama-3.1-8b-instant\", // Faster model\n        \"max_tokens\": 1000,              // Shorter responses\n        \"temperature\": 0.1               // More focused\n    }\n}\n</code></pre></p> </li> <li> <p>Reduce Context Length <pre><code>{\n    \"LLM\": {\n        \"max_context_length\": 8000       // Smaller context\n    }\n}\n</code></pre></p> </li> </ol>"},{"location":"configuration/troubleshooting/#high-token-usage","title":"High Token Usage","text":"<p>Token Consumption</p> <p>Issue: Running out of tokens quickly</p> <p>Solutions:</p> <ol> <li> <p>Set Token Limits <pre><code>{\n    \"AgentConfig\": {\n        \"total_tokens_limit\": 50000,     // Lower limit\n        \"max_steps\": 5                   // Fewer reasoning steps\n    }\n}\n</code></pre></p> </li> <li> <p>Use More Efficient Models <pre><code>{\n    \"LLM\": {\n        \"provider\": \"openai\",\n        \"model\": \"gpt-4o-mini\"           // Most cost-effective\n    }\n}\n</code></pre></p> </li> </ol>"},{"location":"configuration/troubleshooting/#debugging-tools","title":"Debugging Tools","text":""},{"location":"configuration/troubleshooting/#built-in-debug-features","title":"Built-in Debug Features","text":"<pre><code># Enable verbose logging\n/debug\n\n# Check system status\n/status\n\n# View current configuration\n/connections\n\n# Check API usage\n/api_stats\n\n# Refresh server connections\n/refresh\n</code></pre>"},{"location":"configuration/troubleshooting/#log-analysis","title":"Log Analysis","text":"<pre><code># Check application logs\ntail -f mcpomni_connect.log\n\n# Filter for errors\ngrep ERROR mcpomni_connect.log\n\n# Check specific server logs\ngrep \"server-name\" mcpomni_connect.log\n</code></pre>"},{"location":"configuration/troubleshooting/#environment-verification","title":"Environment Verification","text":"<pre><code># Check Python version\npython --version\n\n# Check installed packages\npip list | grep mcpomni\n\n# Verify environment variables\nenv | grep LLM_API_KEY\n\n# Test configuration file\npython -m json.tool servers_config.json\n</code></pre>"},{"location":"configuration/troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"configuration/troubleshooting/#before-asking-for-help","title":"Before Asking for Help","text":"<ol> <li>Enable Debug Mode: <code>/debug</code></li> <li>Check Logs: Look for error messages</li> <li>Verify Configuration: Validate JSON syntax</li> <li>Test Simple Cases: Try basic operations first</li> </ol>"},{"location":"configuration/troubleshooting/#information-to-include","title":"Information to Include","text":"<p>When reporting issues, include:</p> <ul> <li>MCPOmni Connect version: <code>mcpomni_connect --version</code></li> <li>Python version: <code>python --version</code></li> <li>Operating system: <code>uname -a</code> (Linux/Mac) or <code>ver</code> (Windows)</li> <li>Configuration (remove sensitive data):   <pre><code>{\n    \"LLM\": {\n        \"provider\": \"openai\",\n        \"model\": \"gpt-4o-mini\"\n    },\n    \"mcpServers\": {\n        \"server\": {\n            \"transport_type\": \"stdio\",\n            \"command\": \"uvx\",\n            \"args\": [\"mcp-server-filesystem\"]\n        }\n    }\n}\n</code></pre></li> <li>Complete error message</li> <li>Steps to reproduce</li> </ul>"},{"location":"configuration/troubleshooting/#support-channels","title":"Support Channels","text":"<ul> <li>GitHub Issues: Report bugs</li> <li>GitHub Discussions: Ask questions</li> <li>Email: abiolaadedayo1993@gmail.com</li> </ul> <p>Next: User Guide \u2192 </p>"},{"location":"development/contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to MCPOmni Connect! This project thrives on community contributions.</p>"},{"location":"development/contributing/#quick-links","title":"Quick Links","text":"<ul> <li>Main Contributing Guide: See CONTRIBUTING.md in the project root for complete guidelines</li> <li>Issue Tracker: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> </ul>"},{"location":"development/contributing/#ways-to-contribute","title":"Ways to Contribute","text":""},{"location":"development/contributing/#bug-reports","title":"\ud83d\udc1b Bug Reports","text":"<p>Found a bug? Help us fix it:</p> <ol> <li>Check existing issues first</li> <li>Create a new issue with:</li> <li>Clear description of the problem</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Environment details (OS, Python version, etc.)</li> </ol>"},{"location":"development/contributing/#feature-requests","title":"\ud83d\udca1 Feature Requests","text":"<p>Have an idea for improvement?</p> <ol> <li>Check discussions for similar ideas</li> <li>Open a new discussion or issue describing:</li> <li>The problem your feature would solve</li> <li>Proposed solution</li> <li>Alternative solutions considered</li> </ol>"},{"location":"development/contributing/#documentation","title":"\ud83d\udcdd Documentation","text":"<p>Help improve our documentation:</p> <ul> <li>Fix typos or unclear instructions</li> <li>Add examples or use cases</li> <li>Improve API documentation</li> <li>Translate documentation (future)</li> </ul>"},{"location":"development/contributing/#code-contributions","title":"\ud83d\udd27 Code Contributions","text":"<p>Ready to code? Follow the development setup guide.</p>"},{"location":"development/contributing/#documentation-contributions","title":"Documentation Contributions","text":""},{"location":"development/contributing/#working-with-mkdocs","title":"Working with MkDocs","text":"<p>This documentation site uses MkDocs with Material theme:</p> <pre><code># Install documentation dependencies\npip install -r docs/requirements.txt\n\n# Serve documentation locally\nmkdocs serve\n\n# Build documentation\nmkdocs build\n</code></pre>"},{"location":"development/contributing/#documentation-structure","title":"Documentation Structure","text":"<pre><code>docs/\n\u251c\u2500\u2500 index.md              # Homepage\n\u251c\u2500\u2500 getting-started/       # Installation and quick start\n\u251c\u2500\u2500 configuration/         # Configuration guides\n\u251c\u2500\u2500 user-guide/           # Usage instructions\n\u251c\u2500\u2500 features/             # Feature deep-dives\n\u251c\u2500\u2500 advanced/             # Advanced topics\n\u2514\u2500\u2500 development/          # Development guides\n</code></pre>"},{"location":"development/contributing/#writing-guidelines","title":"Writing Guidelines","text":"<p>Documentation Style</p> <ul> <li>Use clear, concise language</li> <li>Include practical examples</li> <li>Add code snippets where helpful</li> <li>Use admonitions (tips, warnings, etc.) for important information</li> <li>Test all code examples before committing</li> </ul>"},{"location":"development/contributing/#development-process","title":"Development Process","text":""},{"location":"development/contributing/#1-fork-and-clone","title":"1. Fork and Clone","text":"<pre><code>git clone https://github.com/YOUR_USERNAME/mcp_omni_connect.git\ncd mcp_omni_connect\n</code></pre>"},{"location":"development/contributing/#2-set-up-development-environment","title":"2. Set Up Development Environment","text":"<pre><code># Install UV (if not already installed)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Install dependencies\nuv sync\n\n# Install pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"development/contributing/#3-create-feature-branch","title":"3. Create Feature Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"development/contributing/#4-make-changes","title":"4. Make Changes","text":"<ul> <li>Write code following the project style</li> <li>Add tests for new functionality</li> <li>Update documentation as needed</li> <li>Ensure all tests pass</li> </ul>"},{"location":"development/contributing/#5-submit-pull-request","title":"5. Submit Pull Request","text":"<ol> <li>Push your branch to your fork</li> <li>Create a pull request with:</li> <li>Clear description of changes</li> <li>Link to related issues</li> <li>Screenshots (if UI changes)</li> </ol>"},{"location":"development/contributing/#code-standards","title":"Code Standards","text":""},{"location":"development/contributing/#python-code","title":"Python Code","text":"<ul> <li>Follow PEP 8 style guidelines</li> <li>Use type hints where appropriate</li> <li>Write docstrings for public functions</li> <li>Add unit tests for new features</li> </ul>"},{"location":"development/contributing/#documentation_1","title":"Documentation","text":"<ul> <li>Use Markdown formatting</li> <li>Follow the existing structure</li> <li>Include code examples</li> <li>Test all examples before committing</li> </ul>"},{"location":"development/contributing/#testing","title":"Testing","text":""},{"location":"development/contributing/#run-tests-locally","title":"Run Tests Locally","text":"<pre><code># Run all tests\npytest tests/ -v\n\n# Run specific test file\npytest tests/test_specific.py -v\n\n# Run with coverage\npytest tests/ --cov=src --cov-report=term-missing\n</code></pre>"},{"location":"development/contributing/#documentation-testing","title":"Documentation Testing","text":"<pre><code># Test documentation build\nmkdocs build --strict\n\n# Serve and test locally\nmkdocs serve\n</code></pre>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":"<p>Need help contributing?</p> <ul> <li>Technical Questions: GitHub Discussions</li> <li>Bug Reports: GitHub Issues</li> <li>Email: abiolaadedayo1993@gmail.com</li> </ul>"},{"location":"development/contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in:</p> <ul> <li>CHANGELOG.md for their contributions</li> <li>GitHub contributors page</li> <li>Special thanks in release notes</li> </ul> <p>Thank you for helping make MCPOmni Connect better! \ud83d\ude80 </p>"},{"location":"development/testing/","title":"Testing","text":"<p>MCPOmni Connect includes comprehensive testing infrastructure to ensure reliability and quality. This guide covers running tests, writing new tests, and testing best practices.</p>"},{"location":"development/testing/#test-structure","title":"Test Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 unit/                    # Unit tests\n\u2502   \u251c\u2500\u2500 test_llm.py         # LLM integration tests\n\u2502   \u251c\u2500\u2500 test_transport.py   # Transport layer tests\n\u2502   \u251c\u2500\u2500 test_memory.py      # Memory management tests\n\u2502   \u2514\u2500\u2500 test_agents.py      # Agent system tests\n\u251c\u2500\u2500 integration/             # Integration tests\n\u2502   \u251c\u2500\u2500 test_mcp_servers.py # MCP server integration\n\u2502   \u251c\u2500\u2500 test_workflows.py   # End-to-end workflows\n\u2502   \u2514\u2500\u2500 test_auth.py        # Authentication flows\n\u251c\u2500\u2500 performance/             # Performance tests\n\u2502   \u251c\u2500\u2500 test_load.py        # Load testing\n\u2502   \u2514\u2500\u2500 test_benchmarks.py  # Benchmark tests\n\u251c\u2500\u2500 fixtures/                # Test data and fixtures\n\u2502   \u251c\u2500\u2500 config/             # Test configurations\n\u2502   \u251c\u2500\u2500 data/               # Sample data files\n\u2502   \u2514\u2500\u2500 responses/          # Mock responses\n\u2514\u2500\u2500 conftest.py             # Pytest configuration\n</code></pre>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#quick-test-commands","title":"Quick Test Commands","text":"<pre><code># Run all tests\npytest tests/ -v\n\n# Run specific test file\npytest tests/test_llm.py -v\n\n# Run tests with coverage\npytest tests/ --cov=src --cov-report=term-missing\n\n# Run only fast tests (skip slow/integration tests)\npytest tests/ -m \"not slow\" -v\n\n# Run tests with detailed output\npytest tests/ -v -s --tb=short\n</code></pre>"},{"location":"development/testing/#test-categories","title":"Test Categories","text":"<p>MCPOmni Connect tests are organized by markers:</p> <pre><code># Unit tests only\npytest tests/ -m \"not integration and not slow\" -v\n\n# Integration tests\npytest tests/ -m \"integration\" -v\n\n# Performance tests\npytest tests/ -m \"performance\" -v\n\n# Tests requiring external services\npytest tests/ -m \"requires_redis\" -v\n\n# OpenAI API integration tests\npytest tests/ -m \"OpenAIIntegration\" -v\n</code></pre>"},{"location":"development/testing/#test-configuration","title":"Test Configuration","text":""},{"location":"development/testing/#environment-setup","title":"Environment Setup","text":"<p>Create a test environment configuration:</p> tests/.env.test<pre><code># Test environment variables\nLLM_API_KEY=test-api-key-mock\nREDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_DB=15  # Use different DB for tests\nDEBUG=true\n</code></pre>"},{"location":"development/testing/#test-configuration-files","title":"Test Configuration Files","text":"tests/fixtures/config/test_servers_config.json<pre><code>{\n    \"AgentConfig\": {\n        \"tool_call_timeout\": 5,\n        \"max_steps\": 3,\n        \"request_limit\": 10,\n        \"total_tokens_limit\": 1000\n    },\n    \"LLM\": {\n        \"provider\": \"openai\",\n        \"model\": \"gpt-3.5-turbo\",\n        \"temperature\": 0.1,\n        \"max_tokens\": 100\n    },\n    \"mcpServers\": {\n        \"test-server\": {\n            \"transport_type\": \"stdio\",\n            \"command\": \"echo\",\n            \"args\": [\"test\"]\n        }\n    }\n}\n</code></pre>"},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":""},{"location":"development/testing/#llm-integration-tests","title":"LLM Integration Tests","text":"tests/unit/test_llm.py<pre><code>import pytest\nfrom unittest.mock import Mock, patch\nfrom mcpomni_connect.llm import LLMIntegration\n\nclass TestLLMIntegration:\n    @pytest.fixture\n    def llm_config(self):\n        return {\n            \"provider\": \"openai\",\n            \"model\": \"gpt-3.5-turbo\",\n            \"temperature\": 0.7,\n            \"max_tokens\": 1000\n        }\n\n    @pytest.fixture\n    def llm_integration(self, llm_config):\n        return LLMIntegration(llm_config)\n\n    @patch('mcpomni_connect.llm.litellm.completion')\n    def test_generate_response(self, mock_completion, llm_integration):\n        # Arrange\n        mock_response = Mock()\n        mock_response.choices = [Mock()]\n        mock_response.choices[0].message.content = \"Test response\"\n        mock_completion.return_value = mock_response\n\n        messages = [{\"role\": \"user\", \"content\": \"Hello\"}]\n\n        # Act\n        response = llm_integration.generate_response(messages)\n\n        # Assert\n        assert response == \"Test response\"\n        mock_completion.assert_called_once()\n\n    def test_message_conversion(self, llm_integration):\n        # Test Message object to dict conversion\n        from pydantic import BaseModel\n\n        class Message(BaseModel):\n            role: str\n            content: str\n\n        message = Message(role=\"user\", content=\"test\")\n        result = llm_integration._convert_message(message)\n\n        assert result == {\"role\": \"user\", \"content\": \"test\"}\n</code></pre>"},{"location":"development/testing/#transport-layer-tests","title":"Transport Layer Tests","text":"tests/unit/test_transport.py<pre><code>import pytest\nimport asyncio\nfrom unittest.mock import Mock, AsyncMock\nfrom mcpomni_connect.transport import StdioTransport, SSETransport\n\nclass TestStdioTransport:\n    @pytest.fixture\n    def stdio_config(self):\n        return {\n            \"command\": \"echo\",\n            \"args\": [\"hello\"]\n        }\n\n    @pytest.mark.asyncio\n    async def test_stdio_connection(self, stdio_config):\n        transport = StdioTransport(stdio_config)\n\n        # Test connection\n        await transport.connect()\n        assert transport.is_connected()\n\n        # Test message sending\n        response = await transport.send_message({\"test\": \"message\"})\n        assert response is not None\n\n        # Cleanup\n        await transport.disconnect()\n\nclass TestSSETransport:\n    @pytest.fixture\n    def sse_config(self):\n        return {\n            \"url\": \"http://localhost:3000/sse\",\n            \"headers\": {\"Authorization\": \"Bearer test-token\"}\n        }\n\n    @pytest.mark.asyncio\n    async def test_sse_connection(self, sse_config):\n        with patch('httpx.AsyncClient') as mock_client:\n            transport = SSETransport(sse_config)\n\n            # Mock successful connection\n            mock_client.return_value.stream.return_value.__aenter__.return_value = Mock()\n\n            await transport.connect()\n            assert transport.is_connected()\n</code></pre>"},{"location":"development/testing/#memory-management-tests","title":"Memory Management Tests","text":"tests/unit/test_memory.py<pre><code>import pytest\nfrom unittest.mock import Mock, patch\nfrom mcpomni_connect.memory import MemoryManager, RedisMemory\n\nclass TestMemoryManager:\n    @pytest.fixture\n    def memory_config(self):\n        return {\n            \"redis_host\": \"localhost\",\n            \"redis_port\": 6379,\n            \"redis_db\": 15\n        }\n\n    def test_session_memory(self, memory_config):\n        memory = MemoryManager(memory_config)\n\n        # Test adding messages\n        message = {\"role\": \"user\", \"content\": \"test\"}\n        memory.add_message(message)\n\n        # Test retrieving context\n        context = memory.get_context()\n        assert len(context) == 1\n        assert context[0] == message\n\n    @pytest.mark.requires_redis\n    def test_redis_memory(self, memory_config):\n        memory = MemoryManager(memory_config)\n        memory.enable_persistence()\n\n        # Test persistence\n        message = {\"role\": \"user\", \"content\": \"persistent test\"}\n        memory.add_message(message)\n\n        # Create new instance and check persistence\n        new_memory = MemoryManager(memory_config)\n        new_memory.enable_persistence()\n        context = new_memory.get_context()\n\n        assert len(context) &gt;= 1\n        assert any(msg[\"content\"] == \"persistent test\" for msg in context)\n</code></pre>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":""},{"location":"development/testing/#mcp-server-integration","title":"MCP Server Integration","text":"tests/integration/test_mcp_servers.py<pre><code>import pytest\nimport asyncio\nfrom mcpomni_connect.core import MCPOmniConnect\n\nclass TestMCPServerIntegration:\n    @pytest.fixture\n    def test_config(self):\n        return {\n            \"LLM\": {\n                \"provider\": \"openai\",\n                \"model\": \"gpt-3.5-turbo\"\n            },\n            \"mcpServers\": {\n                \"filesystem\": {\n                    \"transport_type\": \"stdio\",\n                    \"command\": \"uvx\",\n                    \"args\": [\"mcp-server-filesystem\", \"/tmp\"]\n                }\n            }\n        }\n\n    @pytest.mark.integration\n    @pytest.mark.asyncio\n    async def test_server_connection(self, test_config):\n        client = MCPOmniConnect(test_config)\n\n        try:\n            # Test connection\n            await client.connect_servers()\n\n            # Test tool discovery\n            tools = await client.list_tools()\n            assert len(tools) &gt; 0\n\n            # Test tool execution\n            result = await client.execute_tool(\n                \"list_directory\",\n                {\"path\": \"/tmp\"}\n            )\n            assert result is not None\n\n        finally:\n            await client.disconnect()\n\n    @pytest.mark.integration\n    @pytest.mark.slow\n    def test_workflow_execution(self, test_config):\n        client = MCPOmniConnect(test_config)\n\n        # Test complete workflow\n        result = client.execute_workflow([\n            {\"tool\": \"list_directory\", \"params\": {\"path\": \"/tmp\"}},\n            {\"tool\": \"read_file\", \"params\": {\"path\": \"/tmp/test.txt\"}}\n        ])\n\n        assert result[\"success\"] == True\n        assert len(result[\"steps\"]) == 2\n</code></pre>"},{"location":"development/testing/#authentication-flow-tests","title":"Authentication Flow Tests","text":"tests/integration/test_auth.py<pre><code>import pytest\nfrom unittest.mock import patch, Mock\nfrom mcpomni_connect.auth import OAuthManager, BearerTokenAuth\n\nclass TestOAuthFlow:\n    @pytest.mark.integration\n    @patch('webbrowser.open')\n    @patch('http.server.HTTPServer')\n    def test_oauth_callback_server(self, mock_server, mock_browser):\n        oauth_manager = OAuthManager()\n\n        # Mock OAuth callback\n        mock_request = Mock()\n        mock_request.path = \"/callback?code=test-auth-code\"\n\n        # Test callback handling\n        result = oauth_manager.handle_callback(mock_request)\n        assert result[\"code\"] == \"test-auth-code\"\n\n        # Verify browser was opened\n        mock_browser.assert_called_once()\n\nclass TestBearerTokenAuth:\n    def test_bearer_token_headers(self):\n        auth = BearerTokenAuth(\"test-token-123\")\n        headers = auth.get_headers()\n\n        assert headers[\"Authorization\"] == \"Bearer test-token-123\"\n        assert \"Content-Type\" in headers\n</code></pre>"},{"location":"development/testing/#performance-tests","title":"Performance Tests","text":""},{"location":"development/testing/#load-testing","title":"Load Testing","text":"tests/performance/test_load.py<pre><code>import pytest\nimport asyncio\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\nfrom mcpomni_connect.core import MCPOmniConnect\n\nclass TestPerformance:\n    @pytest.mark.performance\n    def test_concurrent_requests(self):\n        \"\"\"Test handling multiple concurrent requests.\"\"\"\n        client = MCPOmniConnect(test_config)\n\n        def make_request():\n            return client.execute_tool(\"list_directory\", {\"path\": \"/tmp\"})\n\n        # Test 10 concurrent requests\n        with ThreadPoolExecutor(max_workers=10) as executor:\n            start_time = time.time()\n\n            futures = [executor.submit(make_request) for _ in range(10)]\n            results = [future.result() for future in futures]\n\n            end_time = time.time()\n\n        # Assert all requests succeeded\n        assert all(result is not None for result in results)\n\n        # Assert reasonable performance (&lt; 5 seconds for 10 requests)\n        assert end_time - start_time &lt; 5.0\n\n    @pytest.mark.performance\n    def test_memory_usage(self):\n        \"\"\"Test memory usage doesn't grow excessively.\"\"\"\n        import psutil\n        import os\n\n        process = psutil.Process(os.getpid())\n        initial_memory = process.memory_info().rss\n\n        client = MCPOmniConnect(test_config)\n\n        # Execute many operations\n        for _ in range(100):\n            client.execute_tool(\"list_directory\", {\"path\": \"/tmp\"})\n\n        final_memory = process.memory_info().rss\n        memory_growth = final_memory - initial_memory\n\n        # Assert memory growth is reasonable (&lt; 50MB)\n        assert memory_growth &lt; 50 * 1024 * 1024\n</code></pre>"},{"location":"development/testing/#benchmark-tests","title":"Benchmark Tests","text":"tests/performance/test_benchmarks.py<pre><code>import pytest\nimport time\nfrom mcpomni_connect.core import MCPOmniConnect\n\nclass TestBenchmarks:\n    @pytest.mark.performance\n    def test_tool_execution_speed(self):\n        \"\"\"Benchmark tool execution speed.\"\"\"\n        client = MCPOmniConnect(test_config)\n\n        # Warm up\n        for _ in range(5):\n            client.execute_tool(\"list_directory\", {\"path\": \"/tmp\"})\n\n        # Benchmark\n        iterations = 20\n        start_time = time.time()\n\n        for _ in range(iterations):\n            client.execute_tool(\"list_directory\", {\"path\": \"/tmp\"})\n\n        end_time = time.time()\n        avg_time = (end_time - start_time) / iterations\n\n        # Assert average execution time is reasonable\n        assert avg_time &lt; 0.5  # Less than 500ms per execution\n\n        print(f\"Average tool execution time: {avg_time:.3f}s\")\n</code></pre>"},{"location":"development/testing/#test-utilities-and-fixtures","title":"Test Utilities and Fixtures","text":""},{"location":"development/testing/#common-fixtures","title":"Common Fixtures","text":"tests/conftest.py<pre><code>import pytest\nimport tempfile\nimport os\nfrom pathlib import Path\n\n@pytest.fixture(scope=\"session\")\ndef test_data_dir():\n    \"\"\"Provide test data directory.\"\"\"\n    return Path(__file__).parent / \"fixtures\" / \"data\"\n\n@pytest.fixture\ndef temp_dir():\n    \"\"\"Provide temporary directory for tests.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        yield Path(tmpdir)\n\n@pytest.fixture\ndef mock_llm_response():\n    \"\"\"Mock LLM response for testing.\"\"\"\n    return {\n        \"choices\": [\n            {\n                \"message\": {\n                    \"content\": \"Test response from mock LLM\",\n                    \"role\": \"assistant\"\n                }\n            }\n        ],\n        \"usage\": {\n            \"prompt_tokens\": 10,\n            \"completion_tokens\": 8,\n            \"total_tokens\": 18\n        }\n    }\n\n@pytest.fixture\ndef sample_conversation():\n    \"\"\"Sample conversation for testing memory features.\"\"\"\n    return [\n        {\"role\": \"user\", \"content\": \"Hello\"},\n        {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n        {\"role\": \"user\", \"content\": \"How are you?\"},\n        {\"role\": \"assistant\", \"content\": \"I'm doing well, thank you!\"}\n    ]\n</code></pre>"},{"location":"development/testing/#mock-servers","title":"Mock Servers","text":"tests/fixtures/mock_server.py<pre><code>import asyncio\nimport json\nfrom aiohttp import web\n\nclass MockMCPServer:\n    \"\"\"Mock MCP server for testing.\"\"\"\n\n    def __init__(self, port=8899):\n        self.port = port\n        self.app = web.Application()\n        self.setup_routes()\n\n    def setup_routes(self):\n        self.app.router.add_post('/mcp', self.handle_mcp_request)\n\n    async def handle_mcp_request(self, request):\n        data = await request.json()\n\n        # Mock responses based on method\n        if data.get('method') == 'tools/list':\n            return web.json_response({\n                \"tools\": [\n                    {\n                        \"name\": \"mock_tool\",\n                        \"description\": \"A mock tool for testing\",\n                        \"inputSchema\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"input\": {\"type\": \"string\"}\n                            }\n                        }\n                    }\n                ]\n            })\n\n        return web.json_response({\"result\": \"mock response\"})\n\n    async def start(self):\n        runner = web.AppRunner(self.app)\n        await runner.setup()\n        site = web.TCPSite(runner, 'localhost', self.port)\n        await site.start()\n        return runner\n</code></pre>"},{"location":"development/testing/#testing-best-practices","title":"Testing Best Practices","text":""},{"location":"development/testing/#writing-good-tests","title":"Writing Good Tests","text":"<p>Test Writing Guidelines</p> <ol> <li>Descriptive Names: Use clear, descriptive test names</li> <li>Arrange-Act-Assert: Structure tests clearly</li> <li>Single Responsibility: One assertion per test when possible</li> <li>Fast Execution: Keep unit tests fast (&lt; 100ms)</li> <li>Isolated: Tests shouldn't depend on each other</li> <li>Deterministic: Tests should always produce same results</li> </ol>"},{"location":"development/testing/#test-organization","title":"Test Organization","text":"<pre><code>class TestFeatureName:\n    \"\"\"Test class for specific feature.\"\"\"\n\n    @pytest.fixture\n    def feature_setup(self):\n        \"\"\"Setup specific to this feature.\"\"\"\n        pass\n\n    def test_normal_case(self, feature_setup):\n        \"\"\"Test the normal/happy path.\"\"\"\n        pass\n\n    def test_edge_case(self, feature_setup):\n        \"\"\"Test edge cases.\"\"\"\n        pass\n\n    def test_error_handling(self, feature_setup):\n        \"\"\"Test error conditions.\"\"\"\n        pass\n</code></pre>"},{"location":"development/testing/#mock-usage","title":"Mock Usage","text":"<pre><code># Good: Mock external dependencies\n@patch('mcpomni_connect.external_api.requests.get')\ndef test_api_call(self, mock_get):\n    mock_get.return_value.json.return_value = {\"status\": \"ok\"}\n    result = my_function()\n    assert result[\"status\"] == \"ok\"\n\n# Avoid: Mocking internal logic\n# This makes tests brittle and less valuable\n</code></pre>"},{"location":"development/testing/#continuous-integration","title":"Continuous Integration","text":""},{"location":"development/testing/#github-actions-configuration","title":"GitHub Actions Configuration","text":".github/workflows/test.yml<pre><code>name: Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [3.10, 3.11, 3.12]\n\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v3\n      with:\n        python-version: ${{ matrix.python-version }}\n\n    - name: Install UV\n      run: pip install uv\n\n    - name: Install dependencies\n      run: uv sync\n\n    - name: Run tests\n      run: |\n        uv run pytest tests/ -v --cov=src --cov-report=xml\n\n    - name: Upload coverage\n      uses: codecov/codecov-action@v3\n      with:\n        file: ./coverage.xml\n</code></pre>"},{"location":"development/testing/#test-data-management","title":"Test Data Management","text":""},{"location":"development/testing/#sample-data-files","title":"Sample Data Files","text":"tests/fixtures/data/sample_config.json<pre><code>{\n    \"LLM\": {\n        \"provider\": \"openai\",\n        \"model\": \"gpt-3.5-turbo\"\n    },\n    \"mcpServers\": {\n        \"test-server\": {\n            \"transport_type\": \"stdio\",\n            \"command\": \"echo\",\n            \"args\": [\"test\"]\n        }\n    }\n}\n</code></pre>"},{"location":"development/testing/#test-database","title":"Test Database","text":"<p>For integration tests requiring database:</p> tests/fixtures/test_db.py<pre><code>import sqlite3\nimport tempfile\nfrom pathlib import Path\n\n@pytest.fixture\ndef test_database():\n    \"\"\"Create temporary test database.\"\"\"\n    with tempfile.NamedTemporaryFile(suffix='.db', delete=False) as f:\n        db_path = f.name\n\n    # Create test schema\n    conn = sqlite3.connect(db_path)\n    conn.execute('''\n        CREATE TABLE users (\n            id INTEGER PRIMARY KEY,\n            name TEXT NOT NULL,\n            email TEXT UNIQUE\n        )\n    ''')\n    conn.execute('''\n        INSERT INTO users (name, email) VALUES \n        ('Test User', 'test@example.com'),\n        ('Another User', 'another@example.com')\n    ''')\n    conn.commit()\n    conn.close()\n\n    yield db_path\n\n    # Cleanup\n    Path(db_path).unlink()\n</code></pre> <p>This comprehensive testing guide ensures MCPOmni Connect maintains high quality and reliability across all its components and features.</p> <p>Next: Contributing \u2192 </p>"},{"location":"features/agent-system/","title":"Agent System","text":"<p>MCPOmni Connect features a sophisticated AI agent system that goes beyond simple tool calling. It provides multiple operation modes designed for different levels of autonomy and complexity.</p>"},{"location":"features/agent-system/#overview","title":"Overview","text":"<p>The agent system consists of three main operation modes:</p> Mode Description Use Case User Involvement Chat Interactive with approval Learning, careful operations High Autonomous Independent execution Well-defined tasks Low Orchestrator Multi-agent coordination Complex workflows Minimal"},{"location":"features/agent-system/#chat-mode-interactive","title":"Chat Mode (Interactive)","text":"<p>Default mode - Human-in-the-loop with explicit approval for each action.</p>"},{"location":"features/agent-system/#characteristics","title":"Characteristics","text":"<ul> <li>\u2705 User Control: Every tool execution requires approval</li> <li>\u2705 Educational: Shows reasoning process step-by-step</li> <li>\u2705 Safe: No unexpected actions</li> <li>\u2705 Transparent: Clear explanation of what each tool does</li> </ul>"},{"location":"features/agent-system/#example-interaction","title":"Example Interaction","text":"<pre><code>&gt; Can you analyze the log files and create a summary?\n\nI'll help you analyze the log files. Let me start by finding all log files in the current directory.\n\n\ud83d\udd27 Tool: list_directory\n\ud83d\udcc1 Path: .\n\ud83d\udccb Purpose: Find log files to analyze\n\u2753 Execute this tool? (y/n): y\n\nFound 3 log files:\n- error.log (2.3 MB)\n- access.log (15.7 MB) \n- debug.log (892 KB)\n\nNow I'll read each log file to analyze the contents.\n\n\ud83d\udd27 Tool: read_file\n\ud83d\udcc1 File: error.log\n\ud83d\udccb Purpose: Analyze error patterns\n\u2753 Execute this tool? (y/n): y\n\n[Analysis continues with user approval for each step]\n</code></pre>"},{"location":"features/agent-system/#when-to-use-chat-mode","title":"When to Use Chat Mode","text":"<ul> <li>Learning how MCPOmni Connect works</li> <li>Sensitive operations requiring oversight</li> <li>Exploratory data analysis</li> <li>Testing new server configurations</li> <li>When you want to understand the AI's reasoning</li> </ul>"},{"location":"features/agent-system/#autonomous-mode-react-agent","title":"Autonomous Mode (ReAct Agent)","text":"<p>Independent execution - The AI agent operates autonomously using ReAct (Reasoning + Acting) methodology.</p>"},{"location":"features/agent-system/#characteristics_1","title":"Characteristics","text":"<ul> <li>\ud83e\udd16 Independent: No approval required for tool execution</li> <li>\ud83e\udde0 Reasoning: Shows thought process before each action</li> <li>\ud83d\udd04 Iterative: Adapts based on results</li> <li>\ud83d\udcca Goal-Oriented: Focuses on completing the requested task</li> <li>\u26a1 Efficient: Faster execution without approval delays</li> </ul>"},{"location":"features/agent-system/#react-methodology","title":"ReAct Methodology","text":"<p>The ReAct framework follows this cycle:</p> <ol> <li>Thought: Analyze the current situation</li> <li>Action: Choose and execute the best tool</li> <li>Observation: Process the results</li> <li>Repeat: Continue until task completion</li> </ol>"},{"location":"features/agent-system/#example-session","title":"Example Session","text":"<pre><code>&gt; /mode:auto\nNow operating in AUTONOMOUS mode. I will execute tasks independently.\n\n&gt; Create a backup of all configuration files and analyze their contents\n\n\ud83e\udd16 Executing autonomous task: Create backup and analyze configuration files\n\n\ud83d\udcad Thought: I need to find all configuration files first, then create a backup, and finally analyze their contents.\n\n\ud83d\udd27 Action: list_directory(path=\".\", pattern=\"*.{json,yaml,yml,conf,config}\")\n\ud83d\udcca Observation: Found 5 configuration files:\n- servers_config.json\n- .env\n- docker-compose.yaml\n- pytest.ini\n- pyproject.toml\n\n\ud83d\udcad Thought: Now I'll create a backup directory and copy these files.\n\n\ud83d\udd27 Action: create_directory(path=\"backup_2024-01-15\")\n\ud83d\udcca Observation: Backup directory created successfully.\n\n\ud83d\udd27 Action: copy_files(source=[\"servers_config.json\", \"docker-compose.yaml\", \"pytest.ini\", \"pyproject.toml\"], destination=\"backup_2024-01-15/\")\n\ud83d\udcca Observation: 4 files copied successfully (excluding .env for security).\n\n\ud83d\udcad Thought: Now I'll analyze each configuration file to understand the project setup.\n\n[Analysis continues autonomously...]\n\n\u2705 Task Complete: \n- Created backup in backup_2024-01-15/\n- Analyzed 4 configuration files\n- Generated summary report with key findings\n</code></pre>"},{"location":"features/agent-system/#when-to-use-autonomous-mode","title":"When to Use Autonomous Mode","text":"<ul> <li>Well-defined, routine tasks</li> <li>Data processing workflows</li> <li>System maintenance operations</li> <li>Report generation</li> <li>Bulk file operations</li> <li>When you trust the AI to work independently</li> </ul>"},{"location":"features/agent-system/#orchestrator-mode-advanced","title":"Orchestrator Mode (Advanced)","text":"<p>Multi-agent coordination - Strategic planning and execution across multiple MCP servers and agents.</p>"},{"location":"features/agent-system/#characteristics_2","title":"Characteristics","text":"<ul> <li>\ud83c\udfaf Strategic Planning: Breaks complex tasks into subtasks</li> <li>\ud83e\udd1d Multi-Agent: Coordinates multiple specialized agents</li> <li>\u26a1 Parallel Execution: Runs tasks simultaneously when possible</li> <li>\ud83d\udcc8 Scalable: Handles enterprise-level workflows</li> <li>\ud83d\udd04 Adaptive: Adjusts strategy based on real-time results</li> </ul>"},{"location":"features/agent-system/#architecture","title":"Architecture","text":"<pre><code>graph TD\n    A[Orchestrator Agent] --&gt; B[Planning Engine]\n    A --&gt; C[Task Coordinator]\n    A --&gt; D[Progress Monitor]\n\n    B --&gt; E[Task Analysis]\n    B --&gt; F[Resource Assessment]\n    B --&gt; G[Strategy Generation]\n\n    C --&gt; H[Agent 1: Database]\n    C --&gt; I[Agent 2: File System]\n    C --&gt; J[Agent 3: API Services]\n    C --&gt; K[Agent N: Custom Tools]\n\n    D --&gt; L[Progress Tracking]\n    D --&gt; M[Error Handling]\n    D --&gt; N[Result Aggregation]</code></pre>"},{"location":"features/agent-system/#example-workflow","title":"Example Workflow","text":"<pre><code>&gt; /mode:orchestrator\nNow operating in ORCHESTRATOR mode. I will coordinate complex multi-step tasks.\n\n&gt; Migrate user data from the old database to the new system, update all configuration files, and send notifications to stakeholders\n\n\ud83c\udfaf Orchestrator: Planning complex migration task\n\n\ud83d\udccb Strategic Plan:\nPhase 1: Data Assessment (Parallel)\n\u251c\u2500 Agent A: Analyze old database schema\n\u251c\u2500 Agent B: Verify new database connectivity  \n\u2514\u2500 Agent C: Validate user data integrity\n\nPhase 2: Configuration Preparation (Sequential)\n\u251c\u2500 Agent D: Backup current configurations\n\u251c\u2500 Agent E: Generate new configuration templates\n\u2514\u2500 Agent F: Validate configuration compatibility\n\nPhase 3: Migration Execution (Parallel where safe)\n\u251c\u2500 Agent A: Migrate user data (critical path)\n\u251c\u2500 Agent G: Update configuration files\n\u2514\u2500 Agent H: Prepare notification templates\n\nPhase 4: Validation &amp; Notification (Sequential)\n\u251c\u2500 Agent I: Verify migration completeness\n\u251c\u2500 Agent J: Test system functionality\n\u2514\u2500 Agent K: Send stakeholder notifications\n\n\ud83d\ude80 Executing Phase 1: Data Assessment...\n\n\ud83e\udd16 Agent A (Database): Connecting to legacy database...\n\ud83e\udd16 Agent B (Database): Testing new database connection...\n\ud83e\udd16 Agent C (Validation): Running data integrity checks...\n\n[Parallel execution continues...]\n\nPhase 1 Complete \u2705 (2.3 minutes)\n- Legacy schema analyzed: 15 tables, 1.2M records\n- New database connection verified\n- Data integrity: 99.98% clean (3 minor issues flagged)\n\n\ud83d\ude80 Executing Phase 2: Configuration Preparation...\n\n[Sequential execution continues...]\n\n\u2705 Migration Complete (12.7 minutes total):\n- 1,247,892 user records migrated successfully\n- 8 configuration files updated\n- 23 stakeholders notified\n- Zero data loss, minimal downtime\n</code></pre>"},{"location":"features/agent-system/#when-to-use-orchestrator-mode","title":"When to Use Orchestrator Mode","text":"<ul> <li>Complex multi-system operations</li> <li>Data migration projects</li> <li>Deployment workflows</li> <li>Business process automation</li> <li>Enterprise integration tasks</li> <li>When you need sophisticated coordination</li> </ul>"},{"location":"features/agent-system/#mode-switching","title":"Mode Switching","text":"<p>You can switch between modes at any time:</p> <pre><code># Switch to autonomous mode\n/mode:auto\n\n# Switch to orchestrator mode  \n/mode:orchestrator\n\n# Return to interactive chat mode\n/mode:chat\n\n# Check current mode\n/status\n</code></pre>"},{"location":"features/agent-system/#agent-configuration","title":"Agent Configuration","text":"<p>Control agent behavior through <code>AgentConfig</code> in your <code>servers_config.json</code>:</p> <pre><code>{\n    \"AgentConfig\": {\n        \"tool_call_timeout\": 30,        // Tool execution timeout (seconds)\n        \"max_steps\": 15,                // Maximum steps per task\n        \"request_limit\": 1000,          // Maximum API requests\n        \"total_tokens_limit\": 100000    // Maximum token usage\n    }\n}\n</code></pre>"},{"location":"features/agent-system/#configuration-options","title":"Configuration Options","text":"Setting Description Chat Mode Auto Mode Orchestrator <code>tool_call_timeout</code> Tool execution timeout \u2705 \u2705 \u2705 <code>max_steps</code> Maximum reasoning steps \u274c \u2705 \u2705 <code>request_limit</code> API call limit \u2705 \u2705 \u2705 <code>total_tokens_limit</code> Token usage limit \u2705 \u2705 \u2705"},{"location":"features/agent-system/#safety-and-control","title":"Safety and Control","text":""},{"location":"features/agent-system/#built-in-safeguards","title":"Built-in Safeguards","text":"<ul> <li>Timeout Protection: Prevents infinite loops</li> <li>Resource Limits: Controls API usage and costs</li> <li>Step Limits: Prevents excessive reasoning cycles</li> <li>Error Recovery: Graceful handling of failures</li> <li>User Override: Emergency stop with Ctrl+C</li> </ul>"},{"location":"features/agent-system/#best-practices","title":"Best Practices","text":"<p>Safe Agent Usage</p> <ol> <li>Start with Chat Mode when learning</li> <li>Test in Autonomous Mode with simple tasks first</li> <li>Use Orchestrator Mode for complex, well-understood workflows</li> <li>Set Conservative Limits for production environments</li> <li>Monitor Resource Usage with <code>/api_stats</code></li> </ol>"},{"location":"features/agent-system/#security-considerations","title":"Security Considerations","text":"<ul> <li>No Arbitrary Code Execution: Agents only use predefined tools</li> <li>Server Isolation: Tools are isolated to their respective servers</li> <li>Audit Trail: All actions are logged for review</li> <li>Permission Model: Agents respect server-level permissions</li> </ul>"},{"location":"features/agent-system/#advanced-features","title":"Advanced Features","text":""},{"location":"features/agent-system/#context-management","title":"Context Management","text":"<ul> <li>Long-term Memory: Persistent context across sessions</li> <li>Task Context: Maintains context within multi-step tasks</li> <li>Cross-Agent Communication: Agents can share relevant information</li> </ul>"},{"location":"features/agent-system/#error-handling","title":"Error Handling","text":"<ul> <li>Graceful Degradation: Continues with available tools if some fail</li> <li>Retry Logic: Automatic retry for transient failures</li> <li>Fallback Strategies: Alternative approaches when primary methods fail</li> </ul>"},{"location":"features/agent-system/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Parallel Execution: Runs independent tasks simultaneously</li> <li>Tool Caching: Caches frequently used tool results</li> <li>Smart Batching: Groups related operations for efficiency</li> </ul> <p>Next: Tool Orchestration \u2192 </p>"},{"location":"features/tool-orchestration/","title":"Tool Orchestration","text":"<p>MCPOmni Connect provides sophisticated tool orchestration capabilities, allowing seamless coordination and execution of tools across multiple MCP servers with intelligent routing and context management.</p>"},{"location":"features/tool-orchestration/#overview","title":"Overview","text":"<p>Tool orchestration enables:</p> <ul> <li>Cross-Server Coordination: Use tools from multiple servers in a single workflow</li> <li>Intelligent Routing: Automatic selection of the best tool for each task</li> <li>Context Sharing: Pass data between tools seamlessly</li> <li>Parallel Execution: Run independent tools simultaneously</li> <li>Error Handling: Graceful handling of tool failures with fallbacks</li> </ul>"},{"location":"features/tool-orchestration/#core-concepts","title":"Core Concepts","text":""},{"location":"features/tool-orchestration/#tool-discovery","title":"Tool Discovery","text":"<p>MCPOmni Connect automatically discovers tools from all connected servers:</p> <pre><code>/tools\n</code></pre> <p>Output Example: <pre><code>Available tools across 4 servers:\n\n\ud83d\udcc1 filesystem server:\n  - read_file: Read contents of a file\n  - write_file: Write content to a file\n  - list_directory: List directory contents\n  - search_files: Search for files matching patterns\n\n\ud83d\uddc3\ufe0f  database server:\n  - query_database: Execute SQL queries\n  - get_schema: Get database schema information\n  - backup_database: Create database backup\n  - migrate_data: Migrate data between tables\n\n\ud83d\udce7 notifications server:\n  - send_email: Send email notifications\n  - create_alert: Create system alerts\n  - send_slack: Send Slack messages\n  - log_event: Log system events\n\n\ud83c\udf10 api server:\n  - http_request: Make HTTP requests\n  - webhook_trigger: Trigger webhooks\n  - api_auth: Authenticate with external APIs\n</code></pre></p>"},{"location":"features/tool-orchestration/#tool-capabilities","title":"Tool Capabilities","text":"<p>Each tool provides metadata about its capabilities:</p> <ul> <li>Input Parameters: Required and optional parameters</li> <li>Output Format: Type and structure of responses</li> <li>Error Conditions: Possible failure modes</li> <li>Performance Characteristics: Expected execution time</li> <li>Dependencies: Other tools or resources required</li> </ul>"},{"location":"features/tool-orchestration/#intelligent-tool-routing","title":"Intelligent Tool Routing","text":""},{"location":"features/tool-orchestration/#automatic-tool-selection","title":"Automatic Tool Selection","text":"<p>The AI automatically selects appropriate tools based on the task:</p> <pre><code>&gt; Analyze the database and create a backup, then notify the team\n\n\ud83e\udd16 Planning task execution:\n\nStep 1: \ud83d\udd27 get_schema (database server)\n  \u2514\u2500 Purpose: Analyze database structure\n\nStep 2: \ud83d\udd27 query_database (database server)\n  \u2514\u2500 Purpose: Check database health\n\nStep 3: \ud83d\udd27 backup_database (database server)\n  \u2514\u2500 Purpose: Create backup\n\nStep 4: \ud83d\udd27 send_slack (notifications server)\n  \u2514\u2500 Purpose: Notify team of completion\n</code></pre>"},{"location":"features/tool-orchestration/#tool-preference-logic","title":"Tool Preference Logic","text":"<p>MCPOmni Connect uses several factors for tool selection:</p> <ol> <li>Functional Match: Does the tool perform the required function?</li> <li>Server Availability: Is the server currently accessible?</li> <li>Parameter Compatibility: Can the tool accept available data?</li> <li>Performance History: Has this tool been reliable?</li> <li>Context Relevance: Does it fit the current workflow?</li> </ol>"},{"location":"features/tool-orchestration/#cross-server-workflows","title":"Cross-Server Workflows","text":""},{"location":"features/tool-orchestration/#data-flow-between-servers","title":"Data Flow Between Servers","text":"<p>Tools can pass data seamlessly across servers:</p> <pre><code>&gt; Read the config file, query the database for matching records, and send a summary email\n\n\ud83d\udd27 Step 1: read_file (filesystem) \u2192 config.json content\n\ud83d\udcca Step 2: query_database (database) \u2190 config parameters\n\ud83d\udce7 Step 3: send_email (notifications) \u2190 query results\n</code></pre>"},{"location":"features/tool-orchestration/#context-preservation","title":"Context Preservation","text":"<p>Data flows between tools while maintaining context:</p> <pre><code># Conceptual data flow\nconfig_data = filesystem.read_file(\"config.json\")\nquery_params = extract_db_params(config_data)\nresults = database.query_database(query_params)\nsummary = generate_summary(results)\nnotifications.send_email(to=\"team@company.com\", subject=\"Summary\", body=summary)\n</code></pre>"},{"location":"features/tool-orchestration/#parallel-tool-execution","title":"Parallel Tool Execution","text":""},{"location":"features/tool-orchestration/#concurrent-operations","title":"Concurrent Operations","text":"<p>Independent tools can run simultaneously:</p> <pre><code>&gt; Check system health: verify database connectivity, check file system space, and test API endpoints\n\n\ud83e\udd16 Executing parallel health checks:\n\n\ud83d\udd27 Parallel Execution Group 1:\n\u251c\u2500 get_schema (database server) \u23f1\ufe0f  2.1s \u2705\n\u251c\u2500 list_directory (filesystem server) \u23f1\ufe0f  0.8s \u2705  \n\u2514\u2500 http_request (api server) \u23f1\ufe0f  1.5s \u2705\n\nAll health checks completed in 2.1s (fastest possible)\n</code></pre>"},{"location":"features/tool-orchestration/#dependency-management","title":"Dependency Management","text":"<p>Tools with dependencies execute in proper order:</p> <pre><code>&gt; Deploy application: build, test, deploy, then notify\n\n\ud83e\udd16 Dependency-aware execution:\n\nPhase 1 (Sequential):\n\ud83d\udd27 build_application \u2192 \u2705 (3.2s)\n\ud83d\udd27 run_tests \u2190 build artifacts \u2192 \u2705 (5.7s)\n\nPhase 2 (Parallel):\n\ud83d\udd27 deploy_staging \u2190 test results \u2192 \u2705 (12.3s)\n\ud83d\udd27 update_documentation \u2192 \u2705 (4.1s)\n\ud83d\udd27 prepare_notifications \u2192 \u2705 (0.5s)\n\nPhase 3 (Final):\n\ud83d\udd27 send_deployment_notification \u2190 all results \u2192 \u2705 (1.2s)\n\nTotal time: 22.7s (vs 27.0s sequential)\n</code></pre>"},{"location":"features/tool-orchestration/#tool-chaining-and-composition","title":"Tool Chaining and Composition","text":""},{"location":"features/tool-orchestration/#simple-tool-chains","title":"Simple Tool Chains","text":"<p>Basic sequential tool execution:</p> <pre><code># User request\n&gt; Create a backup of the user data and email me the results\n\n# Tool chain\nread_database \u2192 backup_data \u2192 compress_backup \u2192 send_email\n</code></pre>"},{"location":"features/tool-orchestration/#complex-compositions","title":"Complex Compositions","text":"<p>Advanced workflows with branching and merging:</p> <pre><code># User request\n&gt; Analyze all log files, identify errors, and create both a summary report and individual notifications\n\n# Complex composition\nlist_log_files \u2192 \n\u251c\u2500 analyze_errors \u2192 summarize_errors \u2192 create_report\n\u251c\u2500 extract_critical \u2192 send_alerts\n\u2514\u2500 archive_logs \u2192 update_inventory\n</code></pre>"},{"location":"features/tool-orchestration/#conditional-execution","title":"Conditional Execution","text":"<p>Tools can execute based on conditions:</p> <pre><code>&gt; Check database health and backup if needed\n\n\ud83e\udd16 Conditional workflow:\n\n\ud83d\udd27 check_database_health \u2192 Status: DEGRADED\n  \u251c\u2500 Condition: health &lt; 90% \u2192 TRUE\n  \u251c\u2500 \ud83d\udd27 backup_database \u2192 \u2705 Backup created\n  \u251c\u2500 \ud83d\udd27 send_alert \u2192 \u2705 Team notified\n  \u2514\u2500 \ud83d\udd27 schedule_maintenance \u2192 \u2705 Maintenance scheduled\n\n\ud83d\udd27 check_database_health \u2192 Status: HEALTHY  \n  \u2514\u2500 Condition: health &lt; 90% \u2192 FALSE (no backup needed)\n</code></pre>"},{"location":"features/tool-orchestration/#error-handling-and-resilience","title":"Error Handling and Resilience","text":""},{"location":"features/tool-orchestration/#automatic-retries","title":"Automatic Retries","text":"<p>Failed tools are automatically retried with backoff:</p> <pre><code>\ud83d\udd27 http_request (api server) \u2192 \u274c Connection timeout\n  \u251c\u2500 Retry 1/3 (wait 2s) \u2192 \u274c Connection timeout  \n  \u251c\u2500 Retry 2/3 (wait 4s) \u2192 \u274c Connection timeout\n  \u251c\u2500 Retry 3/3 (wait 8s) \u2192 \u2705 Success\n  \u2514\u2500 Total time: 14.2s (with retries)\n</code></pre>"},{"location":"features/tool-orchestration/#fallback-strategies","title":"Fallback Strategies","text":"<p>Alternative tools when primary tools fail:</p> <pre><code>\ud83d\udd27 send_slack (notifications server) \u2192 \u274c Server unreachable\n  \u251c\u2500 Fallback: send_email (notifications server) \u2192 \u274c SMTP error\n  \u251c\u2500 Fallback: log_event (filesystem server) \u2192 \u2705 Logged to file\n  \u2514\u2500 Strategy: Notify via available channel\n</code></pre>"},{"location":"features/tool-orchestration/#graceful-degradation","title":"Graceful Degradation","text":"<p>Continue workflow even when some tools fail:</p> <pre><code>&gt; Generate system report with all subsystem status\n\n\ud83d\udd27 Orchestrated reporting:\n\u251c\u2500 database_status \u2192 \u2705 Connected (healthy)\n\u251c\u2500 filesystem_status \u2192 \u274c Permission denied\n\u251c\u2500 api_status \u2192 \u2705 All endpoints responding\n\u251c\u2500 notification_status \u2192 \u2705 Services operational\n\n\ud83d\udcca Result: Partial report generated (3/4 subsystems)\n\u26a0\ufe0f  Warning: Filesystem status unavailable (permission issue)\n</code></pre>"},{"location":"features/tool-orchestration/#performance-optimization","title":"Performance Optimization","text":""},{"location":"features/tool-orchestration/#tool-caching","title":"Tool Caching","text":"<p>Frequently used tool results are cached:</p> <pre><code>\ud83d\udd27 get_schema (database server)\n  \u251c\u2500 Cache check \u2192 \u274c Not found\n  \u251c\u2500 Execute tool \u2192 \u2705 Schema retrieved (2.3s)\n  \u2514\u2500 Cache stored \u2192 Valid for 5 minutes\n\n\ud83d\udd27 get_schema (database server) [later request]\n  \u251c\u2500 Cache check \u2192 \u2705 Found (age: 2m 15s)\n  \u2514\u2500 Return cached \u2192 Instant response (0.001s)\n</code></pre>"},{"location":"features/tool-orchestration/#connection-pooling","title":"Connection Pooling","text":"<p>Reuse server connections for better performance:</p> <pre><code>Connection Pool Status:\n\u251c\u2500 filesystem server: 2 active connections\n\u251c\u2500 database server: 1 active connection  \n\u251c\u2500 notifications server: 1 idle connection\n\u2514\u2500 api server: 3 active connections\n\n\ud83d\udd27 Tool execution using pooled connections:\n\u251c\u2500 read_file \u2192 Reused connection #1 (0.1ms setup)\n\u251c\u2500 query_database \u2192 Reused connection #1 (0.1ms setup)\n\u2514\u2500 send_email \u2192 New connection #2 (15.2ms setup)\n</code></pre>"},{"location":"features/tool-orchestration/#batch-operations","title":"Batch Operations","text":"<p>Group similar operations for efficiency:</p> <pre><code># Instead of individual file reads\n\ud83d\udd27 read_file(\"file1.txt\") \u2192 45ms\n\ud83d\udd27 read_file(\"file2.txt\") \u2192 43ms  \n\ud83d\udd27 read_file(\"file3.txt\") \u2192 44ms\nTotal: 132ms\n\n# Batch operation\n\ud83d\udd27 read_multiple_files([\"file1.txt\", \"file2.txt\", \"file3.txt\"]) \u2192 52ms\nImprovement: 60% faster\n</code></pre>"},{"location":"features/tool-orchestration/#tool-monitoring-and-analytics","title":"Tool Monitoring and Analytics","text":""},{"location":"features/tool-orchestration/#real-time-monitoring","title":"Real-time Monitoring","text":"<p>Track tool performance during execution:</p> <pre><code>/debug  # Enable detailed monitoring\n\n\ud83d\udd27 Tool Execution Monitor:\n\u251c\u2500 query_database: 2.3s (normal)\n\u251c\u2500 send_email: 1.1s (fast)  \n\u251c\u2500 http_request: 8.7s (slow) \u26a0\ufe0f\n\u2514\u2500 backup_database: 45.2s (normal for size)\n\nPerformance Alert: http_request exceeding normal time (3s avg)\n</code></pre>"},{"location":"features/tool-orchestration/#usage-analytics","title":"Usage Analytics","text":"<pre><code>/tool_stats\n\n\ud83d\udcca Tool Usage Analytics (Last 24h):\n\u251c\u2500 Most Used: read_file (47 calls)\n\u251c\u2500 Fastest Avg: list_directory (0.3s avg)\n\u251c\u2500 Slowest Avg: backup_database (42.1s avg)\n\u251c\u2500 Highest Success Rate: send_email (100%)\n\u251c\u2500 Lowest Success Rate: http_request (87%)\n\u2514\u2500 Total Tool Calls: 234\n</code></pre>"},{"location":"features/tool-orchestration/#advanced-orchestration-features","title":"Advanced Orchestration Features","text":""},{"location":"features/tool-orchestration/#dynamic-tool-loading","title":"Dynamic Tool Loading","text":"<p>Add new tools during runtime:</p> <pre><code>/add_servers:new_tools.json\n\n\ud83d\udd27 New tools discovered:\n\u251c\u2500 image_processing server: 4 new tools\n\u251c\u2500 ml_analysis server: 7 new tools\n\u2514\u2500 Updated tool registry: 23 \u2192 34 tools\n\nAuto-integration complete: New tools available immediately\n</code></pre>"},{"location":"features/tool-orchestration/#tool-composition-templates","title":"Tool Composition Templates","text":"<p>Save common workflow patterns:</p> <pre><code># Save workflow as template\n/save_workflow:database_maintenance\n\n# Template includes:\n\u251c\u2500 backup_database\n\u251c\u2500 analyze_performance  \n\u251c\u2500 optimize_indexes\n\u251c\u2500 update_statistics\n\u2514\u2500 send_completion_report\n\n# Reuse template\n/execute_workflow:database_maintenance\n</code></pre>"},{"location":"features/tool-orchestration/#custom-tool-routing","title":"Custom Tool Routing","text":"<p>Override automatic tool selection:</p> <pre><code># Force specific tool selection\n&gt; Use the backup tool from the primary database server to backup the users table\n\n\ud83e\udd16 Custom routing applied:\n\u251c\u2500 Server: primary-database (forced)\n\u251c\u2500 Tool: backup_database (forced)\n\u251c\u2500 Parameters: table=users\n\u2514\u2500 Alternative tools ignored\n</code></pre>"},{"location":"features/tool-orchestration/#troubleshooting-tool-orchestration","title":"Troubleshooting Tool Orchestration","text":""},{"location":"features/tool-orchestration/#common-issues","title":"Common Issues","text":"<p>Tool Not Found</p> <p>Error: <code>Tool 'unknown_tool' not found</code></p> <p>Solutions: <pre><code># Check available tools\n/tools\n\n# Refresh server capabilities\n/refresh\n\n# Check server connections\n/connections\n</code></pre></p> <p>Tool Execution Timeout</p> <p>Error: <code>Tool execution timeout after 30s</code></p> <p>Solutions: <pre><code># Increase timeout in configuration\n{\n    \"AgentConfig\": {\n        \"tool_call_timeout\": 60\n    }\n}\n\n# Check server performance\n/debug\n\n# Try manual tool execution\n/prompt:tool_name/params=values\n</code></pre></p>"},{"location":"features/tool-orchestration/#performance-issues","title":"Performance Issues","text":"<p>Slow Tool Execution</p> <p>Issue: Tools taking longer than expected</p> <p>Diagnosis: <pre><code># Enable performance monitoring\n/debug\n\n# Check server status\n/connections\n\n# View tool statistics\n/tool_stats\n</code></pre></p>"},{"location":"features/tool-orchestration/#best-practices","title":"Best Practices","text":"<p>Orchestration Best Practices</p> <ol> <li>Start Simple: Begin with single-tool operations</li> <li>Monitor Performance: Use <code>/debug</code> for complex workflows</li> <li>Handle Errors: Plan for tool failures</li> <li>Use Caching: Enable caching for repeated operations</li> <li>Optimize Parallel: Identify independent operations</li> </ol> <p>Next: Resource Management \u2192 </p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing MCPOmni Connect, ensure you have the following:</p> <p>System Requirements</p> <ul> <li>Python 3.10+ (Python 3.11+ recommended)</li> <li>LLM API key from any supported provider</li> <li>UV package manager (recommended) or pip</li> <li>Redis server (optional, for persistent memory)</li> </ul>"},{"location":"getting-started/installation/#check-python-version","title":"Check Python Version","text":"<pre><code>python --version\n# Should show Python 3.10.0 or higher\n</code></pre>"},{"location":"getting-started/installation/#install-uv-recommended","title":"Install UV (Recommended)","text":"<p>UV is the fastest Python package manager and is recommended for MCPOmni Connect:</p> macOS/LinuxWindowsPython pip <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <pre><code>powershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre> <pre><code>pip install uv\n</code></pre>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#method-1-uv-recommended","title":"Method 1: UV (Recommended)","text":"<pre><code>uv add mcpomni-connect\n</code></pre>"},{"location":"getting-started/installation/#method-2-pip","title":"Method 2: pip","text":"<pre><code>pip install mcpomni-connect\n</code></pre>"},{"location":"getting-started/installation/#method-3-from-source","title":"Method 3: From Source","text":"<p>For development or latest features:</p> <pre><code>git clone https://github.com/Abiorh001/mcp_omni_connect.git\ncd mcp_omni_connect\nuv sync\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>After installation, verify MCPOmni Connect is correctly installed:</p> <pre><code>mcpomni_connect --version\n</code></pre> <p>You should see the version number displayed.</p>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#redis-for-persistent-memory","title":"Redis (For Persistent Memory)","text":"<p>MCPOmni Connect can use Redis for persistent conversation memory:</p> Ubuntu/DebianmacOSWindowsDocker <pre><code>sudo apt update\nsudo apt install redis-server\nsudo systemctl start redis-server\nsudo systemctl enable redis-server\n</code></pre> <pre><code>brew install redis\nbrew services start redis\n</code></pre> <p>Download from Redis Windows releases or use WSL with Linux instructions.</p> <pre><code>docker run -d --name redis -p 6379:6379 redis:alpine\n</code></pre>"},{"location":"getting-started/installation/#verify-redis-connection","title":"Verify Redis Connection","text":"<pre><code>redis-cli ping\n# Should respond with: PONG\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once installation is complete:</p> <ol> <li>Set up configuration - Create your <code>.env</code> and <code>servers_config.json</code> files</li> <li>Follow the Quick Start guide - Get your first MCP connection working</li> <li>Explore operation modes - Learn about chat, autonomous, and orchestrator modes</li> </ol>"},{"location":"getting-started/installation/#troubleshooting-installation","title":"Troubleshooting Installation","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<p>Python Version Error</p> <p>Error: <code>MCPOmni Connect requires Python 3.10+</code></p> <p>Solution: Upgrade your Python version: <pre><code># Check available Python versions\npython3.10 --version  # or python3.11, python3.12\n\n# Use specific Python version with UV\nuv python install 3.11\nuv add mcpomni-connect\n</code></pre></p> <p>Permission Denied</p> <p>Error: Permission denied during installation</p> <p>Solution: Use user installation: <pre><code>pip install --user mcpomni-connect\n</code></pre></p> <p>Command Not Found</p> <p>Error: <code>mcpomni_connect: command not found</code></p> <p>Solution: Add to PATH or use full path: <pre><code># Check installation path\npip show mcpomni-connect\n\n# Or run with python -m\npython -m mcpomni_connect\n</code></pre></p>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the troubleshooting guide</li> <li>Search existing issues</li> <li>Create a new issue with:</li> <li>Your operating system</li> <li>Python version</li> <li>Installation method used</li> <li>Complete error message</li> </ol> <p>Next: Quick Start Guide \u2192 </p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>This guide will get you up and running with MCPOmni Connect in under 5 minutes.</p>"},{"location":"getting-started/quick-start/#step-1-basic-configuration","title":"Step 1: Basic Configuration","text":"<p>Create the two required configuration files:</p>"},{"location":"getting-started/quick-start/#create-env-file","title":"Create <code>.env</code> file","text":"<pre><code># Create .env file with your LLM API key\necho \"LLM_API_KEY=your_api_key_here\" &gt; .env\n</code></pre> <p>Supported API Keys</p> <p>You can use API keys from OpenAI, Anthropic, Google, Groq, or any other supported LLM provider.</p>"},{"location":"getting-started/quick-start/#create-servers_configjson","title":"Create <code>servers_config.json</code>","text":"<pre><code>cat &gt; servers_config.json &lt;&lt; 'EOF'\n{\n    \"AgentConfig\": {\n        \"tool_call_timeout\": 30,\n        \"max_steps\": 15,\n        \"request_limit\": 1000,\n        \"total_tokens_limit\": 100000\n    },\n    \"LLM\": {\n        \"provider\": \"openai\",\n        \"model\": \"gpt-4o-mini\",\n        \"temperature\": 0.5,\n        \"max_tokens\": 5000,\n        \"top_p\": 0.7\n    },\n    \"mcpServers\": {}\n}\nEOF\n</code></pre>"},{"location":"getting-started/quick-start/#step-2-start-mcpomni-connect","title":"Step 2: Start MCPOmni Connect","text":"<pre><code>mcpomni_connect\n</code></pre> <p>You should see the MCPOmni Connect CLI start up:</p> <pre><code>\ud83d\ude80 MCPOmni Connect - Universal Gateway to MCP Servers\nConnected to 0 MCP servers\nMode: CHAT (type /mode:auto for autonomous mode)\n\n&gt; \n</code></pre>"},{"location":"getting-started/quick-start/#step-3-test-basic-functionality","title":"Step 3: Test Basic Functionality","text":"<p>Try these commands to verify everything is working:</p>"},{"location":"getting-started/quick-start/#check-available-commands","title":"Check Available Commands","text":"<pre><code>/help\n</code></pre>"},{"location":"getting-started/quick-start/#test-llm-connection","title":"Test LLM Connection","text":"<pre><code>Hello! Can you tell me about yourself?\n</code></pre> <p>The AI should respond, confirming your LLM configuration is working.</p>"},{"location":"getting-started/quick-start/#step-4-add-your-first-mcp-server","title":"Step 4: Add Your First MCP Server","text":"<p>Let's add a simple MCP server to demonstrate connectivity:</p>"},{"location":"getting-started/quick-start/#option-a-file-system-server-local","title":"Option A: File System Server (Local)","text":"<p>Edit your <code>servers_config.json</code> to add a file system server:</p> <pre><code>{\n    \"AgentConfig\": {\n        \"tool_call_timeout\": 30,\n        \"max_steps\": 15,\n        \"request_limit\": 1000,\n        \"total_tokens_limit\": 100000\n    },\n    \"LLM\": {\n        \"provider\": \"openai\",\n        \"model\": \"gpt-4o-mini\",\n        \"temperature\": 0.5,\n        \"max_tokens\": 5000,\n        \"top_p\": 0.7\n    },\n    \"mcpServers\": {\n        \"filesystem\": {\n            \"transport_type\": \"stdio\",\n            \"command\": \"uvx\",\n            \"args\": [\"mcp-server-filesystem\", \"/tmp\"]\n        }\n    }\n}\n</code></pre>"},{"location":"getting-started/quick-start/#option-b-remote-http-server","title":"Option B: Remote HTTP Server","text":"<pre><code>{\n    \"mcpServers\": {\n        \"remote-server\": {\n            \"transport_type\": \"streamable_http\",\n            \"url\": \"http://your-server.com:8080/mcp\",\n            \"headers\": {\n                \"Authorization\": \"Bearer your-token\"\n            },\n            \"timeout\": 60\n        }\n    }\n}\n</code></pre>"},{"location":"getting-started/quick-start/#restart-and-test","title":"Restart and Test","text":"<pre><code># Restart MCPOmni Connect\nmcpomni_connect\n</code></pre> <p>Now check available tools: <pre><code>/tools\n</code></pre></p> <p>You should see tools from your connected MCP server!</p>"},{"location":"getting-started/quick-start/#step-5-try-different-operation-modes","title":"Step 5: Try Different Operation Modes","text":""},{"location":"getting-started/quick-start/#chat-mode-default","title":"Chat Mode (Default)","text":"<p><pre><code>Can you list the files in the current directory?\n</code></pre> The AI will ask for approval before executing tools</p>"},{"location":"getting-started/quick-start/#autonomous-mode","title":"Autonomous Mode","text":"<p><pre><code>/mode:auto\nCan you analyze the files in the current directory and create a summary?\n</code></pre> The AI will execute tasks independently</p>"},{"location":"getting-started/quick-start/#switch-back-to-chat-mode","title":"Switch Back to Chat Mode","text":"<pre><code>/mode:chat\n</code></pre>"},{"location":"getting-started/quick-start/#common-first-tasks","title":"Common First Tasks","text":""},{"location":"getting-started/quick-start/#explore-available-capabilities","title":"Explore Available Capabilities","text":"<pre><code>/tools      # List all available tools\n/prompts    # Show available prompts\n/resources  # Display available resources\n</code></pre>"},{"location":"getting-started/quick-start/#memory-management","title":"Memory Management","text":"<pre><code>/memory     # Toggle Redis memory persistence\n</code></pre>"},{"location":"getting-started/quick-start/#debug-mode","title":"Debug Mode","text":"<pre><code>/debug      # Enable detailed logging for troubleshooting\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you have MCPOmni Connect running:</p> <ol> <li>Configure additional LLM providers - Try different AI models</li> <li>Add more MCP servers - Connect to databases, APIs, and tools</li> <li>Explore advanced features - Learn about ReAct agents and orchestration</li> <li>Set up authentication - Configure OAuth and secure connections</li> </ol>"},{"location":"getting-started/quick-start/#troubleshooting-quick-start","title":"Troubleshooting Quick Start","text":"<p>Connection Failed</p> <p>If you see \"Failed to connect to server\":</p> <ol> <li>Check your <code>servers_config.json</code> syntax</li> <li>Verify the MCP server is actually running</li> <li>See the troubleshooting guide</li> </ol> <p>API Key Error</p> <p>If you see \"Invalid API key\":</p> <ol> <li>Verify your <code>.env</code> file contains the correct key</li> <li>Check you're using the right provider in <code>servers_config.json</code></li> <li>Ensure the API key has proper permissions</li> </ol> <p>Command Not Found</p> <p>If <code>mcpomni_connect</code> command isn't found:</p> <ol> <li>Try <code>python -m mcpomni_connect</code></li> <li>Check your PATH includes the installation directory</li> <li>Reinstall with <code>pip install --user mcpomni-connect</code></li> </ol> <p>Getting Help</p> <ul> <li>Join our GitHub Discussions</li> <li>Check existing issues</li> <li>Read the full configuration guide</li> </ul> <p>Congratulations! \ud83c\udf89 You now have MCPOmni Connect running. Ready to explore more advanced features?</p> <p>Next: Configuration Guide \u2192 </p>"},{"location":"user-guide/basic-usage/","title":"Basic Usage","text":"<p>This guide covers the everyday usage patterns and commands for MCPOmni Connect.</p>"},{"location":"user-guide/basic-usage/#starting-mcpomni-connect","title":"Starting MCPOmni Connect","text":"<pre><code># Start with default configuration\nmcpomni_connect\n\n# Start with custom config file\nmcpomni_connect --config custom.servers_config.json\n\n# Start with debug mode enabled\nmcpomni_connect --debug\n</code></pre>"},{"location":"user-guide/basic-usage/#understanding-the-interface","title":"Understanding the Interface","text":"<p>When you start MCPOmni Connect, you'll see:</p> <pre><code>\ud83d\ude80 MCPOmni Connect - Universal Gateway to MCP Servers\nConnected to 3 MCP servers: filesystem, database, notifications\nMode: CHAT (type /mode:auto for autonomous mode)\n\n&gt; \n</code></pre>"},{"location":"user-guide/basic-usage/#interface-elements","title":"Interface Elements","text":"<ul> <li>Connection Status: Shows how many MCP servers are connected</li> <li>Current Mode: CHAT (interactive) or AUTO (autonomous)</li> <li>Prompt: <code>&gt;</code> indicates MCPOmni Connect is ready for input</li> </ul>"},{"location":"user-guide/basic-usage/#command-types","title":"Command Types","text":"<p>MCPOmni Connect accepts two types of input:</p>"},{"location":"user-guide/basic-usage/#1-system-commands-start-with","title":"1. System Commands (start with <code>/</code>)","text":"<p>System commands control MCPOmni Connect's behavior:</p> <pre><code>/tools              # List available tools\n/help              # Show help information\n/mode:auto         # Switch to autonomous mode\n/debug             # Toggle debug mode\n</code></pre>"},{"location":"user-guide/basic-usage/#2-natural-language-queries","title":"2. Natural Language Queries","text":"<p>Direct questions or requests to the AI:</p> <pre><code>List the files in the current directory\nWhat tools are available for database operations?\nCan you help me analyze this log file?\n</code></pre>"},{"location":"user-guide/basic-usage/#essential-commands","title":"Essential Commands","text":""},{"location":"user-guide/basic-usage/#information-commands","title":"Information Commands","text":"<pre><code># Show all available tools across servers\n/tools\n\n# List available prompts\n/prompts\n\n# Display available resources\n/resources\n\n# Show current API usage statistics\n/api_stats\n\n# Get help information\n/help\n</code></pre>"},{"location":"user-guide/basic-usage/#mode-management","title":"Mode Management","text":"<pre><code># Switch to autonomous mode (AI acts independently)\n/mode:auto\n\n# Switch to interactive chat mode (requires approval)\n/mode:chat\n\n# Check current mode\n/status\n</code></pre>"},{"location":"user-guide/basic-usage/#server-management","title":"Server Management","text":"<pre><code># Refresh server capabilities\n/refresh\n\n# Add new servers from config file\n/add_servers:path/to/new_config.json\n\n# Remove a server\n/remove_server:server_name\n</code></pre>"},{"location":"user-guide/basic-usage/#memory-and-history","title":"Memory and History","text":"<pre><code># Toggle Redis memory persistence\n/memory\n\n# Save current conversation to file\n/save:conversation.json\n\n# Load previous conversation\n/load:conversation.json\n</code></pre>"},{"location":"user-guide/basic-usage/#debugging","title":"Debugging","text":"<pre><code># Toggle debug mode for detailed logging\n/debug\n\n# Show detailed connection information\n/connections\n</code></pre>"},{"location":"user-guide/basic-usage/#working-with-tools","title":"Working with Tools","text":""},{"location":"user-guide/basic-usage/#discovering-tools","title":"Discovering Tools","text":"<pre><code># List all available tools\n/tools\n</code></pre> <p>Example output: <pre><code>Available tools across 3 servers:\n\n\ud83d\udcc1 filesystem server:\n  - read_file: Read contents of a file\n  - write_file: Write content to a file\n  - list_directory: List directory contents\n\n\ud83d\uddc3\ufe0f  database server:\n  - query_database: Execute SQL queries\n  - get_schema: Get database schema information\n\n\ud83d\udce7 notifications server:\n  - send_email: Send email notifications\n  - create_alert: Create system alerts\n</code></pre></p>"},{"location":"user-guide/basic-usage/#using-tools-in-chat-mode","title":"Using Tools in Chat Mode","text":"<p>In chat mode, the AI will ask for permission before executing tools:</p> <pre><code>&gt; Can you read the contents of config.json?\n\nI'll help you read the contents of config.json. Let me use the file reading tool.\n\n\ud83d\udd27 Tool: read_file\n\ud83d\udcc1 File: config.json\n\u2753 Execute this tool? (y/n): y\n\n[Tool executes and shows results]\n</code></pre>"},{"location":"user-guide/basic-usage/#using-tools-in-autonomous-mode","title":"Using Tools in Autonomous Mode","text":"<p>In autonomous mode, tools execute automatically:</p> <pre><code>&gt; /mode:auto\nNow operating in AUTONOMOUS mode.\n\n&gt; Analyze all log files and create a summary report\n\n\ud83e\udd16 Executing autonomous task...\n\ud83d\udd27 Using tool: list_directory \u2192 found 5 log files\n\ud83d\udd27 Using tool: read_file \u2192 analyzing error.log\n\ud83d\udd27 Using tool: read_file \u2192 analyzing access.log\n...\n\ud83d\udcca Summary report created successfully\n</code></pre>"},{"location":"user-guide/basic-usage/#working-with-prompts","title":"Working with Prompts","text":""},{"location":"user-guide/basic-usage/#discovering-prompts","title":"Discovering Prompts","text":"<pre><code># List all available prompts\n/prompts\n</code></pre>"},{"location":"user-guide/basic-usage/#executing-prompts","title":"Executing Prompts","text":"<p>Prompts are pre-defined templates with specific parameters:</p> <pre><code># Basic prompt execution\n/prompt:weather/location=tokyo\n\n# Multiple parameters\n/prompt:travel-planner/from=london/to=paris/date=2024-03-25\n\n# JSON format for complex arguments\n/prompt:analyze-data/{\n    \"dataset\": \"sales_2024\",\n    \"metrics\": [\"revenue\", \"growth\"],\n    \"filters\": {\n        \"region\": \"europe\",\n        \"period\": \"q1\"\n    }\n}\n</code></pre>"},{"location":"user-guide/basic-usage/#working-with-resources","title":"Working with Resources","text":""},{"location":"user-guide/basic-usage/#discovering-resources","title":"Discovering Resources","text":"<pre><code># List all available resources\n/resources\n</code></pre>"},{"location":"user-guide/basic-usage/#accessing-resources","title":"Accessing Resources","text":"<pre><code># Access a specific resource\n/resource:file:///path/to/document.pdf\n/resource:http://example.com/api/data\n/resource:database://connection/table\n</code></pre>"},{"location":"user-guide/basic-usage/#common-usage-patterns","title":"Common Usage Patterns","text":""},{"location":"user-guide/basic-usage/#file-operations","title":"File Operations","text":"<pre><code># Read and analyze files\nCan you read config.json and explain the settings?\n\n# Batch file operations\nPlease read all .log files and find any error patterns\n\n# File creation\nCreate a new README.md file with project documentation\n</code></pre>"},{"location":"user-guide/basic-usage/#data-analysis","title":"Data Analysis","text":"<pre><code># Database queries\nShow me the top 10 customers by revenue this month\n\n# Log analysis\nAnalyze the error logs and categorize the issues\n\n# Report generation\nCreate a summary report of yesterday's user activity\n</code></pre>"},{"location":"user-guide/basic-usage/#system-management","title":"System Management","text":"<pre><code># Server monitoring\nCheck the status of all connected services\n\n# Configuration management\nCompare the current config with the backup version\n\n# Deployment tasks\nDeploy the latest changes to the staging environment\n</code></pre>"},{"location":"user-guide/basic-usage/#conversation-flow","title":"Conversation Flow","text":""},{"location":"user-guide/basic-usage/#interactive-session-example","title":"Interactive Session Example","text":"<pre><code>&gt; /tools\n[Shows available tools]\n\n&gt; Can you help me find all Python files in this project?\n\nI'll help you find Python files. Let me list the directory contents \nand look for .py files.\n\n\ud83d\udd27 Tool: list_directory\n\ud83d\udcc1 Path: .\n\u2753 Execute this tool? (y/n): y\n\nFound the following Python files:\n- src/main.py\n- tests/test_main.py\n- scripts/deploy.py\n\nWould you like me to analyze any of these files?\n\n&gt; Yes, please analyze the main.py file\n\n\ud83d\udd27 Tool: read_file\n\ud83d\udcc1 File: src/main.py\n\u2753 Execute this tool? (y/n): y\n\n[File analysis results]\n</code></pre>"},{"location":"user-guide/basic-usage/#autonomous-session-example","title":"Autonomous Session Example","text":"<pre><code>&gt; /mode:auto\n\n&gt; Create a backup of all configuration files\n\n\ud83e\udd16 Planning autonomous task: Create backup of configuration files\n\nStep 1: \ud83d\udd27 list_directory \u2192 scanning for config files\nStep 2: \ud83d\udd27 read_file \u2192 reading config.json\nStep 3: \ud83d\udd27 read_file \u2192 reading settings.yml\nStep 4: \ud83d\udd27 write_file \u2192 creating backup/config_backup_2024-01-15.tar.gz\n\n\u2705 Task completed: Created backup containing 5 configuration files\n</code></pre>"},{"location":"user-guide/basic-usage/#memory-management","title":"Memory Management","text":""},{"location":"user-guide/basic-usage/#using-redis-memory","title":"Using Redis Memory","text":"<pre><code># Enable persistent memory\n/memory\nMemory persistence is now ENABLED using Redis\n\n# Your conversations will be saved and restored\n&gt; Hello, I'm working on a Python project\n[AI responds and remembers this context]\n\n# Later session\n&gt; What were we working on last time?\n[AI recalls: \"You were working on a Python project...\"]\n</code></pre>"},{"location":"user-guide/basic-usage/#file-based-history","title":"File-Based History","text":"<pre><code># Save current conversation\n/save:project_discussion.json\nConversation saved to project_discussion.json\n\n# Load previous conversation\n/load:project_discussion.json\nConversation loaded from project_discussion.json\n</code></pre>"},{"location":"user-guide/basic-usage/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/basic-usage/#effective-communication","title":"Effective Communication","text":"<p>Clear Instructions</p> <ul> <li>Be specific about what you want</li> <li>Mention relevant file paths or parameters</li> <li>Ask follow-up questions for clarification</li> </ul> <pre><code># \u2705 Good\nRead the config.json file and explain the database settings\n\n# \u274c Vague\nTell me about the config\n</code></pre>"},{"location":"user-guide/basic-usage/#mode-selection","title":"Mode Selection","text":"<p>Choose the Right Mode</p> <ul> <li>Chat Mode: When you want control and oversight</li> <li>Autonomous Mode: For well-defined tasks you trust the AI to complete</li> </ul>"},{"location":"user-guide/basic-usage/#error-handling","title":"Error Handling","text":"<p>When Things Go Wrong</p> <ol> <li>Enable debug mode: <code>/debug</code></li> <li>Check server status: <code>/connections</code></li> <li>Verify configuration: <code>/refresh</code></li> <li>Restart with clean state if needed</li> </ol>"},{"location":"user-guide/basic-usage/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"user-guide/basic-usage/#tool-not-found","title":"Tool Not Found","text":"<pre><code>&gt; Use the database tool to query users\n\n\u274c Error: No database tools found\n\nSolutions:\n1. Check if database server is connected: /tools\n2. Verify server config: /connections\n3. Add the server: /add_servers:db_config.json\n</code></pre>"},{"location":"user-guide/basic-usage/#permission-denied","title":"Permission Denied","text":"<pre><code>&gt; Read the sensitive file\n\n\u274c Error: Permission denied\n\nSolutions:\n1. Check file permissions\n2. Run with appropriate user privileges\n3. Verify server has access to the file path\n</code></pre>"},{"location":"user-guide/basic-usage/#connection-lost","title":"Connection Lost","text":"<pre><code>\u274c Error: Connection to server 'database' lost\n\nSolutions:\n1. Check if the server is still running\n2. Refresh connections: /refresh\n3. Restart MCPOmni Connect\n4. Verify network connectivity\n</code></pre> <p>Next: Operation Modes \u2192 </p>"},{"location":"user-guide/commands/","title":"Commands Reference","text":"<p>MCPOmni Connect provides a comprehensive set of commands to control its behavior and interact with connected MCP servers.</p>"},{"location":"user-guide/commands/#command-types","title":"Command Types","text":"<p>MCPOmni Connect accepts two types of input:</p>"},{"location":"user-guide/commands/#system-commands-start-with","title":"System Commands (start with <code>/</code>)","text":"<p>Control MCPOmni Connect's behavior and settings</p>"},{"location":"user-guide/commands/#natural-language-queries","title":"Natural Language Queries","text":"<p>Direct questions or requests processed by the AI</p>"},{"location":"user-guide/commands/#system-commands","title":"System Commands","text":""},{"location":"user-guide/commands/#information-commands","title":"Information Commands","text":""},{"location":"user-guide/commands/#tools","title":"<code>/tools</code>","text":"<p>List all available tools across connected MCP servers.</p> <pre><code>/tools\n</code></pre> <p>Output Example: <pre><code>Available tools across 3 servers:\n\n\ud83d\udcc1 filesystem server:\n  - read_file: Read contents of a file\n  - write_file: Write content to a file\n  - list_directory: List directory contents\n\n\ud83d\uddc3\ufe0f  database server:\n  - query_database: Execute SQL queries\n  - get_schema: Get database schema information\n\n\ud83d\udce7 notifications server:\n  - send_email: Send email notifications\n  - create_alert: Create system alerts\n</code></pre></p>"},{"location":"user-guide/commands/#prompts","title":"<code>/prompts</code>","text":"<p>Display all available prompts from connected servers.</p> <pre><code>/prompts\n</code></pre> <p>Output Example: <pre><code>Available prompts:\n\n\ud83c\udf24\ufe0f  weather server:\n  - weather: Get weather information for a location\n    Args: location (required)\n\n\ud83d\udcca analytics server:\n  - generate_report: Create analytics report\n    Args: dataset, metrics, date_range\n</code></pre></p>"},{"location":"user-guide/commands/#resources","title":"<code>/resources</code>","text":"<p>List all available resources across servers.</p> <pre><code>/resources\n</code></pre>"},{"location":"user-guide/commands/#api_stats","title":"<code>/api_stats</code>","text":"<p>Show current API usage statistics and limits.</p> <pre><code>/api_stats\n</code></pre> <p>Output Example: <pre><code>\ud83d\udcca API Usage Statistics:\n- Total Tokens Used: 15,247 / 100,000\n- Total Requests: 45 / 1,000\n- Session Duration: 23 minutes\n- Average Tokens per Request: 339\n</code></pre></p>"},{"location":"user-guide/commands/#help","title":"<code>/help</code>","text":"<p>Display help information and available commands.</p> <pre><code>/help\n</code></pre>"},{"location":"user-guide/commands/#status","title":"<code>/status</code>","text":"<p>Show current system status and configuration.</p> <pre><code>/status\n</code></pre> <p>Output Example: <pre><code>\ud83d\ude80 MCPOmni Connect Status:\n- Mode: AUTONOMOUS\n- Connected Servers: 3\n- Memory: ENABLED (Redis)\n- Debug: OFF\n- Uptime: 1h 23m\n</code></pre></p>"},{"location":"user-guide/commands/#mode-management","title":"Mode Management","text":""},{"location":"user-guide/commands/#modechat","title":"<code>/mode:chat</code>","text":"<p>Switch to interactive chat mode (requires approval for actions).</p> <pre><code>/mode:chat\n</code></pre> <p>Response: <pre><code>Now operating in CHAT mode. I will ask for approval before executing tasks.\n</code></pre></p>"},{"location":"user-guide/commands/#modeauto","title":"<code>/mode:auto</code>","text":"<p>Switch to autonomous mode (independent execution).</p> <pre><code>/mode:auto\n</code></pre> <p>Response: <pre><code>Now operating in AUTONOMOUS mode. I will execute tasks independently.\n</code></pre></p>"},{"location":"user-guide/commands/#modeorchestrator","title":"<code>/mode:orchestrator</code>","text":"<p>Switch to orchestrator mode (complex multi-step coordination).</p> <pre><code>/mode:orchestrator\n</code></pre> <p>Response: <pre><code>Now operating in ORCHESTRATOR mode. I will coordinate complex multi-step tasks.\n</code></pre></p>"},{"location":"user-guide/commands/#server-management","title":"Server Management","text":""},{"location":"user-guide/commands/#refresh","title":"<code>/refresh</code>","text":"<p>Refresh server capabilities and reconnect if needed.</p> <pre><code>/refresh\n</code></pre> <p>Use Cases: - Server capabilities have changed - Connection issues - After server restart</p>"},{"location":"user-guide/commands/#add_serversconfig_file","title":"<code>/add_servers:&lt;config_file&gt;</code>","text":"<p>Add new servers from a configuration file.</p> <pre><code>/add_servers:new_servers.json\n</code></pre> <p>Example <code>new_servers.json</code>: <pre><code>{\n    \"new-database\": {\n        \"transport_type\": \"streamable_http\",\n        \"url\": \"http://localhost:5432/mcp\",\n        \"headers\": {\n            \"Authorization\": \"Bearer db-token\"\n        }\n    },\n    \"file-processor\": {\n        \"transport_type\": \"stdio\",\n        \"command\": \"uvx\",\n        \"args\": [\"mcp-file-processor\"]\n    }\n}\n</code></pre></p>"},{"location":"user-guide/commands/#remove_serverserver_name","title":"<code>/remove_server:&lt;server_name&gt;</code>","text":"<p>Remove a server by name.</p> <pre><code>/remove_server:old-database\n</code></pre> <p>Response: <pre><code>\u2705 Server 'old-database' removed successfully.\nConnected servers: 2 (filesystem, notifications)\n</code></pre></p>"},{"location":"user-guide/commands/#connections","title":"<code>/connections</code>","text":"<p>Show detailed information about current server connections.</p> <pre><code>/connections\n</code></pre>"},{"location":"user-guide/commands/#memory-and-history","title":"Memory and History","text":""},{"location":"user-guide/commands/#memory","title":"<code>/memory</code>","text":"<p>Toggle Redis memory persistence on/off.</p> <pre><code>/memory\n</code></pre> <p>Responses: <pre><code># When enabling\nMemory persistence is now ENABLED using Redis\nConversations will be saved and restored across sessions.\n\n# When disabling  \nMemory persistence is now DISABLED\nConversations will not be saved to Redis.\n</code></pre></p>"},{"location":"user-guide/commands/#savefilename","title":"<code>/save:&lt;filename&gt;</code>","text":"<p>Save current conversation to a file.</p> <pre><code>/save:project_discussion.json\n</code></pre> <p>Response: <pre><code>\u2705 Conversation saved to project_discussion.json\n</code></pre></p>"},{"location":"user-guide/commands/#loadfilename","title":"<code>/load:&lt;filename&gt;</code>","text":"<p>Load a previous conversation from file.</p> <pre><code>/load:project_discussion.json\n</code></pre> <p>Response: <pre><code>\u2705 Conversation loaded from project_discussion.json\nLoaded 23 messages from previous session.\n</code></pre></p>"},{"location":"user-guide/commands/#clear","title":"<code>/clear</code>","text":"<p>Clear current conversation history.</p> <pre><code>/clear\n</code></pre>"},{"location":"user-guide/commands/#debugging-and-diagnostics","title":"Debugging and Diagnostics","text":""},{"location":"user-guide/commands/#debug","title":"<code>/debug</code>","text":"<p>Toggle debug mode for detailed logging.</p> <pre><code>/debug\n</code></pre> <p>Responses: <pre><code># When enabling\n\ud83d\udc1b Debug mode ENABLED - Detailed logging active\nYou'll see verbose information about tool calls and server communication.\n\n# When disabling\n\ud83d\udc1b Debug mode DISABLED - Normal logging restored\n</code></pre></p>"},{"location":"user-guide/commands/#history","title":"<code>/history</code>","text":"<p>Show recent command history (in autonomous/orchestrator modes).</p> <pre><code>/history\n</code></pre>"},{"location":"user-guide/commands/#prompt-execution-commands","title":"Prompt Execution Commands","text":""},{"location":"user-guide/commands/#basic-prompt-execution","title":"Basic Prompt Execution","text":""},{"location":"user-guide/commands/#promptprompt_nameargs","title":"<code>/prompt:&lt;prompt_name&gt;/&lt;args&gt;</code>","text":"<p>Execute a prompt with simple arguments.</p> <pre><code>/prompt:weather/location=tokyo\n/prompt:file_search/pattern=*.py/directory=src\n</code></pre>"},{"location":"user-guide/commands/#advanced-prompt-execution","title":"Advanced Prompt Execution","text":""},{"location":"user-guide/commands/#json-format-arguments","title":"JSON Format Arguments","text":"<p>For complex nested arguments:</p> <pre><code>/prompt:analytics_report/{\n    \"dataset\": \"sales_2024\",\n    \"metrics\": [\"revenue\", \"growth\", \"conversion\"],\n    \"filters\": {\n        \"region\": \"north_america\",\n        \"period\": \"Q1\",\n        \"product_category\": \"software\"\n    },\n    \"output_format\": \"pdf\"\n}\n</code></pre>"},{"location":"user-guide/commands/#multiple-arguments","title":"Multiple Arguments","text":"<pre><code>/prompt:database_query/table=users/columns=name,email,created_at/limit=100\n</code></pre>"},{"location":"user-guide/commands/#prompt-examples-by-type","title":"Prompt Examples by Type","text":"Simple ParametersMultiple ParametersComplex JSON <pre><code>/prompt:weather/location=london\n/prompt:translate/text=hello/target_lang=spanish\n/prompt:file_info/path=/etc/hosts\n</code></pre> <pre><code>/prompt:search/query=python/type=files/directory=src\n/prompt:backup/source=/data/files/destination=/backup/target\n/prompt:deploy/env=staging/version=1.2.3/notify=true\n</code></pre> <pre><code>/prompt:complex_analysis/{\n    \"input\": {\n        \"data_source\": \"database\",\n        \"query\": \"SELECT * FROM orders WHERE date &gt; '2024-01-01'\",\n        \"format\": \"json\"\n    },\n    \"processing\": {\n        \"aggregate_by\": [\"month\", \"product\"],\n        \"metrics\": [\"sum\", \"average\", \"count\"],\n        \"filters\": {\n            \"status\": \"completed\",\n            \"amount\": {\"min\": 100, \"max\": 10000}\n        }\n    },\n    \"output\": {\n        \"format\": \"chart\",\n        \"type\": \"bar\",\n        \"save_to\": \"reports/monthly_sales.png\"\n    }\n}\n</code></pre>"},{"location":"user-guide/commands/#resource-access-commands","title":"Resource Access Commands","text":""},{"location":"user-guide/commands/#resourceuri","title":"<code>/resource:&lt;uri&gt;</code>","text":"<p>Access and analyze a specific resource.</p> <pre><code>/resource:file:///path/to/document.pdf\n/resource:http://example.com/api/data\n/resource:database://connection/table_name\n</code></pre> <p>Examples: <pre><code># File resources\n/resource:file:///home/user/report.pdf\n/resource:file://./config.json\n\n# HTTP resources\n/resource:http://api.github.com/repos/owner/repo\n/resource:https://example.com/data.csv\n\n# Database resources\n/resource:database://localhost:5432/mydb/users\n</code></pre></p>"},{"location":"user-guide/commands/#command-combinations-and-workflows","title":"Command Combinations and Workflows","text":""},{"location":"user-guide/commands/#typical-workflow-commands","title":"Typical Workflow Commands","text":"<pre><code># 1. Check available capabilities\n/tools\n/prompts\n/resources\n\n# 2. Switch to appropriate mode\n/mode:auto\n\n# 3. Execute tasks\nProcess all log files and create a summary report\n\n# 4. Check progress and usage\n/api_stats\n/debug\n\n# 5. Save work\n/save:log_analysis_session.json\n</code></pre>"},{"location":"user-guide/commands/#debugging-workflow","title":"Debugging Workflow","text":"<pre><code># 1. Enable debug mode\n/debug\n\n# 2. Check connections\n/connections\n/status\n\n# 3. Refresh if needed\n/refresh\n\n# 4. Test simple operation\n/tools\n\n# 5. Try basic prompt\n/prompt:simple_test/input=hello\n</code></pre>"},{"location":"user-guide/commands/#server-management-workflow","title":"Server Management Workflow","text":"<pre><code># 1. Check current servers\n/status\n\n# 2. Add new servers\n/add_servers:additional_servers.json\n\n# 3. Verify connections\n/connections\n/refresh\n\n# 4. Test new capabilities\n/tools\n/prompts\n\n# 5. Remove old servers if needed\n/remove_server:old_server_name\n</code></pre>"},{"location":"user-guide/commands/#command-parameters-and-options","title":"Command Parameters and Options","text":""},{"location":"user-guide/commands/#global-parameters","title":"Global Parameters","text":"<p>Most commands support these modifiers:</p> Parameter Description Example <code>--verbose</code> Detailed output <code>/tools --verbose</code> <code>--json</code> JSON format output <code>/status --json</code> <code>--help</code> Command-specific help <code>/prompt --help</code>"},{"location":"user-guide/commands/#mode-specific-behavior","title":"Mode-Specific Behavior","text":"<p>Commands behave differently based on current mode:</p> Command Chat Mode Auto Mode Orchestrator Mode <code>/tools</code> Shows tools + asks what to use Shows tools Shows tools + suggests workflows Natural queries Asks approval for each step Executes independently Plans and coordinates <code>/debug</code> Shows user-friendly info Shows execution details Shows coordination details"},{"location":"user-guide/commands/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/commands/#common-command-errors","title":"Common Command Errors","text":"<p>Command Not Found</p> <p>Error: <code>/unknown_command</code></p> <p>Response: <code>Unknown command. Type /help for available commands.</code></p> <p>Invalid Arguments</p> <p>Error: <code>/prompt:weather/invalid=args</code></p> <p>Response: <code>Invalid arguments for prompt 'weather'. Expected: location</code></p> <p>Server Not Connected</p> <p>Error: <code>/prompt:server_prompt/args=value</code></p> <p>Response: <code>Server not found. Use /connections to see available servers.</code></p>"},{"location":"user-guide/commands/#recovery-commands","title":"Recovery Commands","text":"<pre><code># Reset to known good state\n/mode:chat\n/refresh\n/clear\n\n# Check system health\n/status\n/connections\n/debug\n</code></pre>"},{"location":"user-guide/commands/#advanced-command-usage","title":"Advanced Command Usage","text":""},{"location":"user-guide/commands/#chaining-commands","title":"Chaining Commands","text":"<pre><code># Multiple commands in sequence\n/mode:auto &amp;&amp; /debug &amp;&amp; /tools\n</code></pre>"},{"location":"user-guide/commands/#conditional-execution","title":"Conditional Execution","text":"<pre><code># Execute based on conditions\n/status | grep \"CONNECTED\" &amp;&amp; /tools\n</code></pre>"},{"location":"user-guide/commands/#scripting-with-commands","title":"Scripting with Commands","text":"<pre><code># Save to script file\necho \"/mode:auto\" &gt; automation.txt\necho \"Process all CSV files in ./data/\" &gt;&gt; automation.txt\necho \"/save:csv_processing_results.json\" &gt;&gt; automation.txt\n\n# Execute script (future feature)\n/script:automation.txt\n</code></pre>"},{"location":"user-guide/commands/#tips-and-best-practices","title":"Tips and Best Practices","text":""},{"location":"user-guide/commands/#efficient-command-usage","title":"Efficient Command Usage","text":"<p>Command Efficiency</p> <ol> <li>Use shortcuts: <code>/t</code> instead of <code>/tools</code> (if available)</li> <li>Group related commands: Check status before major operations</li> <li>Save frequently: Use <code>/save:</code> for important sessions</li> <li>Monitor usage: Regular <code>/api_stats</code> checks</li> <li>Debug when needed: Enable <code>/debug</code> for troubleshooting</li> </ol>"},{"location":"user-guide/commands/#common-patterns","title":"Common Patterns","text":"<pre><code># Daily workflow start\n/status &amp;&amp; /refresh &amp;&amp; /tools\n\n# Before major operation\n/mode:chat &amp;&amp; /debug\n\n# After complex task\n/api_stats &amp;&amp; /save:session_$(date +%Y%m%d).json\n\n# Troubleshooting\n/debug &amp;&amp; /connections &amp;&amp; /refresh\n</code></pre> <p>Next: Memory Management \u2192 </p>"},{"location":"user-guide/memory-management/","title":"Memory Management","text":"<p>MCPOmni Connect provides flexible memory management through Redis persistence and file-based conversation history, allowing you to maintain context across sessions and save important conversations.</p>"},{"location":"user-guide/memory-management/#memory-types","title":"Memory Types","text":"Type Storage Persistence Use Case Session Memory RAM Current session only Active conversation Redis Memory Redis Database Across sessions Long-term context File History JSON Files Manual save/load Conversation backups"},{"location":"user-guide/memory-management/#redis-powered-persistence","title":"Redis-Powered Persistence","text":""},{"location":"user-guide/memory-management/#overview","title":"Overview","text":"<p>Redis memory provides automatic conversation persistence across MCPOmni Connect sessions, maintaining context and conversation history even after restarts.</p>"},{"location":"user-guide/memory-management/#setup-redis","title":"Setup Redis","text":"Ubuntu/DebianmacOSDockerWindows <pre><code>sudo apt update\nsudo apt install redis-server\nsudo systemctl start redis-server\nsudo systemctl enable redis-server\n</code></pre> <pre><code>brew install redis\nbrew services start redis\n</code></pre> <pre><code>docker run -d --name redis -p 6379:6379 redis:alpine\n</code></pre> <p>Download from Redis Windows releases or use WSL.</p>"},{"location":"user-guide/memory-management/#configuration","title":"Configuration","text":"<p>Configure Redis connection in your <code>.env</code> file:</p> .env<pre><code># Redis Configuration (optional)\nREDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_DB=0\nREDIS_PASSWORD=your_password  # if password protected\n</code></pre>"},{"location":"user-guide/memory-management/#using-redis-memory","title":"Using Redis Memory","text":""},{"location":"user-guide/memory-management/#enabledisable-memory-persistence","title":"Enable/Disable Memory Persistence","text":"<pre><code># Toggle memory persistence\n/memory\n</code></pre> <p>When Enabling: <pre><code>\u2705 Memory persistence is now ENABLED using Redis\nConversations will be saved and restored across sessions.\nConnected to Redis at localhost:6379 (DB: 0)\n</code></pre></p> <p>When Disabling: <pre><code>\u274c Memory persistence is now DISABLED\nConversations will not be saved to Redis.\nSession memory will be cleared on restart.\n</code></pre></p>"},{"location":"user-guide/memory-management/#check-memory-status","title":"Check Memory Status","text":"<pre><code>/status\n</code></pre> <p>Output: <pre><code>\ud83d\ude80 MCPOmni Connect Status:\n- Mode: CHAT\n- Connected Servers: 3\n- Memory: ENABLED (Redis localhost:6379)\n- Debug: OFF\n- Session Duration: 45 minutes\n</code></pre></p>"},{"location":"user-guide/memory-management/#how-redis-memory-works","title":"How Redis Memory Works","text":"<ol> <li>Automatic Saving: Conversations automatically saved to Redis</li> <li>Context Preservation: Maintains conversation context across restarts</li> <li>Session Restoration: Previous conversations loaded on startup</li> <li>Intelligent Pruning: Old conversations automatically cleaned up</li> </ol>"},{"location":"user-guide/memory-management/#example-session-with-redis","title":"Example Session with Redis","text":"<pre><code># Session 1\n&gt; /memory\nMemory persistence is now ENABLED using Redis\n\n&gt; Hello, I'm working on a Python project\nAI: Hello! I'd be happy to help with your Python project. What specific \n    aspect are you working on?\n\n&gt; I need to analyze some CSV files\nAI: Great! I can help you analyze CSV files. Do you have the files ready?\n\n# Exit MCPOmni Connect\n\n# Session 2 (later)\n&gt; mcpomni_connect\n\ud83d\ude80 MCPOmni Connect - Universal Gateway to MCP Servers\n\ud83d\udcda Restored conversation from Redis (5 messages)\n\n&gt; What were we discussing?\nAI: We were discussing your Python project, specifically analyzing CSV files.\n    You mentioned you needed help with CSV analysis. Are you ready to continue?\n</code></pre>"},{"location":"user-guide/memory-management/#file-based-chat-history","title":"File-Based Chat History","text":""},{"location":"user-guide/memory-management/#overview_1","title":"Overview","text":"<p>File-based history allows manual saving and loading of specific conversations, useful for:</p> <ul> <li>Project Documentation: Save conversations related to specific projects</li> <li>Backup: Create backups of important discussions</li> <li>Sharing: Share conversation history with team members</li> <li>Templates: Save common workflows as templates</li> </ul>"},{"location":"user-guide/memory-management/#saving-conversations","title":"Saving Conversations","text":""},{"location":"user-guide/memory-management/#manual-save","title":"Manual Save","text":"<pre><code>/save:filename.json\n</code></pre> <p>Examples: <pre><code># Save with descriptive names\n/save:database_migration_discussion.json\n/save:bug_investigation_2024-01-15.json\n/save:deployment_workflow.json\n\n# Save with timestamps\n/save:session_$(date +%Y%m%d_%H%M).json\n</code></pre></p> <p>Response: <pre><code>\u2705 Conversation saved to database_migration_discussion.json\nSaved 15 messages (3,247 tokens)\nFile location: ./database_migration_discussion.json\n</code></pre></p>"},{"location":"user-guide/memory-management/#loading-conversations","title":"Loading Conversations","text":""},{"location":"user-guide/memory-management/#manual-load","title":"Manual Load","text":"<pre><code>/load:filename.json\n</code></pre> <p>Examples: <pre><code># Load specific conversation\n/load:database_migration_discussion.json\n\n# Load recent session\n/load:session_20240115_1430.json\n</code></pre></p> <p>Response: <pre><code>\u2705 Conversation loaded from database_migration_discussion.json\nLoaded 15 messages from previous session\nContext restored: Database migration planning\n</code></pre></p>"},{"location":"user-guide/memory-management/#file-format","title":"File Format","text":"<p>Conversation files are saved in JSON format:</p> example_conversation.json<pre><code>{\n    \"metadata\": {\n        \"version\": \"1.0\",\n        \"created_at\": \"2024-01-15T14:30:00Z\",\n        \"mcpomni_version\": \"0.3.0\",\n        \"total_messages\": 15,\n        \"total_tokens\": 3247,\n        \"session_duration\": \"45 minutes\"\n    },\n    \"configuration\": {\n        \"llm_provider\": \"openai\",\n        \"llm_model\": \"gpt-4o-mini\",\n        \"connected_servers\": [\"filesystem\", \"database\", \"notifications\"]\n    },\n    \"conversation\": [\n        {\n            \"timestamp\": \"2024-01-15T14:30:15Z\",\n            \"role\": \"user\",\n            \"content\": \"Help me analyze the database schema\",\n            \"command\": null\n        },\n        {\n            \"timestamp\": \"2024-01-15T14:30:20Z\",\n            \"role\": \"assistant\",\n            \"content\": \"I'll help you analyze the database schema...\",\n            \"tools_used\": [\"query_database\"],\n            \"tokens\": 156\n        }\n    ]\n}\n</code></pre>"},{"location":"user-guide/memory-management/#memory-management-commands","title":"Memory Management Commands","text":""},{"location":"user-guide/memory-management/#core-commands","title":"Core Commands","text":"Command Description Example <code>/memory</code> Toggle Redis persistence <code>/memory</code> <code>/save:file</code> Save to file <code>/save:project.json</code> <code>/load:file</code> Load from file <code>/load:project.json</code> <code>/clear</code> Clear current memory <code>/clear</code> <code>/history</code> Show recent history <code>/history</code>"},{"location":"user-guide/memory-management/#advanced-commands","title":"Advanced Commands","text":""},{"location":"user-guide/memory-management/#history","title":"<code>/history</code>","text":"<p>Show recent conversation history:</p> <pre><code>/history\n</code></pre> <p>Output: <pre><code>\ud83d\udcda Recent Conversation History:\n[14:30] User: Help me analyze the database schema\n[14:31] AI: I'll analyze the schema using database tools...\n[14:32] User: /tools\n[14:32] AI: Available tools: query_database, get_schema...\n[14:35] User: Show me the users table structure\n[14:36] AI: Here's the users table structure...\n</code></pre></p>"},{"location":"user-guide/memory-management/#clear","title":"<code>/clear</code>","text":"<p>Clear current conversation memory:</p> <pre><code>/clear\n</code></pre> <p>Response: <pre><code>\ud83e\uddf9 Conversation history cleared\n- Session memory: cleared\n- Redis memory: preserved (use /memory to disable)\n- File history: preserved\n</code></pre></p>"},{"location":"user-guide/memory-management/#memory-configuration","title":"Memory Configuration","text":""},{"location":"user-guide/memory-management/#redis-configuration-options","title":"Redis Configuration Options","text":".env<pre><code># Basic Redis setup\nREDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_DB=0\n\n# Advanced Redis setup\nREDIS_PASSWORD=secure_password\nREDIS_SSL=true\nREDIS_MAX_CONNECTIONS=10\nREDIS_TIMEOUT=30\n\n# Memory management\nREDIS_TTL=604800          # 7 days in seconds\nREDIS_MAX_MESSAGES=1000   # Max messages per conversation\n</code></pre>"},{"location":"user-guide/memory-management/#memory-limits","title":"Memory Limits","text":"<p>Configure memory limits in <code>servers_config.json</code>:</p> <pre><code>{\n    \"AgentConfig\": {\n        \"memory_settings\": {\n            \"max_conversation_length\": 1000,    // Max messages\n            \"max_context_tokens\": 50000,        // Max tokens in context\n            \"auto_prune_threshold\": 0.8,        // Prune when 80% full\n            \"redis_ttl_days\": 7                 // Redis expiration\n        }\n    }\n}\n</code></pre>"},{"location":"user-guide/memory-management/#memory-usage-patterns","title":"Memory Usage Patterns","text":""},{"location":"user-guide/memory-management/#project-based-memory","title":"Project-Based Memory","text":"<p>Organize conversations by project:</p> <pre><code># Start project\n/load:project_alpha_setup.json\n\n# Work on project\n[... development conversation ...]\n\n# Save progress\n/save:project_alpha_progress_$(date +%m%d).json\n\n# End session\n/save:project_alpha_final.json\n</code></pre>"},{"location":"user-guide/memory-management/#daily-workflow-memory","title":"Daily Workflow Memory","text":"<pre><code># Morning: Load yesterday's context\n/load:daily_work_$(date -d yesterday +%Y%m%d).json\n\n# Work throughout day with Redis persistence\n/memory  # Ensure Redis is enabled\n\n# Evening: Save daily summary\n/save:daily_work_$(date +%Y%m%d).json\n</code></pre>"},{"location":"user-guide/memory-management/#team-collaboration-memory","title":"Team Collaboration Memory","text":"<pre><code># Save conversation for team review\n/save:team_review_database_design.json\n\n# Team member loads and continues\n/load:team_review_database_design.json\n# Continue conversation with context\n</code></pre>"},{"location":"user-guide/memory-management/#memory-best-practices","title":"Memory Best Practices","text":""},{"location":"user-guide/memory-management/#when-to-use-each-type","title":"When to Use Each Type","text":"<p>Memory Type Selection</p> <ul> <li>Session Memory: Default for temporary work</li> <li>Redis Memory: Long-running projects, development work</li> <li>File History: Important conversations, team sharing, backups</li> </ul>"},{"location":"user-guide/memory-management/#optimization-tips","title":"Optimization Tips","text":"<ol> <li> <p>Regular Cleanup <pre><code># Clear old conversations periodically\n/clear\n\n# Save important parts before clearing\n/save:important_findings.json\n</code></pre></p> </li> <li> <p>Strategic Saving <pre><code># Save at natural breakpoints\n/save:analysis_complete.json    # After major analysis\n/save:before_deployment.json    # Before risky operations\n/save:meeting_notes.json        # After team discussions\n</code></pre></p> </li> <li> <p>Context Management <pre><code># Load relevant context\n/load:project_context.json\n\n# Add new information\n[... conversation ...]\n\n# Save updated context\n/save:project_context_updated.json\n</code></pre></p> </li> </ol>"},{"location":"user-guide/memory-management/#troubleshooting-memory","title":"Troubleshooting Memory","text":""},{"location":"user-guide/memory-management/#redis-connection-issues","title":"Redis Connection Issues","text":"<p>Redis Connection Failed</p> <p>Error: <code>Could not connect to Redis</code></p> <p>Solutions: <pre><code># Check Redis status\nredis-cli ping\n\n# Start Redis if needed\nsudo systemctl start redis-server\n\n# Check configuration\ncat .env | grep REDIS\n\n# Test connection manually\nredis-cli -h localhost -p 6379\n</code></pre></p>"},{"location":"user-guide/memory-management/#file-loading-issues","title":"File Loading Issues","text":"<p>File Not Found</p> <p>Error: <code>Could not load conversation file</code></p> <p>Solutions: <pre><code># Check file exists\nls -la *.json\n\n# Use full path\n/load:/full/path/to/conversation.json\n\n# Check file permissions\nchmod 644 conversation.json\n</code></pre></p>"},{"location":"user-guide/memory-management/#memory-performance-issues","title":"Memory Performance Issues","text":"<p>Slow Memory Operations</p> <p>Issue: Memory operations taking too long</p> <p>Solutions: <pre><code># Check Redis memory usage\nredis-cli info memory\n\n# Clear old conversations\n/clear\n\n# Reduce context size\n# Edit AgentConfig.memory_settings\n</code></pre></p>"},{"location":"user-guide/memory-management/#advanced-memory-features","title":"Advanced Memory Features","text":""},{"location":"user-guide/memory-management/#memory-analytics","title":"Memory Analytics","text":"<pre><code># Check memory usage\n/memory stats\n</code></pre> <p>Output: <pre><code>\ud83d\udcca Memory Usage Statistics:\n- Redis Memory: 15.2 MB\n- Active Conversations: 3\n- Total Messages: 1,247\n- Average Message Size: 89 tokens\n- Oldest Conversation: 5 days ago\n- Memory Efficiency: 92%\n</code></pre></p>"},{"location":"user-guide/memory-management/#automatic-memory-management","title":"Automatic Memory Management","text":"<p>MCPOmni Connect automatically:</p> <ol> <li>Prunes Old Messages: Removes messages beyond token limits</li> <li>Compresses Context: Summarizes old conversations</li> <li>Cleans Expired Data: Removes data past TTL</li> <li>Optimizes Storage: Compacts conversation data</li> </ol>"},{"location":"user-guide/memory-management/#memory-integration-with-modes","title":"Memory Integration with Modes","text":"<p>Different operation modes use memory differently:</p> Mode Memory Usage Context Retention Chat Full history High Autonomous Task-focused Medium Orchestrator Strategic context Variable <p>Next: Prompt Management \u2192 </p>"},{"location":"user-guide/operation-modes/","title":"Operation Modes","text":"<p>MCPOmni Connect offers three distinct operation modes, each designed for different levels of automation and user involvement.</p>"},{"location":"user-guide/operation-modes/#mode-overview","title":"Mode Overview","text":"Mode Description User Involvement Best For Chat Interactive with approval High Learning, careful operations Autonomous Independent execution Low Well-defined tasks Orchestrator Multi-agent coordination Minimal Complex workflows"},{"location":"user-guide/operation-modes/#chat-mode-default","title":"Chat Mode (Default)","text":"<p>Interactive mode where the AI asks for permission before executing any tools.</p>"},{"location":"user-guide/operation-modes/#characteristics","title":"Characteristics","text":"<ul> <li>\u2705 User Control: Every tool execution requires explicit approval</li> <li>\u2705 Educational: Shows reasoning process step-by-step</li> <li>\u2705 Safe: No unexpected actions</li> <li>\u2705 Transparent: Clear explanation of what each tool does</li> </ul>"},{"location":"user-guide/operation-modes/#how-it-works","title":"How It Works","text":"<pre><code>&gt; Can you list the files in the current directory?\n\nI'll help you list the files in the current directory using the filesystem tools.\n\n\ud83d\udd27 Tool: list_directory\n\ud83d\udcc1 Path: .\n\ud83d\udccb Purpose: List all files and directories in the current location\n\u2753 Execute this tool? (y/n): y\n\n[Tool executes after user approval]\n\nFound 5 items:\n- README.md (file)\n- src/ (directory)\n- docs/ (directory)\n- pyproject.toml (file)\n- .gitignore (file)\n</code></pre>"},{"location":"user-guide/operation-modes/#when-to-use-chat-mode","title":"When to Use Chat Mode","text":"<ul> <li>Learning: Understanding how MCPOmni Connect works</li> <li>Sensitive Operations: Working with important files or systems</li> <li>Exploration: Discovering what tools are available</li> <li>Debugging: Step-by-step problem diagnosis</li> </ul>"},{"location":"user-guide/operation-modes/#switching-to-chat-mode","title":"Switching to Chat Mode","text":"<pre><code>/mode:chat\nNow operating in CHAT mode. I will ask for approval before executing tasks.\n</code></pre>"},{"location":"user-guide/operation-modes/#autonomous-mode","title":"Autonomous Mode","text":"<p>Independent execution where the AI operates using ReAct (Reasoning + Acting) methodology.</p>"},{"location":"user-guide/operation-modes/#characteristics_1","title":"Characteristics","text":"<ul> <li>\ud83e\udd16 Independent: No approval required for tool execution</li> <li>\ud83e\udde0 Reasoning: Shows thought process before each action</li> <li>\ud83d\udd04 Iterative: Adapts based on results</li> <li>\ud83d\udcca Goal-Oriented: Focuses on completing the requested task</li> <li>\u26a1 Efficient: Faster execution without approval delays</li> </ul>"},{"location":"user-guide/operation-modes/#how-it-works_1","title":"How It Works","text":"<pre><code>&gt; /mode:auto\nNow operating in AUTONOMOUS mode. I will execute tasks independently.\n\n&gt; Create a backup of all configuration files\n\n\ud83e\udd16 Executing autonomous task: Create backup of configuration files\n\n\ud83d\udcad Thought: I need to find all configuration files, create a backup directory, and copy them safely.\n\n\ud83d\udd27 Action: list_directory(path=\".\", pattern=\"*.{json,yaml,yml,conf}\")\n\ud83d\udcca Observation: Found 3 configuration files: servers_config.json, docker-compose.yaml, .env\n\n\ud83d\udcad Thought: I'll create a timestamped backup directory and copy the files.\n\n\ud83d\udd27 Action: create_directory(path=\"backup_2024-01-15_14-30\")\n\ud83d\udcca Observation: Backup directory created successfully.\n\n\ud83d\udd27 Action: copy_files(source=[\"servers_config.json\", \"docker-compose.yaml\"], destination=\"backup_2024-01-15_14-30/\")\n\ud83d\udcca Observation: 2 files copied successfully (excluding .env for security).\n\n\u2705 Task Complete: Configuration backup created in backup_2024-01-15_14-30/\n</code></pre>"},{"location":"user-guide/operation-modes/#when-to-use-autonomous-mode","title":"When to Use Autonomous Mode","text":"<ul> <li>Routine Tasks: Regular maintenance or data processing</li> <li>Well-Defined Operations: Clear objectives with known steps</li> <li>Batch Processing: Multiple similar operations</li> <li>Time-Sensitive Tasks: When speed is important</li> </ul>"},{"location":"user-guide/operation-modes/#switching-to-autonomous-mode","title":"Switching to Autonomous Mode","text":"<pre><code>/mode:auto\nNow operating in AUTONOMOUS mode. I will execute tasks independently.\n</code></pre>"},{"location":"user-guide/operation-modes/#orchestrator-mode","title":"Orchestrator Mode","text":"<p>Advanced coordination for complex, multi-step workflows across multiple systems.</p>"},{"location":"user-guide/operation-modes/#characteristics_2","title":"Characteristics","text":"<ul> <li>\ud83c\udfaf Strategic Planning: Breaks complex tasks into coordinated subtasks</li> <li>\ud83e\udd1d Multi-Agent: Coordinates multiple specialized agents</li> <li>\u26a1 Parallel Execution: Runs independent tasks simultaneously</li> <li>\ud83d\udcc8 Scalable: Handles enterprise-level workflows</li> <li>\ud83d\udd04 Adaptive: Adjusts strategy based on real-time results</li> </ul>"},{"location":"user-guide/operation-modes/#how-it-works_2","title":"How It Works","text":"<pre><code>&gt; /mode:orchestrator\nNow operating in ORCHESTRATOR mode. I will coordinate complex multi-step tasks.\n\n&gt; Deploy the application, update documentation, and notify the team\n\n\ud83c\udfaf Orchestrator: Planning deployment workflow\n\n\ud83d\udccb Strategic Plan:\nPhase 1: Pre-deployment Checks (Parallel)\n\u251c\u2500 Agent A: Run test suite\n\u251c\u2500 Agent B: Verify database connectivity\n\u2514\u2500 Agent C: Check deployment target status\n\nPhase 2: Deployment (Sequential)\n\u251c\u2500 Agent D: Build application\n\u251c\u2500 Agent E: Deploy to staging\n\u2514\u2500 Agent F: Run smoke tests\n\nPhase 3: Documentation &amp; Notification (Parallel)\n\u251c\u2500 Agent G: Update deployment docs\n\u251c\u2500 Agent H: Generate release notes\n\u2514\u2500 Agent I: Send team notifications\n\n\ud83d\ude80 Executing Phase 1: Pre-deployment Checks...\n\n\ud83e\udd16 Agent A: Running pytest test suite... \u2705 47/47 tests passed\n\ud83e\udd16 Agent B: Database connection verified \u2705 \n\ud83e\udd16 Agent C: Staging environment ready \u2705\n\nPhase 1 Complete (2.1 minutes)\n\n\ud83d\ude80 Executing Phase 2: Deployment...\n[Sequential execution continues...]\n\n\u2705 Workflow Complete (8.3 minutes total):\n- Application deployed successfully\n- Documentation updated\n- Team notified via Slack\n</code></pre>"},{"location":"user-guide/operation-modes/#when-to-use-orchestrator-mode","title":"When to Use Orchestrator Mode","text":"<ul> <li>Complex Deployments: Multi-step deployment workflows</li> <li>Data Migration: Moving data between systems</li> <li>Business Processes: Automated business workflows</li> <li>Integration Tasks: Coordinating multiple systems</li> </ul>"},{"location":"user-guide/operation-modes/#switching-to-orchestrator-mode","title":"Switching to Orchestrator Mode","text":"<pre><code>/mode:orchestrator\nNow operating in ORCHESTRATOR mode. I will coordinate complex multi-step tasks.\n</code></pre>"},{"location":"user-guide/operation-modes/#mode-comparison","title":"Mode Comparison","text":""},{"location":"user-guide/operation-modes/#execution-speed","title":"Execution Speed","text":"Mode Speed Reason Chat Slowest Waits for user approval Autonomous Fast No approval delays Orchestrator Variable Depends on task complexity"},{"location":"user-guide/operation-modes/#safety-level","title":"Safety Level","text":"Mode Safety Control Chat Highest Full user control Autonomous Medium Configurable limits Orchestrator Medium Strategic oversight"},{"location":"user-guide/operation-modes/#use-case-examples","title":"Use Case Examples","text":"Chat ModeAutonomous ModeOrchestrator Mode <ul> <li>Learning MCPOmni Connect</li> <li>Exploring new MCP servers</li> <li>Debugging configuration issues</li> <li>Working with sensitive data</li> <li>Educational demonstrations</li> </ul> <ul> <li>File organization</li> <li>Data analysis reports</li> <li>System health checks</li> <li>Log file analysis</li> <li>Backup operations</li> </ul> <ul> <li>Application deployments</li> <li>Database migrations</li> <li>Multi-system integrations</li> <li>Business process automation</li> <li>Complex reporting workflows</li> </ul>"},{"location":"user-guide/operation-modes/#configuration","title":"Configuration","text":"<p>Control mode behavior through <code>AgentConfig</code>:</p> <pre><code>{\n    \"AgentConfig\": {\n        \"tool_call_timeout\": 30,        // Tool execution timeout\n        \"max_steps\": 15,                // Max steps per task (Auto/Orchestrator)\n        \"request_limit\": 1000,          // API request limit\n        \"total_tokens_limit\": 100000    // Token usage limit\n    }\n}\n</code></pre>"},{"location":"user-guide/operation-modes/#mode-specific-settings","title":"Mode-Specific Settings","text":"Setting Chat Auto Orchestrator <code>tool_call_timeout</code> \u2705 \u2705 \u2705 <code>max_steps</code> \u274c \u2705 \u2705 <code>request_limit</code> \u2705 \u2705 \u2705 <code>total_tokens_limit</code> \u2705 \u2705 \u2705"},{"location":"user-guide/operation-modes/#safety-features","title":"Safety Features","text":""},{"location":"user-guide/operation-modes/#built-in-safeguards","title":"Built-in Safeguards","text":"<ul> <li>Timeout Protection: Prevents infinite loops</li> <li>Step Limits: Controls reasoning cycles</li> <li>Resource Limits: Manages API usage</li> <li>Emergency Stop: Ctrl+C to interrupt</li> </ul>"},{"location":"user-guide/operation-modes/#best-practices","title":"Best Practices","text":"<p>Safe Mode Usage</p> <ol> <li>Start with Chat Mode when learning</li> <li>Test in Autonomous with simple tasks first</li> <li>Use Orchestrator for well-understood complex workflows</li> <li>Set Conservative Limits in production</li> <li>Monitor Resource Usage with <code>/api_stats</code></li> </ol>"},{"location":"user-guide/operation-modes/#troubleshooting-modes","title":"Troubleshooting Modes","text":""},{"location":"user-guide/operation-modes/#mode-not-switching","title":"Mode Not Switching","text":"<pre><code># Check current mode\n/status\n\n# Force mode switch\n/mode:chat\n/mode:auto\n/mode:orchestrator\n</code></pre>"},{"location":"user-guide/operation-modes/#autonomous-mode-stuck","title":"Autonomous Mode Stuck","text":"<pre><code># Check current progress\n/debug\n\n# Stop execution\nCtrl+C\n\n# Switch back to chat mode\n/mode:chat\n</code></pre>"},{"location":"user-guide/operation-modes/#resource-limits-reached","title":"Resource Limits Reached","text":"<pre><code># Check usage\n/api_stats\n\n# Adjust limits in servers_config.json\n{\n    \"AgentConfig\": {\n        \"total_tokens_limit\": 200000,  // Increase limit\n        \"max_steps\": 25               // Allow more steps\n    }\n}\n</code></pre>"},{"location":"user-guide/operation-modes/#advanced-usage","title":"Advanced Usage","text":""},{"location":"user-guide/operation-modes/#mode-specific-commands","title":"Mode-Specific Commands","text":"<pre><code># Check current mode and status\n/status\n\n# View execution history (Autonomous/Orchestrator)\n/history\n\n# Monitor resource usage\n/api_stats\n\n# Enable detailed logging\n/debug\n</code></pre>"},{"location":"user-guide/operation-modes/#combining-modes","title":"Combining Modes","text":"<p>You can switch between modes within a session:</p> <pre><code># Start with exploration in chat mode\n/mode:chat\nWhat tools are available?\n\n# Switch to autonomous for execution\n/mode:auto\nProcess all the log files and create a summary\n\n# Back to chat for review\n/mode:chat\nCan you explain what you found in the logs?\n</code></pre> <p>Next: Commands \u2192 </p>"}]}